{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSpbm26T3Vbz",
        "outputId": "e23ff06b-7add-4303-8f87-a9d90373ff4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torch-geometric scikit-learn pandas networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ckNAtN356c",
        "outputId": "e61977c8-7ace-4f31-c7b0-b4cc0e8bcf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20icbc1u39TW",
        "outputId": "2ed25270-afc8-4de0-c3b6-564d261bfcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch-scatter and torch-sparse compatible with torch==2.6.0+cu124\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html -q\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html -q"
      ],
      "metadata": {
        "id": "WNdPEpUm4Arb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4329e9-dd8c-4df2-f82f-76caf4f347ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTM7qI9m4GdV",
        "outputId": "7ca2e60a-dd69-4d8d-ac98-94f82466d4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os  # Make sure to import the os module\n",
        "\n",
        "benign_data_dir = \"/content/drive/My Drive/ids/iot/benign/\"\n",
        "mirai_data_dir = \"/content/drive/My Drive/ids/iot/mirai/\"\n",
        "\n",
        "benign_files = [f\"{benign_data_dir}{file}\" for file in os.listdir(benign_data_dir) if file.endswith(\".csv\")]\n",
        "mirai_files = [f\"{mirai_data_dir}{file}\" for file in os.listdir(mirai_data_dir) if file.endswith(\".csv\")]\n",
        "\n",
        "benign_dfs = [pd.read_csv(file) for file in benign_files]\n",
        "mirai_dfs = [pd.read_csv(file) for file in mirai_files]\n",
        "\n",
        "benign_df = pd.concat(benign_dfs, ignore_index=True)\n",
        "mirai_df = pd.concat(mirai_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Number of benign samples: {len(benign_df)}\")\n",
        "print(f\"Number of Mirai samples: {len(mirai_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VHRlHeR4OoT",
        "outputId": "77c0ff7c-c1ee-4127-c18a-c039325c41e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of benign samples: 84526\n",
            "Number of Mirai samples: 33758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benign_df['label'] = 0  # 0 for benign\n",
        "mirai_df['label'] = 1   # 1 for Mirai"
      ],
      "metadata": {
        "id": "4-tRErEf41E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([benign_df, mirai_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "Y38g6WBT42gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in benign_df:\")\n",
        "print(benign_df.columns)\n",
        "print(\"\\nColumns in mirai_df:\")\n",
        "print(mirai_df.columns)\n",
        "\n",
        "# Combine the benign and mirai dataframes into a single dataframe named df\n",
        "df = pd.concat([benign_df, mirai_df], ignore_index=True)\n",
        "\n",
        "print(\"\\nColumns in combined df:\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngipWrJC4nE8",
        "outputId": "d1298aef-829c-4c54-e274-933d43763aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in benign_df:\n",
            "Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n",
            "       'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
            "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
            "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
            "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
            "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
            "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
            "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
            "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
            "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
            "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
            "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
            "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
            "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
            "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
            "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
            "       'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
            "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
            "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
            "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
            "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
            "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
            "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
            "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
            "       'Idle Min', 'Label', 'label'],\n",
            "      dtype='object')\n",
            "\n",
            "Columns in mirai_df:\n",
            "Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n",
            "       'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
            "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
            "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
            "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
            "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
            "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
            "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
            "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
            "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
            "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
            "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
            "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
            "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
            "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
            "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
            "       'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
            "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
            "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
            "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
            "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
            "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
            "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
            "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
            "       'Idle Min', 'Label', 'label'],\n",
            "      dtype='object')\n",
            "\n",
            "Columns in combined df:\n",
            "Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n",
            "       'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
            "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
            "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
            "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
            "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
            "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
            "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
            "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
            "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
            "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
            "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
            "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
            "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
            "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
            "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
            "       'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
            "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
            "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
            "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
            "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
            "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
            "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
            "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
            "       'Idle Min', 'Label', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "print(\"Number of infinite values in df:\")\n",
        "print(np.isinf(df[numerical_cols]).sum())\n",
        "print(\"\\nNumber of NaN values in df:\")\n",
        "print(df[numerical_cols].isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KiTJYtz5EJU",
        "outputId": "d24b8f37-42ca-4632-a3db-cfa1a5bc7fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of infinite values in df:\n",
            "Src Port            0\n",
            "Dst Port            0\n",
            "Protocol            0\n",
            "Flow Duration       0\n",
            "Total Fwd Packet    0\n",
            "                   ..\n",
            "Idle Mean           0\n",
            "Idle Std            0\n",
            "Idle Max            0\n",
            "Idle Min            0\n",
            "label               0\n",
            "Length: 80, dtype: int64\n",
            "\n",
            "Number of NaN values in df:\n",
            "Src Port            0\n",
            "Dst Port            0\n",
            "Protocol            0\n",
            "Flow Duration       0\n",
            "Total Fwd Packet    0\n",
            "                   ..\n",
            "Idle Mean           0\n",
            "Idle Std            0\n",
            "Idle Max            0\n",
            "Idle Min            0\n",
            "label               0\n",
            "Length: 80, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Identify numerical and categorical columns (MATCH ACTUAL COLUMN NAMES)\n",
        "numerical_cols = ['Flow Duration',\n",
        "                  'Total Fwd Packet',\n",
        "                  'Total Bwd packets',\n",
        "                  'Total Length of Fwd Packet',\n",
        "                  'Total Length of Bwd Packet',\n",
        "                  'Fwd Packet Length Max',\n",
        "                  'Fwd Packet Length Min',\n",
        "                  'Fwd Packet Length Mean',\n",
        "                  'Fwd Packet Length Std',\n",
        "                  'Bwd Packet Length Max',\n",
        "                  'Bwd Packet Length Min',\n",
        "                  'Bwd Packet Length Mean',\n",
        "                  'Bwd Packet Length Std',\n",
        "                  'Flow Bytes/s',\n",
        "                  'Flow Packets/s'] # Add all the numerical columns you want to use\n",
        "\n",
        "categorical_cols = ['Protocol'] # Use the correct column name\n",
        "\n",
        "# Combine the benign and mirai dataframes before preprocessing\n",
        "df = pd.concat([benign_df, mirai_df], ignore_index=True)\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "df[numerical_cols] = df[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Impute missing values (using median for numerical columns)\n",
        "imputer_numerical = SimpleImputer(strategy='median')\n",
        "df[numerical_cols] = imputer_numerical.fit_transform(df[numerical_cols])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other columns\n",
        ")\n",
        "\n",
        "# Apply the preprocessing\n",
        "df_processed = pd.DataFrame(preprocessor.fit_transform(df))\n",
        "\n",
        "# Get the names of the new columns after one-hot encoding\n",
        "ohe_categories = preprocessor.named_transformers_['cat'].categories_[0]\n",
        "new_cat_cols = [f'Protocol_{cat}' for cat in ohe_categories]\n",
        "\n",
        "# Assign column names to the processed DataFrame\n",
        "feature_names = numerical_cols + new_cat_cols + [col for col in df.columns if col not in numerical_cols + categorical_cols]\n",
        "df_processed.columns = feature_names\n",
        "\n",
        "# Drop the original categorical column\n",
        "df.drop(columns=categorical_cols, inplace=True)\n",
        "\n",
        "print(\"Processed DataFrame shape:\", df_processed.shape)\n",
        "print(\"Processed DataFrame head:\")\n",
        "print(df_processed.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdaLrhD55Lai",
        "outputId": "580f5da9-888b-4fb3-f820-2e0f51fce2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed DataFrame shape: (118284, 87)\n",
            "Processed DataFrame head:\n",
            "  Flow Duration Total Fwd Packet Total Bwd packets Total Length of Fwd Packet  \\\n",
            "0     -0.608945        -0.032014          0.378796                   0.468405   \n",
            "1     -0.593817        -0.041258         -0.020484                  -0.040247   \n",
            "2     -0.611466        -0.044836         -0.037423                  -0.053652   \n",
            "3     -0.599494        -0.008756          0.223924                  -0.051197   \n",
            "4     -0.602297        -0.042152         -0.027744                  -0.048647   \n",
            "\n",
            "  Total Length of Bwd Packet Fwd Packet Length Max Fwd Packet Length Min  \\\n",
            "0                  -0.012989              3.676065             -0.540546   \n",
            "1                   0.013221              1.210733             -0.540546   \n",
            "2                  -0.012989             -0.198538             -0.540546   \n",
            "3                   1.211312             -0.013906             -0.540546   \n",
            "4                   0.003557                 0.218             -0.540546   \n",
            "\n",
            "  Fwd Packet Length Mean Fwd Packet Length Std Bwd Packet Length Max  ...  \\\n",
            "0              15.059867              7.155258             -0.327351  ...   \n",
            "1               0.812521              2.796089              4.017415  ...   \n",
            "2              -0.469124             -0.251263             -0.327351  ...   \n",
            "3              -0.442409             -0.055914              4.017415  ...   \n",
            "4               0.139864              0.817114              4.808055  ...   \n",
            "\n",
            "  Active Mean Active Std Active Max Active Min Idle Mean Idle Std Idle Max  \\\n",
            "0         0.0        0.0        0.0        0.0       0.0      0.0      0.0   \n",
            "1         0.0        0.0        0.0        0.0       0.0      0.0      0.0   \n",
            "2         0.0        0.0        0.0        0.0       0.0      0.0      0.0   \n",
            "3         0.0        0.0        0.0        0.0       0.0      0.0      0.0   \n",
            "4         0.0        0.0        0.0        0.0       0.0      0.0      0.0   \n",
            "\n",
            "  Idle Min            Label label  \n",
            "0      0.0  NeedManualLabel     0  \n",
            "1      0.0  NeedManualLabel     0  \n",
            "2      0.0  NeedManualLabel     0  \n",
            "3      0.0  NeedManualLabel     0  \n",
            "4      0.0  NeedManualLabel     0  \n",
            "\n",
            "[5 rows x 87 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz_pwLT65VLR",
        "outputId": "05adb366-ffb1-4e73-e0f0-7e1a0b157641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt26cu124)\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (935 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.0/936.0 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-spline-conv, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt21cu121 torch-spline-conv-1.2.2+pt21cu121\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_processed.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmsskYBR5i7A",
        "outputId": "3419ecde-8874-46a9-fc0e-23e41064b0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flow Duration                 object\n",
            "Total Fwd Packet              object\n",
            "Total Bwd packets             object\n",
            "Total Length of Fwd Packet    object\n",
            "Total Length of Bwd Packet    object\n",
            "                               ...  \n",
            "Idle Std                      object\n",
            "Idle Max                      object\n",
            "Idle Min                      object\n",
            "Label                         object\n",
            "label                         object\n",
            "Length: 87, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Dummy Data (REPLACE WITH YOUR ACTUAL df_processed) ---\n",
        "# This simulates your preprocessed DataFrame.\n",
        "# In a real scenario, df_processed would be loaded from your CSV/database\n",
        "# and would have undergone feature engineering.\n",
        "print(\"Creating dummy df_processed for demonstration purposes...\")\n",
        "num_samples = 5000 # Let's simulate a moderately sized dataset\n",
        "num_unique_ips = 100 # Number of unique IPs for connection logic\n",
        "\n",
        "# Generate dummy data\n",
        "data = {\n",
        "    'Flow ID': range(num_samples),\n",
        "    'Src IP': np.random.choice([f'192.168.1.{i}' for i in range(num_unique_ips)], size=num_samples),\n",
        "    'Dst IP': np.random.choice([f'192.168.1.{i}' for i in range(num_unique_ips)], size=num_samples),\n",
        "    'Timestamp': pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_samples, freq='s')),\n",
        "    'PacketLength': np.random.randint(50, 1500, num_samples),\n",
        "    'Duration': np.random.rand(num_samples) * 100,\n",
        "    'Protocol': np.random.choice([6, 17], size=num_samples), # TCP (6) or UDP (17)\n",
        "    'label': np.random.randint(0, 2, num_samples) # Binary classification label (e.g., 0: normal, 1: malware/anomaly)\n",
        "}\n",
        "df_processed = pd.DataFrame(data)\n",
        "\n",
        "# Ensure 'label' is integer type\n",
        "df_processed['label'] = df_processed['label'].astype(int)\n",
        "\n",
        "print(f\"Dummy df_processed created with {len(df_processed)} rows and {len(df_processed.columns)} columns.\")\n",
        "print(df_processed.head())\n",
        "print(f\"Unique labels: {df_processed['label'].unique()}\")\n",
        "print(f\"Label distribution:\\n{df_processed['label'].value_counts()}\")\n",
        "\n",
        "\n",
        "# --- 2. Extract Features (x) and Labels (y) ---\n",
        "label_col_name = 'label'\n",
        "# Columns to exclude from features (e.g., identifiers, original string labels)\n",
        "cols_to_exclude_from_features = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', label_col_name]\n",
        "\n",
        "feature_cols = [col for col in df_processed.columns if col not in cols_to_exclude_from_features]\n",
        "\n",
        "# Convert features to PyTorch tensor (float32 is standard for model inputs)\n",
        "x = torch.tensor(df_processed[feature_cols].values.astype(np.float32), dtype=torch.float)\n",
        "\n",
        "# Convert labels to PyTorch tensor (long is required for classification labels)\n",
        "y = torch.tensor(df_processed[label_col_name].values, dtype=torch.long)\n",
        "\n",
        "print(f\"\\nFeatures (x) shape: {x.shape}\")\n",
        "print(f\"Labels (y) shape: {y.shape}\")\n",
        "\n",
        "\n",
        "# --- 3. Graph Construction (CRITICAL FOR MEMORY EFFICIENCY) ---\n",
        "# This is where you define the meaningful connections (edges) between your nodes.\n",
        "# Instead of a dense graph, we build a sparse graph based on shared attributes.\n",
        "# Example: Connect flows that share the same Source IP or Destination IP.\n",
        "\n",
        "print(\"\\nConstructing graph edges based on shared Source/Destination IPs...\")\n",
        "edges = []\n",
        "num_nodes = x.size(0)\n",
        "\n",
        "# Create a mapping from IP address to a list of node indices (Flow IDs) that use that IP\n",
        "ip_to_nodes = {}\n",
        "\n",
        "# Populate the mapping\n",
        "for i, row_data in df_processed.iterrows():\n",
        "    src_ip = row_data['Src IP']\n",
        "    dst_ip = row_data['Dst IP']\n",
        "\n",
        "    # Add current node index to lists for both source and destination IPs\n",
        "    if src_ip not in ip_to_nodes:\n",
        "        ip_to_nodes[src_ip] = []\n",
        "    ip_to_nodes[src_ip].append(i)\n",
        "\n",
        "    if dst_ip not in ip_to_nodes:\n",
        "        ip_to_nodes[dst_ip] = []\n",
        "    ip_to_nodes[dst_ip].append(i)\n",
        "\n",
        "# Now, create edges. For each IP, connect all flows that use it.\n",
        "# This creates small \"cliques\" or star-like structures around each IP.\n",
        "# For very large `nodes_list`, you might want to connect only adjacent nodes in time,\n",
        "# or use a bipartite graph approach (flows connected to IPs, IPs connected to flows).\n",
        "for ip, nodes_list in ip_to_nodes.items():\n",
        "    if len(nodes_list) > 1:\n",
        "        # Connect each node in the list to every other node in the list (forms a clique for that IP)\n",
        "        # This can still generate many edges if an IP is involved in many flows.\n",
        "        # For extremely large datasets, consider connecting only adjacent nodes in `nodes_list`\n",
        "        # or connecting all nodes to a dummy 'IP node' for a bipartite graph.\n",
        "        for i in range(len(nodes_list)):\n",
        "            for j in range(i + 1, len(nodes_list)):\n",
        "                node1 = nodes_list[i]\n",
        "                node2 = nodes_list[j]\n",
        "                edges.append([node1, node2])\n",
        "                edges.append([node2, node1]) # Add reverse edge for undirected graph\n",
        "\n",
        "# Convert the list of edges to a PyTorch tensor\n",
        "if edges:\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "else:\n",
        "    edge_index = torch.empty((2, 0), dtype=torch.long) # Handle case with no edges\n",
        "\n",
        "print(f\"Generated edge_index shape: {edge_index.shape} (Number of edges: {edge_index.shape[1]})\")\n",
        "# Note: The number of edges will depend heavily on the distribution of IPs in your dummy data.\n",
        "# For real data, this can still be large but manageable, unlike num_nodes * num_nodes.\n",
        "\n",
        "\n",
        "# --- 4. Create Train/Test Masks (using sklearn for robust splitting) ---\n",
        "print(\"\\nSplitting data into training and test sets using stratified split...\")\n",
        "if num_nodes > 0:\n",
        "    indices = np.arange(num_nodes)\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.2, # 20% for testing\n",
        "        random_state=42, # For reproducibility\n",
        "        stratify=y.cpu().numpy() # Ensures similar class distribution in both sets\n",
        "    )\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_indices] = True\n",
        "    test_mask[test_indices] = True\n",
        "else:\n",
        "    train_mask = torch.zeros(0, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(0, dtype=torch.bool)\n",
        "\n",
        "\n",
        "# --- 5. Create the PyTorch Geometric Data object ---\n",
        "data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "print(\"\\nPyG Data object created:\")\n",
        "print(data)\n",
        "print(f\"Number of nodes: {data.num_nodes}\")\n",
        "print(f\"Number of edges: {data.num_edges}\")\n",
        "print(f\"Number of features: {data.num_features}\")\n",
        "print(f\"Number of classes: {data.y.unique().size(0)}\")\n",
        "print(f\"Number of training samples: {data.train_mask.sum()}\")\n",
        "print(f\"Number of test samples: {data.test_mask.sum()}\")\n",
        "\n",
        "\n",
        "# --- 6. GraphSAGE Model Definition ---\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        # SAGEConv is a GraphSAGE layer\n",
        "        # in_channels: dimension of input node features\n",
        "        # hidden_channels: dimension of output features for the first layer\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        # out_channels: dimension of final output features (number of classes for classification)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # First GraphSAGE layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        # Apply ReLU activation function\n",
        "        x = F.relu(x)\n",
        "        # Apply dropout for regularization during training\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        # Second GraphSAGE layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# --- 7. Training and Testing Functions ---\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train() # Set the model to training mode\n",
        "    optimizer.zero_grad() # Clear gradients from previous step\n",
        "\n",
        "    # Perform a forward pass\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Calculate loss only on training nodes\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    loss.backward() # Backpropagate the loss\n",
        "    optimizer.step() # Update model parameters\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "\n",
        "    # Perform a forward pass\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Get predicted labels by taking the argmax of the output logits\n",
        "    predicted_labels = out.argmax(dim=1)\n",
        "    true_labels = data.y\n",
        "\n",
        "    # Evaluate performance on training set\n",
        "    y_true_train = true_labels[data.train_mask].cpu().numpy()\n",
        "    y_pred_train = predicted_labels[data.train_mask].cpu().numpy()\n",
        "    print(\"\\n--- Training Set Classification Report ---\")\n",
        "    if len(y_true_train) > 0:\n",
        "        print(classification_report(y_true_train, y_pred_train, zero_division=0))\n",
        "    else:\n",
        "        print(\"No samples in training set mask.\")\n",
        "\n",
        "    # Evaluate performance on test set\n",
        "    y_true_test = true_labels[data.test_mask].cpu().numpy()\n",
        "    y_pred_test = predicted_labels[data.test_mask].cpu().numpy()\n",
        "    print(\"\\n--- Test Set Classification Report ---\")\n",
        "    if len(y_true_test) > 0:\n",
        "        print(classification_report(y_true_test, y_pred_test, zero_division=0))\n",
        "    else:\n",
        "        print(\"No samples in test set mask.\")\n",
        "\n",
        "\n",
        "# --- 8. Model Initialization and Training Loop ---\n",
        "# Determine input and output channels for the model\n",
        "in_channels = data.num_features\n",
        "out_channels = data.y.unique().size(0) # Number of unique classes\n",
        "\n",
        "# Basic checks before initializing model\n",
        "if in_channels == 0:\n",
        "    print(\"Error: No features detected. Please ensure 'feature_cols' are correctly identified.\")\n",
        "elif out_channels < 2:\n",
        "    print(f\"Error: Only {out_channels} class(es) detected. Need at least 2 for classification.\")\n",
        "elif data.num_nodes == 0:\n",
        "    print(\"Error: No nodes in the graph. Cannot train model.\")\n",
        "else:\n",
        "    # Initialize the GraphSAGE model\n",
        "    model = GraphSAGE(in_channels=in_channels, hidden_channels=128, out_channels=out_channels)\n",
        "\n",
        "    # Define optimizer (Adam is a common choice)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Define loss function (CrossEntropyLoss for multi-class classification)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(1, 101): # Train for 100 epochs\n",
        "        loss = train(model, data, optimizer, criterion)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "    print(\"\\nTraining complete. Running final evaluation...\")\n",
        "    test(model, data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knCx1eQBGALd",
        "outputId": "81eba3a1-9ebf-4d40-a131-fa6f8ac4a162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dummy df_processed for demonstration purposes...\n",
            "Dummy df_processed created with 5000 rows and 8 columns.\n",
            "   Flow ID        Src IP        Dst IP           Timestamp  PacketLength  \\\n",
            "0        0  192.168.1.72  192.168.1.26 2023-01-01 00:00:00          1335   \n",
            "1        1  192.168.1.47  192.168.1.55 2023-01-01 00:00:01           473   \n",
            "2        2  192.168.1.38  192.168.1.28 2023-01-01 00:00:02           375   \n",
            "3        3  192.168.1.17  192.168.1.77 2023-01-01 00:00:03          1369   \n",
            "4        4  192.168.1.95   192.168.1.6 2023-01-01 00:00:04           899   \n",
            "\n",
            "    Duration  Protocol  label  \n",
            "0  87.378790         6      0  \n",
            "1   1.189068        17      0  \n",
            "2  50.377009         6      1  \n",
            "3  14.454529         6      0  \n",
            "4  35.512455         6      1  \n",
            "Unique labels: [0 1]\n",
            "Label distribution:\n",
            "label\n",
            "1    2510\n",
            "0    2490\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Features (x) shape: torch.Size([5000, 3])\n",
            "Labels (y) shape: torch.Size([5000])\n",
            "\n",
            "Constructing graph edges based on shared Source/Destination IPs...\n",
            "Generated edge_index shape: torch.Size([2, 998982]) (Number of edges: 998982)\n",
            "\n",
            "Splitting data into training and test sets using stratified split...\n",
            "\n",
            "PyG Data object created:\n",
            "Data(x=[5000, 3], edge_index=[2, 998982], y=[5000], train_mask=[5000], test_mask=[5000])\n",
            "Number of nodes: 5000\n",
            "Number of edges: 998982\n",
            "Number of features: 3\n",
            "Number of classes: 2\n",
            "Number of training samples: 4000\n",
            "Number of test samples: 1000\n",
            "\n",
            "Starting training...\n",
            "Epoch: 010, Loss: 147.4546\n",
            "Epoch: 020, Loss: 45.2795\n",
            "Epoch: 030, Loss: 34.7864\n",
            "Epoch: 040, Loss: 19.5729\n",
            "Epoch: 050, Loss: 18.6746\n",
            "Epoch: 060, Loss: 12.1315\n",
            "Epoch: 070, Loss: 11.0644\n",
            "Epoch: 080, Loss: 16.3686\n",
            "Epoch: 090, Loss: 16.4766\n",
            "Epoch: 100, Loss: 14.5955\n",
            "\n",
            "Training complete. Running final evaluation...\n",
            "\n",
            "--- Training Set Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1992\n",
            "           1       0.50      1.00      0.67      2008\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.25      0.50      0.33      4000\n",
            "weighted avg       0.25      0.50      0.34      4000\n",
            "\n",
            "\n",
            "--- Test Set Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       498\n",
            "           1       0.50      1.00      0.67       502\n",
            "\n",
            "    accuracy                           0.50      1000\n",
            "   macro avg       0.25      0.50      0.33      1000\n",
            "weighted avg       0.25      0.50      0.34      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from datetime import timedelta # Import timedelta for temporal connections\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive (ensure this is run at the beginning of your Colab session)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 1. Data Loading ---\n",
        "print(\"--- Step 1: Data Loading ---\")\n",
        "benign_data_dir = \"/content/drive/My Drive/ids/iot/benign/\"\n",
        "mirai_data_dir = \"/content/drive/My Drive/ids/iot/mirai/\"\n",
        "\n",
        "benign_files = [f\"{benign_data_dir}{file}\" for file in os.listdir(benign_data_dir) if file.endswith(\".csv\")]\n",
        "mirai_files = [f\"{mirai_data_dir}{file}\" for file in os.listdir(mirai_data_dir) if file.endswith(\".csv\")]\n",
        "\n",
        "print(f\"Found {len(benign_files)} benign files and {len(mirai_files)} mirai files.\")\n",
        "\n",
        "# Load and concatenate benign data\n",
        "benign_dfs = []\n",
        "for file in benign_files:\n",
        "    try:\n",
        "        benign_dfs.append(pd.read_csv(file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading benign file {file}: {e}\")\n",
        "benign_df = pd.concat(benign_dfs, ignore_index=True)\n",
        "\n",
        "# Load and concatenate mirai data\n",
        "mirai_dfs = []\n",
        "for file in mirai_files:\n",
        "    try:\n",
        "        mirai_dfs.append(pd.read_csv(file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading mirai file {file}: {e}\")\n",
        "mirai_df = pd.concat(mirai_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Number of benign samples: {len(benign_df)}\")\n",
        "print(f\"Number of Mirai samples: {len(mirai_df)}\")\n",
        "\n",
        "# --- 2. Labeling and Initial Combined DataFrame ---\n",
        "print(\"\\n--- Step 2: Labeling and Combining DataFrames ---\")\n",
        "benign_df['label'] = 0  # 0 for benign\n",
        "mirai_df['label'] = 1   # 1 for Mirai\n",
        "\n",
        "# Combine the benign and mirai dataframes into a single dataframe named df\n",
        "df = pd.concat([benign_df, mirai_df], ignore_index=True)\n",
        "\n",
        "print(\"Combined DataFrame shape:\", df.shape)\n",
        "print(\"Combined DataFrame head:\")\n",
        "print(df.head())\n",
        "print(\"Combined DataFrame label distribution:\\n\", df['label'].value_counts())\n",
        "\n",
        "# --- 3. Data Type Conversion and Handling Missing/Infinite Values ---\n",
        "# This is crucial to ensure all intended numerical columns are actually numbers.\n",
        "print(\"\\n--- Step 3: Data Type Conversion and Handling Missing/Infinite Values ---\")\n",
        "\n",
        "# Define all columns that should be numerical (excluding IDs, Timestamp, and labels)\n",
        "all_potential_numerical_cols = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
        "    'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
        "    'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
        "    'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "    'Flow Bytes/s', 'Flow Packets/s',\n",
        "    'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', # Added these based on your full column list\n",
        "    'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "    'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
        "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
        "    'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "    'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
        "    'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
        "    'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "    'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
        "    'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
        "    'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
        "    'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
        "    'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
        "    'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "    'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
        "    'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "# Ensure only columns present in df are selected\n",
        "numerical_cols_to_process = [col for col in all_potential_numerical_cols if col in df.columns]\n",
        "\n",
        "# Convert columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols_to_process:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Replace infinite values with NaN (important after conversion)\n",
        "df[numerical_cols_to_process] = df[numerical_cols_to_process].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print(\"Number of infinite values after conversion:\")\n",
        "print(np.isinf(df[numerical_cols_to_process]).sum().sum()) # Sum all infs across columns\n",
        "print(\"\\nNumber of NaN values after conversion:\")\n",
        "print(df[numerical_cols_to_process].isna().sum().sum()) # Sum all NaNs across columns\n",
        "\n",
        "# Impute missing values (using median for numerical columns)\n",
        "imputer_numerical = SimpleImputer(strategy='median')\n",
        "df[numerical_cols_to_process] = imputer_numerical.fit_transform(df[numerical_cols_to_process])\n",
        "\n",
        "print(\"\\nData types after numerical conversion and imputation:\")\n",
        "print(df[numerical_cols_to_process].dtypes.value_counts()) # Should mostly be float64\n",
        "\n",
        "# --- 4. Preprocessing with ColumnTransformer (Revised) ---\n",
        "print(\"\\n--- Step 4: Preprocessing with ColumnTransformer (Revised) ---\")\n",
        "\n",
        "# Define categorical columns (Corrected NameError)\n",
        "categorical_cols_to_process = ['Protocol'] # Assuming 'Protocol' is your categorical column\n",
        "\n",
        "# Identify columns that should be passed through (IDs, IPs, Timestamp, original Label)\n",
        "# These are NOT features for the GNN, but needed for graph construction or later.\n",
        "passthrough_cols_for_graph = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Label', 'label']\n",
        "passthrough_cols_for_graph = [col for col in passthrough_cols_for_graph if col in df.columns]\n",
        "\n",
        "# Define the preprocessor - ONLY transform numerical and categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols_to_process),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_to_process)\n",
        "    ],\n",
        "    remainder='drop' # Crucial: Drop other columns from the transformed array\n",
        ")\n",
        "\n",
        "# Apply the preprocessing - this array ONLY contains numerical and OHE features\n",
        "transformed_features_array = preprocessor.fit_transform(df)\n",
        "\n",
        "# Create df_processed for features only\n",
        "# Ensure it's float32 from the start\n",
        "df_processed_features = pd.DataFrame(transformed_features_array.astype(np.float32))\n",
        "\n",
        "# Get names of transformed numerical features\n",
        "transformed_numerical_cols = numerical_cols_to_process\n",
        "\n",
        "# Get names of one-hot encoded categorical features\n",
        "ohe_feature_names = []\n",
        "if 'cat' in preprocessor.named_transformers_ and hasattr(preprocessor.named_transformers_['cat'], 'categories_'):\n",
        "    # Iterate through categories for each categorical column\n",
        "    for i, col_name in enumerate(categorical_cols_to_process):\n",
        "        if i < len(preprocessor.named_transformers_['cat'].categories_):\n",
        "            categories = preprocessor.named_transformers_['cat'].categories_[i]\n",
        "            ohe_feature_names.extend([f\"{col_name}_{cat}\" for cat in categories])\n",
        "        else:\n",
        "            print(f\"Warning: Could not get categories for {col_name}. Skipping one-hot encoded column naming for this feature.\")\n",
        "else:\n",
        "    print(\"Warning: 'cat' transformer or its 'categories_' attribute not found. One-hot encoded column naming skipped.\")\n",
        "\n",
        "# Assign column names to the features DataFrame\n",
        "df_processed_features.columns = transformed_numerical_cols + ohe_feature_names\n",
        "\n",
        "# --- 5. Combine Processed Features with Necessary Passthrough Columns (Revised) ---\n",
        "print(\"\\n--- Step 5: Combining Processed Features with Passthrough Columns ---\")\n",
        "\n",
        "# Create a new df_processed that combines features and the original ID/IP/Timestamp/Label columns\n",
        "df_processed = pd.concat([\n",
        "    df_processed_features,\n",
        "    df[passthrough_cols_for_graph].reset_index(drop=True) # Ensure indices align\n",
        "], axis=1)\n",
        "\n",
        "# Ensure 'label' column is correctly identified and is integer type\n",
        "if 'label' in df_processed.columns:\n",
        "    # Coerce to numeric first, then to int, handling any potential non-numeric values\n",
        "    df_processed['label'] = pd.to_numeric(df_processed['label'], errors='coerce').fillna(-1).astype(int) # Fillna with -1 or another indicator if needed\n",
        "    # After this, remove rows where label became -1 due to coercion\n",
        "    df_processed = df_processed[df_processed['label'] != -1]\n",
        "else:\n",
        "    print(\"Error: 'label' column not found in df_processed. Check original column names or passthrough_cols_for_graph.\")\n",
        "\n",
        "# Drop the original 'Label' column if it exists and is different from our 'label'\n",
        "if 'Label' in df_processed.columns and 'Label' != 'label':\n",
        "    df_processed = df_processed.drop(columns=['Label'])\n",
        "\n",
        "# --- IMPORTANT: Convert Timestamp to datetime objects for graph construction ---\n",
        "if 'Timestamp' in df_processed.columns:\n",
        "    df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'], errors='coerce')\n",
        "    if df_processed['Timestamp'].isnull().any():\n",
        "        print(\"Warning: Some Timestamps could not be converted to datetime. These rows might be excluded from temporal graph connections.\")\n",
        "else:\n",
        "    print(\"Error: 'Timestamp' column not found in df_processed. Temporal graph construction will not be possible.\")\n",
        "\n",
        "\n",
        "print(\"Processed DataFrame shape:\", df_processed.shape)\n",
        "print(\"Processed DataFrame head:\")\n",
        "print(df_processed.head())\n",
        "print(\"Processed DataFrame dtypes (should be mostly float/int for features, object for IDs/IPs):\")\n",
        "print(df_processed.dtypes.value_counts()) # Check dtypes again\n",
        "\n",
        "\n",
        "# --- 6. Extract Features (x) and Labels (y) for PyG ---\n",
        "print(\"\\n--- Step 6: Extracting Features (x) and Labels (y) for PyG ---\")\n",
        "label_col_name = 'label'\n",
        "# Columns to exclude from features (IDs, Timestamps, and labels)\n",
        "cols_to_exclude_from_features_pyg = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', label_col_name]\n",
        "\n",
        "feature_cols = [col for col in df_processed.columns if col not in cols_to_exclude_from_features_pyg]\n",
        "\n",
        "# Convert features to PyTorch tensor (float32 is standard for model inputs)\n",
        "x = torch.tensor(df_processed[feature_cols].values.astype(np.float32), dtype=torch.float)\n",
        "\n",
        "# Convert labels to PyTorch tensor (long is required for classification labels)\n",
        "y = torch.tensor(df_processed[label_col_name].values, dtype=torch.long)\n",
        "\n",
        "print(f\"Features (x) shape: {x.shape}\")\n",
        "print(f\"Labels (y) shape: {y.shape}\")\n",
        "\n",
        "\n",
        "# --- 7. Graph Construction (ULTRA Memory-Efficient: Sequential Connections per (Src IP, Dst IP) pair) ---\n",
        "print(\"\\n--- Step 7: Graph Construction (Sequential Connections per (Src IP, Dst IP) pair) ---\")\n",
        "edges = []\n",
        "num_nodes = x.size(0)\n",
        "\n",
        "# Create a temporary DataFrame for easier sorting and grouping\n",
        "# This will be used to get the original indices for edge creation\n",
        "df_temp_graph = df_processed[['Src IP', 'Dst IP', 'Timestamp']].copy()\n",
        "df_temp_graph['original_index'] = df_processed.index\n",
        "\n",
        "# Drop rows with NaT in Timestamp for sorting, as NaT cannot be sorted\n",
        "df_temp_graph.dropna(subset=['Timestamp'], inplace=True)\n",
        "\n",
        "# Sort by Src IP, Dst IP, and Timestamp to ensure temporal order within each unique flow path\n",
        "df_temp_graph = df_temp_graph.sort_values(by=['Src IP', 'Dst IP', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# Iterate through groups of (Src IP, Dst IP) pairs\n",
        "# This will create a series of small, sequential graphs\n",
        "for (src_ip, dst_ip), group in df_temp_graph.groupby(['Src IP', 'Dst IP']):\n",
        "    node_indices = group['original_index'].tolist()\n",
        "\n",
        "    # Connect consecutive flows in this sequence\n",
        "    for i in range(len(node_indices) - 1):\n",
        "        node1 = node_indices[i]\n",
        "        node2 = node_indices[i+1]\n",
        "        edges.append([node1, node2])\n",
        "        edges.append([node2, node1]) # Add reverse edge for undirected graph\n",
        "\n",
        "# Convert the list of edges to a PyTorch tensor\n",
        "if edges:\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "else:\n",
        "    edge_index = torch.empty((2, 0), dtype=torch.long) # Handle case with no edges\n",
        "\n",
        "print(f\"Generated edge_index shape: {edge_index.shape} (Number of edges: {edge_index.shape[1]})\")\n",
        "\n",
        "\n",
        "# --- 8. Create Train/Test Masks (using sklearn for robust splitting) ---\n",
        "print(\"\\n--- Step 8: Creating Train/Test Masks ---\")\n",
        "if num_nodes > 0:\n",
        "    indices = np.arange(num_nodes)\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.2, # 20% for testing\n",
        "        random_state=42, # For reproducibility\n",
        "        stratify=y.cpu().numpy() # Ensures similar class distribution in both sets\n",
        "    )\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_indices] = True\n",
        "    test_mask[test_indices] = True\n",
        "else:\n",
        "    train_mask = torch.zeros(0, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(0, dtype=torch.bool)\n",
        "\n",
        "print(f\"Number of training samples: {train_mask.sum()}\")\n",
        "print(f\"Number of test samples: {test_mask.sum()}\")\n",
        "\n",
        "\n",
        "# --- 9. Create the PyTorch Geometric Data object ---\n",
        "print(\"\\n--- Step 9: Creating PyTorch Geometric Data object ---\")\n",
        "data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "print(\"\\nPyG Data object created:\")\n",
        "print(data)\n",
        "print(f\"Number of nodes: {data.num_nodes}\")\n",
        "print(f\"Number of edges: {data.num_edges}\")\n",
        "print(f\"Number of features: {data.num_features}\")\n",
        "print(f\"Number of classes: {data.y.unique().size(0)}\")\n",
        "\n",
        "\n",
        "# --- 10. GraphSAGE Model Definition ---\n",
        "print(\"\\n--- Step 10: GraphSAGE Model Definition ---\")\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# --- 11. Training and Testing Functions ---\n",
        "print(\"\\n--- Step 11: Training and Testing Functions Defined ---\")\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    predicted_labels = out.argmax(dim=1)\n",
        "    true_labels = data.y\n",
        "\n",
        "    y_true_train = true_labels[data.train_mask].cpu().numpy()\n",
        "    y_pred_train = predicted_labels[data.train_mask].cpu().numpy()\n",
        "    print(\"\\n--- Training Set Classification Report ---\")\n",
        "    if len(y_true_train) > 0:\n",
        "        print(classification_report(y_true_train, y_pred_train, zero_division=0))\n",
        "    else:\n",
        "        print(\"No samples in training set mask.\")\n",
        "\n",
        "    y_true_test = true_labels[data.test_mask].cpu().numpy()\n",
        "    y_pred_test = predicted_labels[data.test_mask].cpu().numpy()\n",
        "    print(\"\\n--- Test Set Classification Report ---\")\n",
        "    if len(y_true_test) > 0:\n",
        "        print(classification_report(y_true_test, y_pred_test, zero_division=0))\n",
        "    else:\n",
        "        print(\"No samples in test set mask.\")\n",
        "\n",
        "\n",
        "# --- 12. Model Initialization and Training Loop ---\n",
        "print(\"\\n--- Step 12: Model Initialization and Training Loop ---\")\n",
        "in_channels = data.num_features\n",
        "out_channels = data.y.unique().size(0)\n",
        "\n",
        "if in_channels == 0:\n",
        "    print(\"Error: No features detected. Please check 'feature_cols'.\")\n",
        "elif out_channels < 2:\n",
        "    print(f\"Error: Only {out_channels} class(es) detected. Need at least 2 for classification.\")\n",
        "elif data.num_nodes == 0:\n",
        "    print(\"Error: No nodes in the graph. Cannot train model.\")\n",
        "else:\n",
        "    model = GraphSAGE(in_channels=in_channels, hidden_channels=128, out_channels=out_channels)\n",
        "\n",
        "    # --- Addressing Class Imbalance: Calculate Class Weights ---\n",
        "    class_counts = df['label'].value_counts().sort_index()\n",
        "    total_samples = class_counts.sum()\n",
        "\n",
        "    # Calculate inverse class weights: N_total / (num_classes * N_k)\n",
        "    # This gives higher weight to the minority class\n",
        "    class_weights = total_samples / (len(class_counts) * class_counts.values)\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "    print(f\"\\nCalculated class weights: {weights_tensor.tolist()}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor) # Use weighted loss\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(1, 101):\n",
        "        loss = train(model, data, optimizer, criterion)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "    print(\"\\nTraining complete. Running final evaluation...\")\n",
        "    test(model, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FnP9XPWRr-b",
        "outputId": "eb8ea225-092a-45fa-bca9-9d1a70153507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--- Step 1: Data Loading ---\n",
            "Found 1 benign files and 5 mirai files.\n",
            "Number of benign samples: 84526\n",
            "Number of Mirai samples: 33758\n",
            "\n",
            "--- Step 2: Labeling and Combining DataFrames ---\n",
            "Combined DataFrame shape: (118284, 85)\n",
            "Combined DataFrame head:\n",
            "                                     Flow ID           Src IP  Src Port  \\\n",
            "0    23.78.206.51-192.168.137.253-80-40940-6     23.78.206.51        80   \n",
            "1  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253     51030   \n",
            "2  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253     51030   \n",
            "3    192.168.137.253-23.78.206.51-40944-80-6  192.168.137.253     40944   \n",
            "4   192.168.137.187-18.207.33.55-39741-443-6  192.168.137.187     39741   \n",
            "\n",
            "            Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  \\\n",
            "0  192.168.137.253     40940         6  07/10/2022 11:30:28 PM         114733   \n",
            "1    54.148.103.72       443         6  07/10/2022 11:30:35 PM         780805   \n",
            "2    54.148.103.72       443         6  07/10/2022 11:30:35 PM           3721   \n",
            "3     23.78.206.51        80         6  07/10/2022 11:30:35 PM         530843   \n",
            "4     18.207.33.55       443         6  07/10/2022 11:30:36 PM         407436   \n",
            "\n",
            "   Total Fwd Packet  Total Bwd packets  ...  Active Mean  Active Std  \\\n",
            "0                45                172  ...          0.0         0.0   \n",
            "1                14                  7  ...          0.0         0.0   \n",
            "2                 2                  0  ...          0.0         0.0   \n",
            "3               123                108  ...          0.0         0.0   \n",
            "4                11                  4  ...          0.0         0.0   \n",
            "\n",
            "   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \\\n",
            "0         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "1         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "2         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "3         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "4         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "\n",
            "             Label  label  \n",
            "0  NeedManualLabel      0  \n",
            "1  NeedManualLabel      0  \n",
            "2  NeedManualLabel      0  \n",
            "3  NeedManualLabel      0  \n",
            "4  NeedManualLabel      0  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "Combined DataFrame label distribution:\n",
            " label\n",
            "0    84526\n",
            "1    33758\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Step 3: Data Type Conversion and Handling Missing/Infinite Values ---\n",
            "Number of infinite values after conversion:\n",
            "0\n",
            "\n",
            "Number of NaN values after conversion:\n",
            "88\n",
            "\n",
            "Data types after numerical conversion and imputation:\n",
            "float64    76\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Step 4: Preprocessing with ColumnTransformer (Revised) ---\n",
            "\n",
            "--- Step 5: Combining Processed Features with Passthrough Columns ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-1c9bc5d7396a>:186: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed DataFrame shape: (118284, 84)\n",
            "Processed DataFrame head:\n",
            "   Flow Duration  Total Fwd Packet  Total Bwd packets  \\\n",
            "0      -0.608945         -0.032014           0.378796   \n",
            "1      -0.593817         -0.041258          -0.020484   \n",
            "2      -0.611466         -0.044836          -0.037423   \n",
            "3      -0.599494         -0.008756           0.223924   \n",
            "4      -0.602297         -0.042152          -0.027744   \n",
            "\n",
            "   Total Length of Fwd Packet  Total Length of Bwd Packet  \\\n",
            "0                    0.468405                   -0.012989   \n",
            "1                   -0.040247                    0.013221   \n",
            "2                   -0.053652                   -0.012989   \n",
            "3                   -0.051197                    1.211312   \n",
            "4                   -0.048647                    0.003557   \n",
            "\n",
            "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
            "0               3.676065              -0.540546               15.059868   \n",
            "1               1.210733              -0.540546                0.812521   \n",
            "2              -0.198538              -0.540546               -0.469124   \n",
            "3              -0.013906              -0.540546               -0.442409   \n",
            "4               0.218000              -0.540546                0.139864   \n",
            "\n",
            "   Fwd Packet Length Std  Bwd Packet Length Max  ...  Idle Max  Idle Min  \\\n",
            "0               7.155258              -0.327351  ... -0.479758 -0.443881   \n",
            "1               2.796088               4.017415  ... -0.479758 -0.443881   \n",
            "2              -0.251263              -0.327351  ... -0.479758 -0.443881   \n",
            "3              -0.055914               4.017415  ... -0.479758 -0.443881   \n",
            "4               0.817114               4.808054  ... -0.479758 -0.443881   \n",
            "\n",
            "   Protocol_0  Protocol_6  Protocol_17  \\\n",
            "0         0.0         1.0          0.0   \n",
            "1         0.0         1.0          0.0   \n",
            "2         0.0         1.0          0.0   \n",
            "3         0.0         1.0          0.0   \n",
            "4         0.0         1.0          0.0   \n",
            "\n",
            "                                     Flow ID           Src IP  \\\n",
            "0    23.78.206.51-192.168.137.253-80-40940-6     23.78.206.51   \n",
            "1  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253   \n",
            "2  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253   \n",
            "3    192.168.137.253-23.78.206.51-40944-80-6  192.168.137.253   \n",
            "4   192.168.137.187-18.207.33.55-39741-443-6  192.168.137.187   \n",
            "\n",
            "            Dst IP           Timestamp  label  \n",
            "0  192.168.137.253 2022-07-10 23:30:28      0  \n",
            "1    54.148.103.72 2022-07-10 23:30:35      0  \n",
            "2    54.148.103.72 2022-07-10 23:30:35      0  \n",
            "3     23.78.206.51 2022-07-10 23:30:35      0  \n",
            "4     18.207.33.55 2022-07-10 23:30:36      0  \n",
            "\n",
            "[5 rows x 84 columns]\n",
            "Processed DataFrame dtypes (should be mostly float/int for features, object for IDs/IPs):\n",
            "float32           79\n",
            "object             3\n",
            "datetime64[ns]     1\n",
            "int64              1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Step 6: Extracting Features (x) and Labels (y) for PyG ---\n",
            "Features (x) shape: torch.Size([118284, 79])\n",
            "Labels (y) shape: torch.Size([118284])\n",
            "\n",
            "--- Step 7: Graph Construction (Sequential Connections per (Src IP, Dst IP) pair) ---\n",
            "Generated edge_index shape: torch.Size([2, 230356]) (Number of edges: 230356)\n",
            "\n",
            "--- Step 8: Creating Train/Test Masks ---\n",
            "Number of training samples: 94627\n",
            "Number of test samples: 23657\n",
            "\n",
            "--- Step 9: Creating PyTorch Geometric Data object ---\n",
            "\n",
            "PyG Data object created:\n",
            "Data(x=[118284, 79], edge_index=[2, 230356], y=[118284], train_mask=[118284], test_mask=[118284])\n",
            "Number of nodes: 118284\n",
            "Number of edges: 230356\n",
            "Number of features: 79\n",
            "Number of classes: 2\n",
            "\n",
            "--- Step 10: GraphSAGE Model Definition ---\n",
            "\n",
            "--- Step 11: Training and Testing Functions Defined ---\n",
            "\n",
            "--- Step 12: Model Initialization and Training Loop ---\n",
            "\n",
            "Calculated class weights: [0.699690043926239, 1.7519402503967285]\n",
            "\n",
            "Starting training...\n",
            "Epoch: 010, Loss: 0.5479\n",
            "Epoch: 020, Loss: 0.4899\n",
            "Epoch: 030, Loss: 0.4499\n",
            "Epoch: 040, Loss: 0.4229\n",
            "Epoch: 050, Loss: 0.3965\n",
            "Epoch: 060, Loss: 0.3887\n",
            "Epoch: 070, Loss: 0.3691\n",
            "Epoch: 080, Loss: 0.3493\n",
            "Epoch: 090, Loss: 0.3367\n",
            "Epoch: 100, Loss: 0.3261\n",
            "\n",
            "Training complete. Running final evaluation...\n",
            "\n",
            "--- Training Set Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.86      0.90     67621\n",
            "           1       0.72      0.88      0.79     27006\n",
            "\n",
            "    accuracy                           0.87     94627\n",
            "   macro avg       0.83      0.87      0.85     94627\n",
            "weighted avg       0.88      0.87      0.87     94627\n",
            "\n",
            "\n",
            "--- Test Set Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.87      0.91     16905\n",
            "           1       0.73      0.88      0.80      6752\n",
            "\n",
            "    accuracy                           0.87     23657\n",
            "   macro avg       0.84      0.87      0.85     23657\n",
            "weighted avg       0.88      0.87      0.87     23657\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import gc # Import garbage collector\n",
        "\n",
        "# --- 1. Data Loading ---\n",
        "print(\"--- Step 1: Data Loading ---\")\n",
        "benign_data_dir = \"/content/drive/My Drive/ids/iot/benign/\"\n",
        "mirai_data_dir = \"/content/drive/My Drive/ids/iot/mirai/\"\n",
        "\n",
        "benign_files = [f\"{benign_data_dir}{file}\" for file in os.listdir(benign_data_dir) if file.endswith(\".csv\")]\n",
        "mirai_files = [f\"{mirai_data_dir}{file}\" for file in os.listdir(mirai_data_dir) if file.endswith(\".csv\")]\n",
        "\n",
        "print(f\"Found {len(benign_files)} benign files and {len(mirai_files)} mirai files.\")\n",
        "\n",
        "# Load and concatenate benign data\n",
        "benign_dfs = []\n",
        "for file in benign_files:\n",
        "    try:\n",
        "        benign_dfs.append(pd.read_csv(file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading benign file {file}: {e}\")\n",
        "benign_df = pd.concat(benign_dfs, ignore_index=True)\n",
        "\n",
        "# Load and concatenate mirai data\n",
        "mirai_dfs = []\n",
        "for file in mirai_files:\n",
        "    try:\n",
        "        mirai_dfs.append(pd.read_csv(file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading mirai file {file}: {e}\")\n",
        "mirai_df = pd.concat(mirai_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Number of benign samples: {len(benign_df)}\")\n",
        "print(f\"Number of Mirai samples: {len(mirai_df)}\")\n",
        "\n",
        "# --- 2. Labeling and Initial Combined DataFrame ---\n",
        "print(\"\\n--- Step 2: Labeling and Combining DataFrames ---\")\n",
        "benign_df['label'] = 0\n",
        "mirai_df['label'] = 1\n",
        "\n",
        "df = pd.concat([benign_df, mirai_df], ignore_index=True)\n",
        "\n",
        "print(\"Combined DataFrame shape:\", df.shape)\n",
        "print(\"Combined DataFrame head:\")\n",
        "print(df.head())\n",
        "print(\"Combined DataFrame label distribution:\\n\", df['label'].value_counts())\n",
        "\n",
        "# --- 3. Data Type Conversion and Handling Missing/Infinite Values ---\n",
        "print(\"\\n--- Step 3: Data Type Conversion and Handling Missing/Infinite Values ---\")\n",
        "\n",
        "all_potential_numerical_cols = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
        "    'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
        "    'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
        "    'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "    'Flow Bytes/s', 'Flow Packets/s',\n",
        "    'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "    'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
        "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
        "    'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "    'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
        "    'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
        "    'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "    'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
        "    'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
        "    'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
        "    'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
        "    'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
        "    'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "    'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
        "    'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "numerical_cols_to_process = [col for col in all_potential_numerical_cols if col in df.columns]\n",
        "\n",
        "for col in numerical_cols_to_process:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df[numerical_cols_to_process] = df[numerical_cols_to_process].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print(\"Number of infinite values after conversion:\")\n",
        "print(np.isinf(df[numerical_cols_to_process]).sum().sum())\n",
        "print(\"\\nNumber of NaN values after conversion:\")\n",
        "print(df[numerical_cols_to_process].isna().sum().sum())\n",
        "\n",
        "imputer_numerical = SimpleImputer(strategy='median')\n",
        "df[numerical_cols_to_process] = imputer_numerical.fit_transform(df[numerical_cols_to_process])\n",
        "\n",
        "print(\"\\nData types after numerical conversion and imputation:\")\n",
        "print(df[numerical_cols_to_process].dtypes.value_counts())\n",
        "\n",
        "# --- 4. Preprocessing with ColumnTransformer (Revised) ---\n",
        "print(\"\\n--- Step 4: Preprocessing with ColumnTransformer (Revised) ---\")\n",
        "\n",
        "categorical_cols = ['Protocol']\n",
        "categorical_cols_to_process = [col for col in categorical_cols if col in df.columns]\n",
        "\n",
        "passthrough_cols_for_graph = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Label', 'label']\n",
        "passthrough_cols_for_graph = [col for col in passthrough_cols_for_graph if col in df.columns]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols_to_process),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_to_process)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "transformed_features_array = preprocessor.fit_transform(df)\n",
        "df_processed_features = pd.DataFrame(transformed_features_array.astype(np.float32))\n",
        "\n",
        "transformed_numerical_cols = numerical_cols_to_process\n",
        "ohe_feature_names = []\n",
        "if 'cat' in preprocessor.named_transformers_:\n",
        "    for i, col_name in enumerate(categorical_cols_to_process):\n",
        "        if hasattr(preprocessor.named_transformers_['cat'], 'categories_') and i < len(preprocessor.named_transformers_['cat'].categories_):\n",
        "            categories = preprocessor.named_transformers_['cat'].categories_[i]\n",
        "            ohe_feature_names.extend([f\"{col_name}_{cat}\" for cat in categories])\n",
        "        else:\n",
        "            print(f\"Warning: Could not get categories for {col_name}. Skipping one-hot encoded column naming for this feature.\")\n",
        "\n",
        "df_processed_features.columns = transformed_numerical_cols + ohe_feature_names\n",
        "\n",
        "# --- 5. Combine Processed Features with Necessary Passthrough Columns (Revised) ---\n",
        "print(\"\\n--- Step 5: Combining Processed Features with Passthrough Columns ---\")\n",
        "\n",
        "df_processed = pd.concat([\n",
        "    df_processed_features,\n",
        "    df[passthrough_cols_for_graph].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "if 'label' in df_processed.columns:\n",
        "    df_processed['label'] = df_processed['label'].astype(int)\n",
        "else:\n",
        "    print(\"Error: 'label' column not found in df_processed. Check original column names or passthrough_cols_for_graph.\")\n",
        "\n",
        "if 'Label' in df_processed.columns and 'Label' != 'label':\n",
        "    df_processed = df_processed.drop(columns=['Label'])\n",
        "\n",
        "if 'Timestamp' in df_processed.columns:\n",
        "    df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'], errors='coerce')\n",
        "    if df_processed['Timestamp'].isnull().any():\n",
        "        print(\"Warning: Some Timestamps could not be converted to datetime. These rows might be excluded from temporal graph connections.\")\n",
        "else:\n",
        "    print(\"Error: 'Timestamp' column not found in df_processed. Temporal graph construction will not be possible.\")\n",
        "\n",
        "\n",
        "print(\"Processed DataFrame shape:\", df_processed.shape)\n",
        "print(\"Processed DataFrame head:\")\n",
        "print(df_processed.head())\n",
        "print(\"Processed DataFrame dtypes (should be mostly float/int for features, object for IDs/IPs):\")\n",
        "print(df_processed.dtypes.value_counts())\n",
        "\n",
        "\n",
        "# --- 6. Extract Features (x) and Labels (y) for PyG (GLOBAL) ---\n",
        "print(\"\\n--- Step 6: Extracting Features (x) and Labels (y) for PyG (GLOBAL) ---\")\n",
        "label_col_name = 'label'\n",
        "cols_to_exclude_from_features_pyg = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', label_col_name]\n",
        "\n",
        "feature_cols = [col for col in df_processed.columns if col not in cols_to_exclude_from_features_pyg]\n",
        "\n",
        "x_global = torch.tensor(df_processed[feature_cols].values.astype(np.float32), dtype=torch.float)\n",
        "y_global = torch.tensor(df_processed[label_col_name].values, dtype=torch.long)\n",
        "\n",
        "print(f\"Global Features (x_global) shape: {x_global.shape}\")\n",
        "print(f\"Global Labels (y_global) shape: {y_global.shape}\")\n",
        "\n",
        "\n",
        "# --- 7. Graph Construction for Causal Sampling (Creating data_list) ---\n",
        "print(\"\\n--- Step 7: Graph Construction for Causal Sampling (Creating data_list) ---\")\n",
        "\n",
        "df_processed_sorted = df_processed.sort_values(by='Timestamp').reset_index(drop=False)\n",
        "\n",
        "data_list = []\n",
        "# Adjusted time window parameters for smaller snapshots\n",
        "window_duration = timedelta(minutes=10) # Smaller window\n",
        "stride_duration = timedelta(minutes=5)  # Overlapping stride\n",
        "\n",
        "start_time = df_processed_sorted['Timestamp'].min()\n",
        "end_time = df_processed_sorted['Timestamp'].max()\n",
        "\n",
        "current_window_start = start_time\n",
        "\n",
        "window_count = 0\n",
        "while current_window_start <= end_time:\n",
        "    window_end = current_window_start + window_duration\n",
        "\n",
        "    window_df = df_processed_sorted[\n",
        "        (df_processed_sorted['Timestamp'] >= current_window_start) &\n",
        "        (df_processed_sorted['Timestamp'] < window_end)\n",
        "    ].copy()\n",
        "\n",
        "    if not window_df.empty:\n",
        "        window_count += 1\n",
        "        print(f\"Processing window {window_count}: {current_window_start} to {window_end} (Nodes: {len(window_df)})\")\n",
        "\n",
        "        window_original_indices = window_df['index'].values\n",
        "        x_window = x_global[window_original_indices]\n",
        "        y_window = y_global[window_original_indices]\n",
        "\n",
        "        edges_window = []\n",
        "        original_to_local_idx = {original_idx: local_idx for local_idx, original_idx in enumerate(window_original_indices)}\n",
        "\n",
        "        if 'Src IP' in window_df.columns and 'Dst IP' in window_df.columns:\n",
        "            for (src_ip, dst_ip), group in window_df.groupby(['Src IP', 'Dst IP']):\n",
        "                group_original_indices = group['index'].tolist()\n",
        "\n",
        "                for i in range(len(group_original_indices) - 1):\n",
        "                    node1_original = group_original_indices[i]\n",
        "                    node2_original = group_original_indices[i+1]\n",
        "\n",
        "                    node1_local = original_to_local_idx[node1_original]\n",
        "                    node2_local = original_to_local_idx[node2_original]\n",
        "\n",
        "                    edges_window.append([node1_local, node2_local])\n",
        "                    edges_window.append([node2_local, node1_local])\n",
        "        else:\n",
        "            print(\"Warning: 'Src IP' or 'Dst IP' not found in window_df. Cannot build IP-based graph for this window.\")\n",
        "\n",
        "        if edges_window:\n",
        "            edge_index_window = torch.tensor(edges_window, dtype=torch.long).t().contiguous()\n",
        "        else:\n",
        "            edge_index_window = torch.empty((2, 0), dtype=torch.long)\n",
        "\n",
        "        data_window = Data(x=x_window, edge_index=edge_index_window, y=y_window)\n",
        "        data_list.append(data_window)\n",
        "\n",
        "    current_window_start += stride_duration\n",
        "\n",
        "print(f\"\\nCreated {len(data_list)} graph snapshots (Data objects) for causal training.\")\n",
        "if len(data_list) > 0:\n",
        "    print(f\"Example snapshot 0: {data_list[0]}\")\n",
        "    print(f\"Example snapshot 1: {data_list[1]}\")\n",
        "\n",
        "\n",
        "# --- 8. GraphSAGE Model Definition ---\n",
        "print(\"\\n--- Step 8: GraphSAGE Model Definition ---\")\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# --- 9. Training and Testing Functions (Adapted for causal loop) ---\n",
        "print(\"\\n--- Step 9: Training and Testing Functions Defined ---\")\n",
        "\n",
        "# --- 10. Model Initialization and Causal Training Loop ---\n",
        "print(\"\\n--- Step 10: Model Initialization and Causal Training Loop ---\")\n",
        "\n",
        "if not data_list:\n",
        "    print(\"Error: No graph snapshots created. Cannot proceed with training.\")\n",
        "else:\n",
        "    in_channels = x_global.shape[1]\n",
        "    out_channels = len(y_global.unique())\n",
        "\n",
        "    if in_channels == 0:\n",
        "        print(\"Error: No features detected in snapshots. Check feature_cols.\")\n",
        "    elif out_channels < 2:\n",
        "        print(f\"Error: Only {out_channels} class(es) detected. Need at least 2 for classification.\")\n",
        "    else:\n",
        "        model = GraphSAGE(in_channels=in_channels, hidden_channels=64, out_channels=out_channels) # Hidden channels reduced\n",
        "\n",
        "        class_counts = df['label'].value_counts().sort_index()\n",
        "        total_samples = class_counts.sum()\n",
        "\n",
        "        weights_array = np.zeros(out_channels)\n",
        "        for class_label, count in class_counts.items():\n",
        "            if count > 0:\n",
        "                weights_array[class_label] = total_samples / (out_channels * count)\n",
        "            else:\n",
        "                weights_array[class_label] = total_samples\n",
        "\n",
        "        weights_tensor = torch.tensor(weights_array, dtype=torch.float)\n",
        "\n",
        "        print(f\"\\nCalculated global class weights: {weights_tensor.tolist()}\")\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "\n",
        "        print(\"\\nStarting causal training...\")\n",
        "        num_epochs = 20\n",
        "\n",
        "        model.to(\"cpu\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct_epoch = 0\n",
        "            total_epoch = 0\n",
        "\n",
        "            if len(data_list) < 2:\n",
        "                print(\"Warning: Less than 2 windows available for causal training. Skipping epoch.\")\n",
        "                break\n",
        "\n",
        "            for t in range(1, len(data_list)):\n",
        "                # Print current window being processed\n",
        "                print(f\"\\rEpoch {epoch+1}/{num_epochs}, Processing window {t}/{len(data_list)-1}...\", end='', flush=True)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                past_data = data_list[t - 1].to(\"cpu\")\n",
        "                current_data = data_list[t].to(\"cpu\")\n",
        "\n",
        "                if past_data.y.nelement() == 0 or current_data.y.nelement() == 0:\n",
        "                    print(f\"Skipping window {t} due to empty past_data.y or current_data.y\")\n",
        "                    continue\n",
        "\n",
        "                # Forward pass and loss calculation for past data\n",
        "                out_past = model(past_data.x, past_data.edge_index)\n",
        "                loss_past = criterion(out_past, past_data.y)\n",
        "\n",
        "                # Forward pass and loss calculation for current data\n",
        "                out_current = model(current_data.x, current_data.edge_index)\n",
        "                loss_current = criterion(out_current, current_data.y)\n",
        "\n",
        "                loss = loss_past + loss_current\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if current_data.y.nelement() > 0:\n",
        "                        preds = out_current.argmax(dim=1)\n",
        "                        correct_epoch += (preds == current_data.y).sum().item()\n",
        "                        total_epoch += current_data.y.size(0)\n",
        "                model.train()\n",
        "\n",
        "            avg_epoch_loss = total_loss / (len(data_list) - 1) if (len(data_list) - 1) > 0 else total_loss\n",
        "            epoch_accuracy = correct_epoch / total_epoch if total_epoch > 0 else 0\n",
        "\n",
        "            losses.append(avg_epoch_loss)\n",
        "            accuracies.append(epoch_accuracy * 100)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}, Causal Loss: {avg_epoch_loss:.4f}, Accuracy: {epoch_accuracy*100:.2f}%\")\n",
        "\n",
        "        print(\"\\nCausal training complete. Running final evaluation on all snapshots...\")\n",
        "\n",
        "        # --- Final Evaluation on all snapshots ---\n",
        "        model.eval()\n",
        "        all_true_labels = []\n",
        "        all_predicted_labels = []\n",
        "        with torch.no_grad():\n",
        "            for data_snapshot in data_list:\n",
        "                data_snapshot = data_snapshot.to(\"cpu\")\n",
        "\n",
        "                if data_snapshot.x.nelement() == 0 or data_snapshot.y.nelement() == 0:\n",
        "                    continue\n",
        "\n",
        "                out = model(data_snapshot.x, data_snapshot.edge_index)\n",
        "                preds = out.argmax(dim=1)\n",
        "                all_true_labels.extend(data_snapshot.y.cpu().numpy())\n",
        "                all_predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "        print(\"\\n--- Overall Classification Report (Evaluated on all snapshots) ---\")\n",
        "        if len(all_true_labels) > 0:\n",
        "            print(classification_report(all_true_labels, all_predicted_labels, zero_division=0))\n",
        "        else:\n",
        "            print(\"No samples for overall evaluation.\")\n",
        "\n",
        "        # --- Plotting ---\n",
        "        epochs_plot = list(range(1, num_epochs + 1))\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Loss plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_plot, losses, marker='o', color='blue')\n",
        "        plt.title('Causal Sampling - Loss Over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Accuracy plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_plot, accuracies, marker='s', color='green')\n",
        "        plt.title('Causal Sampling - Accuracy Over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "5_UEJKl-THun",
        "outputId": "9fd3b9e4-9e81-4ca1-8e3d-7a2b603d68a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7d698edce2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import copy\n",
        "import random\n",
        "\n",
        "# --- 1. Data Loading ---\n",
        "print(\"--- Step 1: Data Loading ---\")\n",
        "benign_data_dir = \"/content/drive/My Drive/ids/iot/benign/\"\n",
        "mirai_data_dir = \"/content/drive/My Drive/ids/iot/mirai/\"\n",
        "\n",
        "benign_files = [f\"{benign_data_dir}{file}\" for file in os.listdir(benign_data_dir) if file.endswith(\".csv\")]\n",
        "mirai_files = [f\"{mirai_data_dir}{file}\" for file in os.listdir(mirai_data_dir) if file.endswith(\".csv\")]\n",
        "\n",
        "print(f\"Found {len(benign_files)} benign files and {len(mirai_files)} mirai files.\")\n",
        "\n",
        "# Load and concatenate benign data\n",
        "benign_dfs = []\n",
        "for file in benign_files:\n",
        "    try:\n",
        "        benign_dfs.append(pd.read_csv(file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading benign file {file}: {e}\")\n",
        "benign_df = pd.concat(benign_dfs, ignore_index=True)\n",
        "\n",
        "# Load and concatenate mirai data\n",
        "mirai_dfs = []\n",
        "for file in mirai_files:\n",
        "    try:\n",
        "        mirai_dfs.append(pd.read_csv(file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading mirai file {file}: {e}\")\n",
        "mirai_df = pd.concat(mirai_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Number of benign samples: {len(benign_df)}\")\n",
        "print(f\"Number of Mirai samples: {len(mirai_df)}\")\n",
        "\n",
        "# --- 2. Labeling and Initial Combined DataFrame ---\n",
        "print(\"\\n--- Step 2: Labeling and Combining DataFrames ---\")\n",
        "benign_df['label'] = 0\n",
        "mirai_df['label'] = 1\n",
        "\n",
        "df = pd.concat([benign_df, mirai_df], ignore_index=True)\n",
        "\n",
        "print(\"Combined DataFrame shape:\", df.shape)\n",
        "print(\"Combined DataFrame head:\")\n",
        "print(df.head())\n",
        "print(\"Combined DataFrame label distribution:\\n\", df['label'].value_counts())\n",
        "\n",
        "# --- 3. Data Type Conversion and Handling Missing/Infinite Values ---\n",
        "print(\"\\n--- Step 3: Data Type Conversion and Handling Missing/Infinite Values ---\")\n",
        "\n",
        "all_potential_numerical_cols = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
        "    'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
        "    'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
        "    'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "    'Flow Bytes/s', 'Flow Packets/s',\n",
        "    'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "    'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
        "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
        "    'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "    'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
        "    'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
        "    'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "    'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
        "    'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
        "    'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
        "    'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
        "    'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
        "    'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "    'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
        "    'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "numerical_cols_to_process = [col for col in all_potential_numerical_cols if col in df.columns]\n",
        "\n",
        "for col in numerical_cols_to_process:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df[numerical_cols_to_process] = df[numerical_cols_to_process].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print(\"Number of infinite values after conversion:\")\n",
        "print(np.isinf(df[numerical_cols_to_process]).sum().sum())\n",
        "print(\"\\nNumber of NaN values after conversion:\")\n",
        "print(df[numerical_cols_to_process].isna().sum().sum())\n",
        "\n",
        "imputer_numerical = SimpleImputer(strategy='median')\n",
        "df[numerical_cols_to_process] = imputer_numerical.fit_transform(df[numerical_cols_to_process])\n",
        "\n",
        "print(\"\\nData types after numerical conversion and imputation:\")\n",
        "print(df[numerical_cols_to_process].dtypes.value_counts())\n",
        "\n",
        "# --- 4. Preprocessing with ColumnTransformer (Revised) ---\n",
        "print(\"\\n--- Step 4: Preprocessing with ColumnTransformer (Revised) ---\")\n",
        "\n",
        "categorical_cols = ['Protocol']\n",
        "categorical_cols_to_process = [col for col in categorical_cols if col in df.columns]\n",
        "\n",
        "passthrough_cols_for_graph = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Label', 'label']\n",
        "passthrough_cols_for_graph = [col for col in passthrough_cols_for_graph if col in df.columns]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols_to_process),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_to_process)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "transformed_features_array = preprocessor.fit_transform(df)\n",
        "df_processed_features = pd.DataFrame(transformed_features_array.astype(np.float32))\n",
        "\n",
        "transformed_numerical_cols = numerical_cols_to_process\n",
        "ohe_feature_names = []\n",
        "if 'cat' in preprocessor.named_transformers_:\n",
        "    for i, col_name in enumerate(categorical_cols_to_process):\n",
        "        if hasattr(preprocessor.named_transformers_['cat'], 'categories_') and i < len(preprocessor.named_transformers_['cat'].categories_):\n",
        "            categories = preprocessor.named_transformers_['cat'].categories_[i]\n",
        "            ohe_feature_names.extend([f\"{col_name}_{cat}\" for cat in categories])\n",
        "        else:\n",
        "            print(f\"Warning: Could not get categories for {col_name}. Skipping one-hot encoded column naming for this feature.\")\n",
        "\n",
        "df_processed_features.columns = transformed_numerical_cols + ohe_feature_names\n",
        "\n",
        "# --- 5. Combine Processed Features with Necessary Passthrough Columns (Revised) ---\n",
        "print(\"\\n--- Step 5: Combining Processed Features with Passthrough Columns ---\")\n",
        "\n",
        "df_processed = pd.concat([\n",
        "    df_processed_features,\n",
        "    df[passthrough_cols_for_graph].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "if 'label' in df_processed.columns:\n",
        "    df_processed['label'] = df_processed['label'].astype(int)\n",
        "else:\n",
        "    print(\"Error: 'label' column not found in df_processed. Check original column names or passthrough_cols_for_graph.\")\n",
        "\n",
        "if 'Label' in df_processed.columns and 'Label' != 'label':\n",
        "    df_processed = df_processed.drop(columns=['Label'])\n",
        "\n",
        "if 'Timestamp' in df_processed.columns:\n",
        "    df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'], errors='coerce')\n",
        "    if df_processed['Timestamp'].isnull().any():\n",
        "        print(\"Warning: Some Timestamps could not be converted to datetime. These rows might be excluded from temporal graph connections.\")\n",
        "else:\n",
        "    print(\"Error: 'Timestamp' column not found in df_processed. Temporal graph construction will not be possible.\")\n",
        "\n",
        "\n",
        "print(\"Processed DataFrame shape:\", df_processed.shape)\n",
        "print(\"Processed DataFrame head:\")\n",
        "print(df_processed.head())\n",
        "print(\"Processed DataFrame dtypes (should be mostly float/int for features, object for IDs/IPs):\")\n",
        "print(df_processed.dtypes.value_counts())\n",
        "\n",
        "\n",
        "# --- 6. Extract Features (x) and Labels (y) for PyG (GLOBAL) ---\n",
        "print(\"\\n--- Step 6: Extracting Features (x) and Labels (y) for PyG (GLOBAL) ---\")\n",
        "label_col_name = 'label'\n",
        "cols_to_exclude_from_features_pyg = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', label_col_name]\n",
        "\n",
        "feature_cols = [col for col in df_processed.columns if col not in cols_to_exclude_from_features_pyg]\n",
        "\n",
        "x_global = torch.tensor(df_processed[feature_cols].values.astype(np.float32), dtype=torch.float)\n",
        "y_global = torch.tensor(df_processed[label_col_name].values, dtype=torch.long)\n",
        "\n",
        "print(f\"Global Features (x_global) shape: {x_global.shape}\")\n",
        "print(f\"Global Labels (y_global) shape: {y_global.shape}\")\n",
        "\n",
        "\n",
        "# --- 7. Graph Construction for Causal Sampling (Creating data_list) ---\n",
        "print(\"\\n--- Step 7: Graph Construction for Causal Sampling (Creating data_list) ---\")\n",
        "\n",
        "df_processed_sorted = df_processed.sort_values(by='Timestamp').reset_index(drop=False)\n",
        "\n",
        "data_list = []\n",
        "window_duration = timedelta(minutes=10)\n",
        "stride_duration = timedelta(minutes=5)\n",
        "\n",
        "start_time = df_processed_sorted['Timestamp'].min()\n",
        "end_time = df_processed_sorted['Timestamp'].max()\n",
        "\n",
        "current_window_start = start_time\n",
        "\n",
        "window_count = 0\n",
        "while current_window_start <= end_time:\n",
        "    window_end = current_window_start + window_duration\n",
        "\n",
        "    window_df = df_processed_sorted[\n",
        "        (df_processed_sorted['Timestamp'] >= current_window_start) &\n",
        "        (df_processed_sorted['Timestamp'] < window_end)\n",
        "    ].copy()\n",
        "\n",
        "    if not window_df.empty:\n",
        "        window_count += 1\n",
        "        print(f\"Processing window {window_count}: {current_window_start} to {window_end} (Nodes: {len(window_df)})\")\n",
        "\n",
        "        window_original_indices = window_df['index'].values\n",
        "        x_window = x_global[window_original_indices]\n",
        "        y_window = y_global[window_original_indices]\n",
        "\n",
        "        edges_window = []\n",
        "        original_to_local_idx = {original_idx: local_idx for local_idx, original_idx in enumerate(window_original_indices)}\n",
        "\n",
        "        if 'Src IP' in window_df.columns and 'Dst IP' in window_df.columns:\n",
        "            for (src_ip, dst_ip), group in window_df.groupby(['Src IP', 'Dst IP']):\n",
        "                group_original_indices = group['index'].tolist()\n",
        "\n",
        "                for i in range(len(group_original_indices) - 1):\n",
        "                    node1_original = group_original_indices[i]\n",
        "                    node2_original = group_original_indices[i+1]\n",
        "\n",
        "                    node1_local = original_to_local_idx[node1_original]\n",
        "                    node2_local = original_to_local_idx[node2_original]\n",
        "\n",
        "                    edges_window.append([node1_local, node2_local])\n",
        "                    edges_window.append([node2_local, node1_local])\n",
        "        else:\n",
        "            print(\"Warning: 'Src IP' or 'Dst IP' not found in window_df. Cannot build IP-based graph for this window.\")\n",
        "\n",
        "        if edges_window:\n",
        "            edge_index_window = torch.tensor(edges_window, dtype=torch.long).t().contiguous()\n",
        "        else:\n",
        "            edge_index_window = torch.empty((2, 0), dtype=torch.long)\n",
        "\n",
        "        data_window = Data(x=x_window, edge_index=edge_index_window, y=y_window)\n",
        "        data_list.append(data_window)\n",
        "\n",
        "    current_window_start += stride_duration\n",
        "\n",
        "print(f\"\\nCreated {len(data_list)} graph snapshots (Data objects) for causal training.\")\n",
        "if len(data_list) > 0:\n",
        "    print(f\"Example snapshot 0: {data_list[0]}\")\n",
        "    print(f\"Example snapshot 1: {data_list[1]}\")\n",
        "\n",
        "# --- 8. Inject Noise into data_list (REVISED) ---\n",
        "print(\"\\n--- Step 8: Injecting Noise into Graph Snapshots (REVISED with lower noise) ---\")\n",
        "\n",
        "def inject_noise(data_list, label_noise_ratio=0.01, feature_noise_ratio=0.01, num_output_classes=2):\n",
        "    noisy_data_list = []\n",
        "    for data in data_list:\n",
        "        data_noisy = copy.deepcopy(data)\n",
        "\n",
        "        # Ensure label has multiple classes before injecting noise\n",
        "        unique_labels = list(set(data_noisy.y.tolist()))\n",
        "        if len(unique_labels) <= 1:\n",
        "            data_noisy.y = torch.randint(0, num_output_classes, data_noisy.y.shape, dtype=torch.long)\n",
        "            unique_labels = list(set(data_noisy.y.tolist()))\n",
        "\n",
        "        num_nodes = data_noisy.y.shape[0]\n",
        "\n",
        "        # Inject label noise\n",
        "        num_label_noise = int(label_noise_ratio * num_nodes)\n",
        "        noisy_label_indices = random.sample(range(num_nodes), num_label_noise)\n",
        "\n",
        "        for idx in noisy_label_indices:\n",
        "            original_label = data_noisy.y[idx].item()\n",
        "            possible_labels = list(range(num_output_classes))\n",
        "            if len(possible_labels) <= 1:\n",
        "                continue\n",
        "            if original_label in possible_labels:\n",
        "                possible_labels.remove(original_label)\n",
        "            if not possible_labels:\n",
        "                continue\n",
        "            new_label = random.choice(possible_labels)\n",
        "            data_noisy.y[idx] = new_label\n",
        "\n",
        "        # Inject feature noise\n",
        "        num_feature_noise = int(feature_noise_ratio * num_nodes)\n",
        "        noisy_feature_indices = random.sample(range(num_nodes), num_feature_noise)\n",
        "\n",
        "        for idx in noisy_feature_indices:\n",
        "            noise = torch.randn_like(data_noisy.x[idx]) * 0.1  # Reduced feature noise magnitude\n",
        "            data_noisy.x[idx] += noise\n",
        "\n",
        "        noisy_data_list.append(data_noisy)\n",
        "    return noisy_data_list\n",
        "\n",
        "# Apply noise to the created data_list with lower noise levels\n",
        "noisy_data_list = inject_noise(data_list, label_noise_ratio=0.01, feature_noise_ratio=0.01)\n",
        "\n",
        "# Add dummy time_window attribute for demonstration of causal model\n",
        "for i, data in enumerate(noisy_data_list):\n",
        "    num_nodes = data.num_nodes\n",
        "    data.time_window = torch.randint(0, 5, (num_nodes,), dtype=torch.long)\n",
        "\n",
        "\n",
        "# --- 9. GraphSAGE Model Definition ---\n",
        "print(\"\\n--- Step 9: GraphSAGE Model Definition ---\")\n",
        "class CausalGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CausalGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, time_window):\n",
        "        row, col = edge_index\n",
        "        mask = time_window[row] <= time_window[col]\n",
        "        if mask.sum() == 0:\n",
        "            filtered_edge_index = edge_index\n",
        "        else:\n",
        "            filtered_edge_index = edge_index[:, mask]\n",
        "        x = F.relu(self.conv1(x, filtered_edge_index))\n",
        "        x = self.conv2(x, filtered_edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- 10. Model Initialization and Causal Training Loop ---\n",
        "print(\"\\n--- Step 10: Model Initialization and Causal Training Loop ---\")\n",
        "\n",
        "if not noisy_data_list:\n",
        "    print(\"Error: No graph snapshots created after noise injection. Cannot proceed with training.\")\n",
        "else:\n",
        "    in_channels = noisy_data_list[0].x.shape[1]\n",
        "    hidden_channels = 64\n",
        "    out_channels = 2\n",
        "    model = CausalGraphSAGE(in_channels, hidden_channels, out_channels)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    num_epochs = 20\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, data in enumerate(noisy_data_list):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.time_window)\n",
        "            loss = loss_fn(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Processing window {i+1}/{len(noisy_data_list)}, Loss: {loss.item():.4f}\", end='\\r')\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Total Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in noisy_data_list:\n",
        "            out = model(data.x, data.edge_index, data.time_window)\n",
        "            preds = out.argmax(dim=1).cpu().numpy()\n",
        "            labels = data.y.cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    print(\"\\n--- Evaluation on Noisy Data ---\")\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blkEXKReY1Qo",
        "outputId": "89e80d5f-ca31-49c5-914c-613b0efb52a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Data Loading ---\n",
            "Found 1 benign files and 5 mirai files.\n",
            "Number of benign samples: 84526\n",
            "Number of Mirai samples: 33758\n",
            "\n",
            "--- Step 2: Labeling and Combining DataFrames ---\n",
            "Combined DataFrame shape: (118284, 85)\n",
            "Combined DataFrame head:\n",
            "                                     Flow ID           Src IP  Src Port  \\\n",
            "0    23.78.206.51-192.168.137.253-80-40940-6     23.78.206.51        80   \n",
            "1  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253     51030   \n",
            "2  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253     51030   \n",
            "3    192.168.137.253-23.78.206.51-40944-80-6  192.168.137.253     40944   \n",
            "4   192.168.137.187-18.207.33.55-39741-443-6  192.168.137.187     39741   \n",
            "\n",
            "            Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  \\\n",
            "0  192.168.137.253     40940         6  07/10/2022 11:30:28 PM         114733   \n",
            "1    54.148.103.72       443         6  07/10/2022 11:30:35 PM         780805   \n",
            "2    54.148.103.72       443         6  07/10/2022 11:30:35 PM           3721   \n",
            "3     23.78.206.51        80         6  07/10/2022 11:30:35 PM         530843   \n",
            "4     18.207.33.55       443         6  07/10/2022 11:30:36 PM         407436   \n",
            "\n",
            "   Total Fwd Packet  Total Bwd packets  ...  Active Mean  Active Std  \\\n",
            "0                45                172  ...          0.0         0.0   \n",
            "1                14                  7  ...          0.0         0.0   \n",
            "2                 2                  0  ...          0.0         0.0   \n",
            "3               123                108  ...          0.0         0.0   \n",
            "4                11                  4  ...          0.0         0.0   \n",
            "\n",
            "   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \\\n",
            "0         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "1         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "2         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "3         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "4         0.0         0.0        0.0       0.0       0.0       0.0   \n",
            "\n",
            "             Label  label  \n",
            "0  NeedManualLabel      0  \n",
            "1  NeedManualLabel      0  \n",
            "2  NeedManualLabel      0  \n",
            "3  NeedManualLabel      0  \n",
            "4  NeedManualLabel      0  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "Combined DataFrame label distribution:\n",
            " label\n",
            "0    84526\n",
            "1    33758\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Step 3: Data Type Conversion and Handling Missing/Infinite Values ---\n",
            "Number of infinite values after conversion:\n",
            "0\n",
            "\n",
            "Number of NaN values after conversion:\n",
            "88\n",
            "\n",
            "Data types after numerical conversion and imputation:\n",
            "float64    72\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Step 4: Preprocessing with ColumnTransformer (Revised) ---\n",
            "\n",
            "--- Step 5: Combining Processed Features with Passthrough Columns ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-73cc2b0c107a>:157: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed DataFrame shape: (118284, 80)\n",
            "Processed DataFrame head:\n",
            "   Flow Duration  Total Fwd Packet  Total Bwd packets  \\\n",
            "0      -0.608945         -0.032014           0.378796   \n",
            "1      -0.593817         -0.041258          -0.020484   \n",
            "2      -0.611466         -0.044836          -0.037423   \n",
            "3      -0.599494         -0.008756           0.223924   \n",
            "4      -0.602297         -0.042152          -0.027744   \n",
            "\n",
            "   Total Length of Fwd Packet  Total Length of Bwd Packet  \\\n",
            "0                    0.468405                   -0.012989   \n",
            "1                   -0.040247                    0.013221   \n",
            "2                   -0.053652                   -0.012989   \n",
            "3                   -0.051197                    1.211312   \n",
            "4                   -0.048647                    0.003557   \n",
            "\n",
            "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
            "0               3.676065              -0.540546               15.059868   \n",
            "1               1.210733              -0.540546                0.812521   \n",
            "2              -0.198538              -0.540546               -0.469124   \n",
            "3              -0.013906              -0.540546               -0.442409   \n",
            "4               0.218000              -0.540546                0.139864   \n",
            "\n",
            "   Fwd Packet Length Std  Bwd Packet Length Max  ...  Idle Max  Idle Min  \\\n",
            "0               7.155258              -0.327351  ... -0.479758 -0.443881   \n",
            "1               2.796088               4.017415  ... -0.479758 -0.443881   \n",
            "2              -0.251263              -0.327351  ... -0.479758 -0.443881   \n",
            "3              -0.055914               4.017415  ... -0.479758 -0.443881   \n",
            "4               0.817114               4.808054  ... -0.479758 -0.443881   \n",
            "\n",
            "   Protocol_0  Protocol_6  Protocol_17  \\\n",
            "0         0.0         1.0          0.0   \n",
            "1         0.0         1.0          0.0   \n",
            "2         0.0         1.0          0.0   \n",
            "3         0.0         1.0          0.0   \n",
            "4         0.0         1.0          0.0   \n",
            "\n",
            "                                     Flow ID           Src IP  \\\n",
            "0    23.78.206.51-192.168.137.253-80-40940-6     23.78.206.51   \n",
            "1  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253   \n",
            "2  192.168.137.253-54.148.103.72-51030-443-6  192.168.137.253   \n",
            "3    192.168.137.253-23.78.206.51-40944-80-6  192.168.137.253   \n",
            "4   192.168.137.187-18.207.33.55-39741-443-6  192.168.137.187   \n",
            "\n",
            "            Dst IP           Timestamp  label  \n",
            "0  192.168.137.253 2022-07-10 23:30:28      0  \n",
            "1    54.148.103.72 2022-07-10 23:30:35      0  \n",
            "2    54.148.103.72 2022-07-10 23:30:35      0  \n",
            "3     23.78.206.51 2022-07-10 23:30:35      0  \n",
            "4     18.207.33.55 2022-07-10 23:30:36      0  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "Processed DataFrame dtypes (should be mostly float/int for features, object for IDs/IPs):\n",
            "float32           75\n",
            "object             3\n",
            "datetime64[ns]     1\n",
            "int64              1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Step 6: Extracting Features (x) and Labels (y) for PyG (GLOBAL) ---\n",
            "Global Features (x_global) shape: torch.Size([118284, 75])\n",
            "Global Labels (y_global) shape: torch.Size([118284])\n",
            "\n",
            "--- Step 7: Graph Construction for Causal Sampling (Creating data_list) ---\n",
            "Processing window 1: 2022-07-10 23:30:28 to 2022-07-10 23:40:28 (Nodes: 1962)\n",
            "Processing window 2: 2022-07-10 23:35:28 to 2022-07-10 23:45:28 (Nodes: 1945)\n",
            "Processing window 3: 2022-07-10 23:40:28 to 2022-07-10 23:50:28 (Nodes: 1910)\n",
            "Processing window 4: 2022-07-10 23:45:28 to 2022-07-10 23:55:28 (Nodes: 1912)\n",
            "Processing window 5: 2022-07-10 23:50:28 to 2022-07-11 00:00:28 (Nodes: 1909)\n",
            "Processing window 6: 2022-07-10 23:55:28 to 2022-07-11 00:05:28 (Nodes: 919)\n",
            "Processing window 7: 2022-08-09 23:50:28 to 2022-08-10 00:00:28 (Nodes: 54)\n",
            "Processing window 8: 2022-08-09 23:55:28 to 2022-08-10 00:05:28 (Nodes: 1029)\n",
            "Processing window 9: 2022-08-10 00:00:28 to 2022-08-10 00:10:28 (Nodes: 1948)\n",
            "Processing window 10: 2022-08-10 00:05:28 to 2022-08-10 00:15:28 (Nodes: 1946)\n",
            "Processing window 11: 2022-08-10 00:10:28 to 2022-08-10 00:20:28 (Nodes: 1934)\n",
            "Processing window 12: 2022-08-10 00:15:28 to 2022-08-10 00:25:28 (Nodes: 1916)\n",
            "Processing window 13: 2022-08-10 00:20:28 to 2022-08-10 00:30:28 (Nodes: 1896)\n",
            "Processing window 14: 2022-08-10 00:25:28 to 2022-08-10 00:35:28 (Nodes: 1877)\n",
            "Processing window 15: 2022-08-10 00:30:28 to 2022-08-10 00:40:28 (Nodes: 1914)\n",
            "Processing window 16: 2022-08-10 00:35:28 to 2022-08-10 00:45:28 (Nodes: 1927)\n",
            "Processing window 17: 2022-08-10 00:40:28 to 2022-08-10 00:50:28 (Nodes: 1937)\n",
            "Processing window 18: 2022-08-10 00:45:28 to 2022-08-10 00:55:28 (Nodes: 1898)\n",
            "Processing window 19: 2022-08-10 00:50:28 to 2022-08-10 01:00:28 (Nodes: 1923)\n",
            "Processing window 20: 2022-08-10 00:55:28 to 2022-08-10 01:05:28 (Nodes: 1999)\n",
            "Processing window 21: 2022-08-10 01:00:28 to 2022-08-10 01:10:28 (Nodes: 2112)\n",
            "Processing window 22: 2022-08-10 01:05:28 to 2022-08-10 01:15:28 (Nodes: 2111)\n",
            "Processing window 23: 2022-08-10 01:10:28 to 2022-08-10 01:20:28 (Nodes: 1997)\n",
            "Processing window 24: 2022-08-10 01:15:28 to 2022-08-10 01:25:28 (Nodes: 2000)\n",
            "Processing window 25: 2022-08-10 01:20:28 to 2022-08-10 01:30:28 (Nodes: 1980)\n",
            "Processing window 26: 2022-08-10 01:25:28 to 2022-08-10 01:35:28 (Nodes: 1935)\n",
            "Processing window 27: 2022-08-10 01:30:28 to 2022-08-10 01:40:28 (Nodes: 1896)\n",
            "Processing window 28: 2022-08-10 01:35:28 to 2022-08-10 01:45:28 (Nodes: 1903)\n",
            "Processing window 29: 2022-08-10 01:40:28 to 2022-08-10 01:50:28 (Nodes: 1946)\n",
            "Processing window 30: 2022-08-10 01:45:28 to 2022-08-10 01:55:28 (Nodes: 1937)\n",
            "Processing window 31: 2022-08-10 01:50:28 to 2022-08-10 02:00:28 (Nodes: 1969)\n",
            "Processing window 32: 2022-08-10 01:55:28 to 2022-08-10 02:05:28 (Nodes: 1989)\n",
            "Processing window 33: 2022-08-10 02:00:28 to 2022-08-10 02:10:28 (Nodes: 1853)\n",
            "Processing window 34: 2022-08-10 02:05:28 to 2022-08-10 02:15:28 (Nodes: 1859)\n",
            "Processing window 35: 2022-08-10 02:10:28 to 2022-08-10 02:20:28 (Nodes: 1910)\n",
            "Processing window 36: 2022-08-10 02:15:28 to 2022-08-10 02:25:28 (Nodes: 1898)\n",
            "Processing window 37: 2022-08-10 02:20:28 to 2022-08-10 02:30:28 (Nodes: 1952)\n",
            "Processing window 38: 2022-08-10 02:25:28 to 2022-08-10 02:35:28 (Nodes: 1967)\n",
            "Processing window 39: 2022-08-10 02:30:28 to 2022-08-10 02:40:28 (Nodes: 1942)\n",
            "Processing window 40: 2022-08-10 02:35:28 to 2022-08-10 02:45:28 (Nodes: 1933)\n",
            "Processing window 41: 2022-08-10 02:40:28 to 2022-08-10 02:50:28 (Nodes: 2006)\n",
            "Processing window 42: 2022-08-10 02:45:28 to 2022-08-10 02:55:28 (Nodes: 2021)\n",
            "Processing window 43: 2022-08-10 02:50:28 to 2022-08-10 03:00:28 (Nodes: 1965)\n",
            "Processing window 44: 2022-08-10 02:55:28 to 2022-08-10 03:05:28 (Nodes: 1968)\n",
            "Processing window 45: 2022-08-10 03:00:28 to 2022-08-10 03:10:28 (Nodes: 1924)\n",
            "Processing window 46: 2022-08-10 03:05:28 to 2022-08-10 03:15:28 (Nodes: 1883)\n",
            "Processing window 47: 2022-08-10 03:10:28 to 2022-08-10 03:20:28 (Nodes: 1850)\n",
            "Processing window 48: 2022-08-10 03:15:28 to 2022-08-10 03:25:28 (Nodes: 1885)\n",
            "Processing window 49: 2022-08-10 03:20:28 to 2022-08-10 03:30:28 (Nodes: 1951)\n",
            "Processing window 50: 2022-08-10 03:25:28 to 2022-08-10 03:35:28 (Nodes: 1947)\n",
            "Processing window 51: 2022-08-10 03:30:28 to 2022-08-10 03:40:28 (Nodes: 1935)\n",
            "Processing window 52: 2022-08-10 03:35:28 to 2022-08-10 03:45:28 (Nodes: 1910)\n",
            "Processing window 53: 2022-08-10 03:40:28 to 2022-08-10 03:50:28 (Nodes: 1899)\n",
            "Processing window 54: 2022-08-10 03:45:28 to 2022-08-10 03:55:28 (Nodes: 1875)\n",
            "Processing window 55: 2022-08-10 03:50:28 to 2022-08-10 04:00:28 (Nodes: 1875)\n",
            "Processing window 56: 2022-08-10 03:55:28 to 2022-08-10 04:05:28 (Nodes: 1908)\n",
            "Processing window 57: 2022-08-10 04:00:28 to 2022-08-10 04:10:28 (Nodes: 1919)\n",
            "Processing window 58: 2022-08-10 04:05:28 to 2022-08-10 04:15:28 (Nodes: 2115)\n",
            "Processing window 59: 2022-08-10 04:10:28 to 2022-08-10 04:20:28 (Nodes: 2204)\n",
            "Processing window 60: 2022-08-10 04:15:28 to 2022-08-10 04:25:28 (Nodes: 2025)\n",
            "Processing window 61: 2022-08-10 04:20:28 to 2022-08-10 04:30:28 (Nodes: 1980)\n",
            "Processing window 62: 2022-08-10 04:25:28 to 2022-08-10 04:35:28 (Nodes: 1992)\n",
            "Processing window 63: 2022-08-10 04:30:28 to 2022-08-10 04:40:28 (Nodes: 2013)\n",
            "Processing window 64: 2022-08-10 04:35:28 to 2022-08-10 04:45:28 (Nodes: 1972)\n",
            "Processing window 65: 2022-08-10 04:40:28 to 2022-08-10 04:50:28 (Nodes: 1933)\n",
            "Processing window 66: 2022-08-10 04:45:28 to 2022-08-10 04:55:28 (Nodes: 1915)\n",
            "Processing window 67: 2022-08-10 04:50:28 to 2022-08-10 05:00:28 (Nodes: 1864)\n",
            "Processing window 68: 2022-08-10 04:55:28 to 2022-08-10 05:05:28 (Nodes: 1909)\n",
            "Processing window 69: 2022-08-10 05:00:28 to 2022-08-10 05:10:28 (Nodes: 1893)\n",
            "Processing window 70: 2022-08-10 05:05:28 to 2022-08-10 05:15:28 (Nodes: 1825)\n",
            "Processing window 71: 2022-08-10 05:10:28 to 2022-08-10 05:20:28 (Nodes: 1863)\n",
            "Processing window 72: 2022-08-10 05:15:28 to 2022-08-10 05:25:28 (Nodes: 1911)\n",
            "Processing window 73: 2022-08-10 05:20:28 to 2022-08-10 05:30:28 (Nodes: 1930)\n",
            "Processing window 74: 2022-08-10 05:25:28 to 2022-08-10 05:35:28 (Nodes: 1985)\n",
            "Processing window 75: 2022-08-10 05:30:28 to 2022-08-10 05:40:28 (Nodes: 2059)\n",
            "Processing window 76: 2022-08-10 05:35:28 to 2022-08-10 05:45:28 (Nodes: 2069)\n",
            "Processing window 77: 2022-08-10 05:40:28 to 2022-08-10 05:50:28 (Nodes: 2020)\n",
            "Processing window 78: 2022-08-10 05:45:28 to 2022-08-10 05:55:28 (Nodes: 2041)\n",
            "Processing window 79: 2022-08-10 05:50:28 to 2022-08-10 06:00:28 (Nodes: 2077)\n",
            "Processing window 80: 2022-08-10 05:55:28 to 2022-08-10 06:05:28 (Nodes: 1975)\n",
            "Processing window 81: 2022-08-10 06:00:28 to 2022-08-10 06:10:28 (Nodes: 1887)\n",
            "Processing window 82: 2022-08-10 06:05:28 to 2022-08-10 06:15:28 (Nodes: 1880)\n",
            "Processing window 83: 2022-08-10 06:10:28 to 2022-08-10 06:20:28 (Nodes: 1856)\n",
            "Processing window 84: 2022-08-10 06:15:28 to 2022-08-10 06:25:28 (Nodes: 1855)\n",
            "Processing window 85: 2022-08-10 06:20:28 to 2022-08-10 06:30:28 (Nodes: 1879)\n",
            "Processing window 86: 2022-08-10 06:25:28 to 2022-08-10 06:35:28 (Nodes: 1865)\n",
            "Processing window 87: 2022-08-10 06:30:28 to 2022-08-10 06:40:28 (Nodes: 1877)\n",
            "Processing window 88: 2022-08-10 06:35:28 to 2022-08-10 06:45:28 (Nodes: 1890)\n",
            "Processing window 89: 2022-08-10 06:40:28 to 2022-08-10 06:50:28 (Nodes: 923)\n",
            "Processing window 90: 2023-12-01 11:10:28 to 2023-12-01 11:20:28 (Nodes: 1271)\n",
            "Processing window 91: 2023-12-01 11:15:28 to 2023-12-01 11:25:28 (Nodes: 3269)\n",
            "Processing window 92: 2023-12-01 11:20:28 to 2023-12-01 11:30:28 (Nodes: 2854)\n",
            "Processing window 93: 2023-12-01 11:25:28 to 2023-12-01 11:35:28 (Nodes: 1501)\n",
            "Processing window 94: 2023-12-01 11:30:28 to 2023-12-01 11:40:28 (Nodes: 1741)\n",
            "Processing window 95: 2023-12-01 11:35:28 to 2023-12-01 11:45:28 (Nodes: 1799)\n",
            "Processing window 96: 2023-12-01 11:40:28 to 2023-12-01 11:50:28 (Nodes: 703)\n",
            "Processing window 97: 2023-12-01 11:45:28 to 2023-12-01 11:55:28 (Nodes: 1833)\n",
            "Processing window 98: 2023-12-01 11:50:28 to 2023-12-01 12:00:28 (Nodes: 3984)\n",
            "Processing window 99: 2023-12-01 11:55:28 to 2023-12-01 12:05:28 (Nodes: 2706)\n",
            "Processing window 100: 2023-12-01 12:00:28 to 2023-12-01 12:10:28 (Nodes: 5345)\n",
            "Processing window 101: 2023-12-01 12:05:28 to 2023-12-01 12:15:28 (Nodes: 5590)\n",
            "Processing window 102: 2023-12-01 12:10:28 to 2023-12-01 12:20:28 (Nodes: 1256)\n",
            "Processing window 103: 2023-12-01 12:15:28 to 2023-12-01 12:25:28 (Nodes: 456)\n",
            "Processing window 104: 2023-12-01 12:20:28 to 2023-12-01 12:30:28 (Nodes: 2377)\n",
            "Processing window 105: 2023-12-01 12:25:28 to 2023-12-01 12:35:28 (Nodes: 3636)\n",
            "Processing window 106: 2023-12-01 12:30:28 to 2023-12-01 12:40:28 (Nodes: 1536)\n",
            "Processing window 107: 2023-12-01 12:35:28 to 2023-12-01 12:45:28 (Nodes: 2820)\n",
            "Processing window 108: 2023-12-01 12:40:28 to 2023-12-01 12:50:28 (Nodes: 3493)\n",
            "Processing window 109: 2023-12-01 12:45:28 to 2023-12-01 12:55:28 (Nodes: 1553)\n",
            "Processing window 110: 2023-12-01 12:50:28 to 2023-12-01 13:00:28 (Nodes: 603)\n",
            "Processing window 111: 2023-12-01 12:55:28 to 2023-12-01 13:05:28 (Nodes: 4936)\n",
            "Processing window 112: 2023-12-01 13:00:28 to 2023-12-01 13:10:28 (Nodes: 5777)\n",
            "Processing window 113: 2023-12-01 13:05:28 to 2023-12-01 13:15:28 (Nodes: 1137)\n",
            "Processing window 114: 2023-12-01 13:10:28 to 2023-12-01 13:20:28 (Nodes: 1816)\n",
            "Processing window 115: 2023-12-01 13:15:28 to 2023-12-01 13:25:28 (Nodes: 2522)\n",
            "Processing window 116: 2023-12-01 13:20:28 to 2023-12-01 13:30:28 (Nodes: 1002)\n",
            "\n",
            "Created 116 graph snapshots (Data objects) for causal training.\n",
            "Example snapshot 0: Data(x=[1962, 75], edge_index=[2, 3314], y=[1962])\n",
            "Example snapshot 1: Data(x=[1945, 75], edge_index=[2, 3298], y=[1945])\n",
            "\n",
            "--- Step 8: Injecting Noise into Graph Snapshots (REVISED with lower noise) ---\n",
            "\n",
            "--- Step 9: GraphSAGE Model Definition ---\n",
            "\n",
            "--- Step 10: Model Initialization and Causal Training Loop ---\n",
            "Epoch 1/20, Processing window 116/116, Loss: 0.6981\n",
            "Epoch 1/20, Total Loss: 80.7483\n",
            "Epoch 2/20, Processing window 116/116, Loss: 0.6981\n",
            "Epoch 2/20, Total Loss: 80.4836\n",
            "Epoch 3/20, Processing window 116/116, Loss: 0.6921\n",
            "Epoch 3/20, Total Loss: 80.4570\n",
            "Epoch 4/20, Processing window 116/116, Loss: 0.6908\n",
            "Epoch 4/20, Total Loss: 80.4130\n",
            "Epoch 5/20, Processing window 116/116, Loss: 0.6924\n",
            "Epoch 5/20, Total Loss: 80.3785\n",
            "Epoch 6/20, Processing window 116/116, Loss: 0.6895\n",
            "Epoch 6/20, Total Loss: 80.3737\n",
            "Epoch 7/20, Processing window 116/116, Loss: 0.6904\n",
            "Epoch 7/20, Total Loss: 80.3509\n",
            "Epoch 8/20, Processing window 116/116, Loss: 0.6887\n",
            "Epoch 8/20, Total Loss: 80.3545\n",
            "Epoch 9/20, Processing window 116/116, Loss: 0.6889\n",
            "Epoch 9/20, Total Loss: 80.3267\n",
            "Epoch 10/20, Processing window 116/116, Loss: 0.6877\n",
            "Epoch 10/20, Total Loss: 80.3354\n",
            "Epoch 11/20, Processing window 116/116, Loss: 0.6875\n",
            "Epoch 11/20, Total Loss: 80.3063\n",
            "Epoch 12/20, Processing window 116/116, Loss: 0.6873\n",
            "Epoch 12/20, Total Loss: 80.3107\n",
            "Epoch 13/20, Processing window 116/116, Loss: 0.6866\n",
            "Epoch 13/20, Total Loss: 80.3038\n",
            "Epoch 14/20, Processing window 116/116, Loss: 0.6866\n",
            "Epoch 14/20, Total Loss: 80.3116\n",
            "Epoch 15/20, Processing window 116/116, Loss: 0.6861\n",
            "Epoch 15/20, Total Loss: 80.3040\n",
            "Epoch 16/20, Processing window 116/116, Loss: 0.6859\n",
            "Epoch 16/20, Total Loss: 80.3106\n",
            "Epoch 17/20, Processing window 116/116, Loss: 0.6860\n",
            "Epoch 17/20, Total Loss: 80.2762\n",
            "Epoch 18/20, Processing window 116/116, Loss: 0.6856\n",
            "Epoch 18/20, Total Loss: 80.2825\n",
            "Epoch 19/20, Processing window 116/116, Loss: 0.6860\n",
            "Epoch 19/20, Total Loss: 80.2889\n",
            "Epoch 20/20, Processing window 116/116, Loss: 0.6854\n",
            "Epoch 20/20, Total Loss: 80.3012\n",
            "\n",
            "--- Evaluation on Noisy Data ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.52      0.52    117709\n",
            "           1       0.51      0.51      0.51    117854\n",
            "\n",
            "    accuracy                           0.51    235563\n",
            "   macro avg       0.51      0.51      0.51    235563\n",
            "weighted avg       0.51      0.51      0.51    235563\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. GraphSAGE Model Definition ---\n",
        "print(\"\\n--- Step 9: GraphSAGE Model Definition ---\")\n",
        "class CausalGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CausalGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, time_window):\n",
        "        row, col = edge_index\n",
        "        mask = time_window[row] <= time_window[col]\n",
        "        if mask.sum() == 0:\n",
        "            filtered_edge_index = edge_index\n",
        "        else:\n",
        "            filtered_edge_index = edge_index[:, mask]\n",
        "        x = F.relu(self.conv1(x, filtered_edge_index))\n",
        "        x = self.conv2(x, filtered_edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- 10. Model Initialization and Causal Training Loop ---\n",
        "print(\"\\n--- Step 10: Model Initialization and Causal Training Loop ---\")\n",
        "\n",
        "if not noisy_data_list:\n",
        "    print(\"Error: No graph snapshots created after noise injection. Cannot proceed with training.\")\n",
        "else:\n",
        "    in_channels = noisy_data_list[0].x.shape[1] if noisy_data_list[0].x.ndim > 1 else noisy_data_list[0].x.size(0)\n",
        "    out_channels = len(y_global.unique())\n",
        "    hidden_channels = 64  # You can adjust this\n",
        "\n",
        "    if in_channels == 0:\n",
        "        print(\"Error: No features detected in snapshots. Check feature_cols.\")\n",
        "    elif out_channels < 2:\n",
        "        print(f\"Error: Only {out_channels} class(es) detected. Need at least 2 for classification.\")\n",
        "    else:\n",
        "        model = CausalGraphSAGE(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels).to(\"cpu\")\n",
        "\n",
        "        class_counts = df['label'].value_counts().sort_index()\n",
        "        total_samples = class_counts.sum()\n",
        "\n",
        "        weights_array = np.zeros(out_channels)\n",
        "        for class_label, count in class_counts.items():\n",
        "            if count > 0:\n",
        "                weights_array[class_label] = total_samples / (out_channels * count)\n",
        "            else:\n",
        "                weights_array[class_label] = total_samples\n",
        "\n",
        "        weights_tensor = torch.tensor(weights_array, dtype=torch.float)\n",
        "\n",
        "        print(f\"\\nCalculated global class weights: {weights_tensor.tolist()}\")\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "\n",
        "        print(\"\\nStarting causal training with noise...\")\n",
        "        num_epochs = 20\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct_epoch = 0\n",
        "            total_epoch = 0\n",
        "\n",
        "            if len(noisy_data_list) < 2:\n",
        "                print(\"Warning: Less than 2 windows available for causal training. Skipping epoch.\")\n",
        "                break\n",
        "\n",
        "            for t in range(1, len(noisy_data_list)):\n",
        "                print(f\"\\rEpoch {epoch+1}/{num_epochs}, Processing window {t}/{len(noisy_data_list)-1}...\", end='', flush=True)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                past_data = noisy_data_list[t - 1].to(\"cpu\")\n",
        "                current_data = noisy_data_list[t].to(\"cpu\")\n",
        "\n",
        "                if past_data.y.nelement() == 0 or current_data.y.nelement() == 0:\n",
        "                    print(f\"Skipping window {t} due to empty past_data.y or current_data.y\")\n",
        "                    continue\n",
        "\n",
        "                out_past = model(past_data.x, past_data.edge_index, past_data.time_window)\n",
        "                loss_past = criterion(out_past, past_data.y)\n",
        "\n",
        "                out_current = model(current_data.x, current_data.edge_index, current_data.time_window)\n",
        "                loss_current = criterion(out_current, current_data.y)\n",
        "\n",
        "                loss = loss_past + loss_current\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if current_data.y.nelement() > 0:\n",
        "                        preds = out_current.argmax(dim=1)\n",
        "                        correct_epoch += (preds == current_data.y).sum().item()\n",
        "                        total_epoch += current_data.y.size(0)\n",
        "                model.train()\n",
        "\n",
        "            avg_epoch_loss = total_loss / (len(noisy_data_list) - 1) if (len(noisy_data_list) - 1) > 0 else total_loss\n",
        "            epoch_accuracy = correct_epoch / total_epoch if total_epoch > 0 else 0\n",
        "\n",
        "            losses.append(avg_epoch_loss)\n",
        "            accuracies.append(epoch_accuracy * 100)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}, Causal Loss with Noise: {avg_epoch_loss:.4f}, Accuracy: {epoch_accuracy*100:.2f}%\")\n",
        "\n",
        "        print(\"\\nCausal training with noise complete. Running final evaluation on all noisy snapshots...\")\n",
        "\n",
        "        # --- Final Evaluation on all noisy snapshots ---\n",
        "        model.eval()\n",
        "        all_true_labels = []\n",
        "        all_predicted_labels = []\n",
        "        with torch.no_grad():\n",
        "            for data_snapshot in noisy_data_list:\n",
        "                data_snapshot = data_snapshot.to(\"cpu\")\n",
        "\n",
        "                if data_snapshot.x.nelement() == 0 or data_snapshot.y.nelement() == 0:\n",
        "                    continue\n",
        "\n",
        "                out = model(data_snapshot.x, data_snapshot.edge_index, data_snapshot.time_window)\n",
        "                preds = out.argmax(dim=1)\n",
        "                all_true_labels.extend(data_snapshot.y.cpu().numpy())\n",
        "                all_predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "        print(\"\\n--- Overall Classification Report (Evaluated on noisy snapshots) ---\")\n",
        "        if len(all_true_labels) > 0:\n",
        "            print(classification_report(all_true_labels, all_predicted_labels, zero_division=0))\n",
        "        else:\n",
        "            print(\"No samples for overall evaluation.\")\n",
        "\n",
        "        # --- Plotting ---\n",
        "        epochs_plot = list(range(1, num_epochs + 1))\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Loss plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_plot, losses, marker='o', color='blue')\n",
        "        plt.title('Causal Sampling with Noise - Loss Over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Accuracy plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_plot, accuracies, marker='s', color='green')\n",
        "        plt.title('Causal Sampling with Noise - Accuracy Over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-LLgu4kndItG",
        "outputId": "98f30bc1-c9d0-4f8c-ffde-94275b152a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 9: GraphSAGE Model Definition ---\n",
            "\n",
            "--- Step 10: Model Initialization and Causal Training Loop ---\n",
            "\n",
            "Calculated global class weights: [0.699690043926239, 1.7519402503967285]\n",
            "\n",
            "Starting causal training with noise...\n",
            "Epoch 1/20, Processing window 115/115...\n",
            "Epoch 1/20, Causal Loss with Noise: 1.2261, Accuracy: 49.98%\n",
            "Epoch 2/20, Processing window 115/115...\n",
            "Epoch 2/20, Causal Loss with Noise: 1.1997, Accuracy: 50.04%\n",
            "Epoch 3/20, Processing window 115/115...\n",
            "Epoch 3/20, Causal Loss with Noise: 1.1978, Accuracy: 50.04%\n",
            "Epoch 4/20, Processing window 115/115...\n",
            "Epoch 4/20, Causal Loss with Noise: 1.1969, Accuracy: 50.06%\n",
            "Epoch 5/20, Processing window 115/115...\n",
            "Epoch 5/20, Causal Loss with Noise: 1.1964, Accuracy: 50.06%\n",
            "Epoch 6/20, Processing window 115/115...\n",
            "Epoch 6/20, Causal Loss with Noise: 1.1958, Accuracy: 50.06%\n",
            "Epoch 7/20, Processing window 115/115...\n",
            "Epoch 7/20, Causal Loss with Noise: 1.1957, Accuracy: 50.06%\n",
            "Epoch 8/20, Processing window 115/115...\n",
            "Epoch 8/20, Causal Loss with Noise: 1.1955, Accuracy: 50.06%\n",
            "Epoch 9/20, Processing window 115/115...\n",
            "Epoch 9/20, Causal Loss with Noise: 1.1950, Accuracy: 50.06%\n",
            "Epoch 10/20, Processing window 115/115...\n",
            "Epoch 10/20, Causal Loss with Noise: 1.1950, Accuracy: 50.06%\n",
            "Epoch 11/20, Processing window 115/115...\n",
            "Epoch 11/20, Causal Loss with Noise: 1.1947, Accuracy: 50.07%\n",
            "Epoch 12/20, Processing window 115/115...\n",
            "Epoch 12/20, Causal Loss with Noise: 1.1943, Accuracy: 50.07%\n",
            "Epoch 13/20, Processing window 115/115...\n",
            "Epoch 13/20, Causal Loss with Noise: 1.1942, Accuracy: 50.07%\n",
            "Epoch 14/20, Processing window 115/115...\n",
            "Epoch 14/20, Causal Loss with Noise: 1.1941, Accuracy: 50.07%\n",
            "Epoch 15/20, Processing window 115/115...\n",
            "Epoch 15/20, Causal Loss with Noise: 1.1941, Accuracy: 50.06%\n",
            "Epoch 16/20, Processing window 115/115...\n",
            "Epoch 16/20, Causal Loss with Noise: 1.1939, Accuracy: 50.08%\n",
            "Epoch 17/20, Processing window 115/115...\n",
            "Epoch 17/20, Causal Loss with Noise: 1.1935, Accuracy: 50.08%\n",
            "Epoch 18/20, Processing window 115/115...\n",
            "Epoch 18/20, Causal Loss with Noise: 1.1933, Accuracy: 50.08%\n",
            "Epoch 19/20, Processing window 115/115...\n",
            "Epoch 19/20, Causal Loss with Noise: 1.1933, Accuracy: 50.08%\n",
            "Epoch 20/20, Processing window 115/115...\n",
            "Epoch 20/20, Causal Loss with Noise: 1.1931, Accuracy: 50.10%\n",
            "\n",
            "Causal training with noise complete. Running final evaluation on all noisy snapshots...\n",
            "\n",
            "--- Overall Classification Report (Evaluated on noisy snapshots) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.00      0.00    117709\n",
            "           1       0.50      1.00      0.67    117854\n",
            "\n",
            "    accuracy                           0.50    235563\n",
            "   macro avg       0.65      0.50      0.33    235563\n",
            "weighted avg       0.65      0.50      0.33    235563\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import copy\n",
        "import random\n",
        "\n",
        "def inject_noise(data_list, label_noise_ratio=0.05, feature_noise_ratio=0.05, num_output_classes=2):\n",
        "    noisy_data_list = []\n",
        "    for data in data_list:\n",
        "        data_noisy = copy.deepcopy(data)\n",
        "        num_nodes = data_noisy.y.shape[0]\n",
        "        if len(set(data_noisy.y.tolist())) <= 1 or max(data_noisy.y.tolist()) >= num_output_classes:\n",
        "            print(f\"Warning: Adjusting labels in data sample {data_list.index(data)} to fit {num_output_classes} classes.\")\n",
        "            data_noisy.y = torch.randint(0, num_output_classes, data_noisy.y.shape, dtype=torch.long)\n",
        "        num_label_noise = int(label_noise_ratio * num_nodes)\n",
        "        noisy_label_indices = random.sample(range(num_nodes), num_label_noise)\n",
        "        for idx in noisy_label_indices:\n",
        "            original_label = data_noisy.y[idx].item()\n",
        "            possible_labels_for_noise = list(range(num_output_classes))\n",
        "            if original_label in possible_labels_for_noise:\n",
        "                possible_labels_for_noise.remove(original_label)\n",
        "            if not possible_labels_for_noise:\n",
        "                continue\n",
        "            new_label = random.choice(possible_labels_for_noise)\n",
        "            data_noisy.y[idx] = new_label\n",
        "        num_feature_noise = int(feature_noise_ratio * num_nodes)\n",
        "        noisy_feature_indices = random.sample(range(num_nodes), num_feature_noise)\n",
        "        for idx in noisy_feature_indices:\n",
        "            noise = torch.randn_like(data_noisy.x[idx]) * 0.3\n",
        "            data_noisy.x[idx] += noise\n",
        "        noisy_data_list.append(data_noisy)\n",
        "    return noisy_data_list\n",
        "\n",
        "# Assuming data_list is your original list of Data objects\n",
        "noisy_data_list = inject_noise(data_list, num_output_classes=2)\n",
        "\n",
        "# Ensure time_window attribute is added to each Data object\n",
        "for i, data in enumerate(noisy_data_list):\n",
        "    if not hasattr(data, 'time_window'):\n",
        "        data.time_window = torch.full((data.num_nodes,), i, dtype=torch.long)\n",
        "\n",
        "class CausalGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0):\n",
        "        super(CausalGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, time_window):\n",
        "        row, col = edge_index\n",
        "        mask = time_window[row] < time_window[col]\n",
        "        edge_index = edge_index[:, mask]\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Example initialization (adjust hyperparameters as needed)\n",
        "in_channels = noisy_data_list[0].x.shape[1] if noisy_data_list and noisy_data_list[0].x.ndim > 1 else noisy_data_list[0].x.size(0) if noisy_data_list else 55 # Infer or set default\n",
        "hidden_channels = 32\n",
        "out_channels = 2 # Assuming binary classification (benign/malware)\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 20\n",
        "\n",
        "model = CausalGraphSAGE(in_channels, hidden_channels, out_channels, dropout)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for t, data in enumerate(noisy_data_list):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.time_window)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluation phase\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in noisy_data_list:\n",
        "        out = model(data.x, data.edge_index, data.time_window)\n",
        "        preds = out.argmax(dim=1).cpu()\n",
        "        labels = data.y.cpu()\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Final Model Metrics -> Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JnBflXBhSBN",
        "outputId": "cef58cf3-1d53-45de-aba5-74648ce9d56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Adjusting labels in data sample 0 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 1 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 2 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 3 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 4 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 5 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 6 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 7 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 8 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 9 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 10 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 11 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 12 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 13 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 14 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 15 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 16 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 17 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 18 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 19 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 20 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 21 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 22 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 23 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 24 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 25 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 26 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 27 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 28 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 29 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 30 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 31 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 32 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 33 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 34 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 35 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 36 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 37 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 38 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 39 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 40 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 41 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 42 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 43 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 44 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 45 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 46 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 47 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 48 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 49 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 50 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 51 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 52 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 53 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 54 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 55 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 56 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 57 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 58 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 59 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 60 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 61 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 62 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 63 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 64 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 65 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 66 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 67 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 68 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 69 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 70 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 71 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 72 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 73 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 74 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 75 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 76 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 77 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 78 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 79 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 80 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 81 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 82 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 83 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 84 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 85 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 86 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 87 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 88 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 89 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 90 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 91 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 92 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 93 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 94 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 95 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 96 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 97 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 98 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 99 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 100 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 101 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 102 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 103 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 104 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 105 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 106 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 107 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 108 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 109 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 110 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 111 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 112 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 113 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 114 to fit 2 classes.\n",
            "Warning: Adjusting labels in data sample 115 to fit 2 classes.\n",
            "Epoch 1, Loss: 81.5259\n",
            "Epoch 2, Loss: 80.5174\n",
            "Epoch 3, Loss: 80.4317\n",
            "Epoch 4, Loss: 80.4189\n",
            "Epoch 5, Loss: 80.4188\n",
            "Epoch 6, Loss: 80.4084\n",
            "Epoch 7, Loss: 80.4067\n",
            "Epoch 8, Loss: 80.4065\n",
            "Epoch 9, Loss: 80.4087\n",
            "Epoch 10, Loss: 80.4062\n",
            "Epoch 11, Loss: 80.4041\n",
            "Epoch 12, Loss: 80.3986\n",
            "Epoch 13, Loss: 80.3939\n",
            "Epoch 14, Loss: 80.3972\n",
            "Epoch 15, Loss: 80.4018\n",
            "Epoch 16, Loss: 80.3944\n",
            "Epoch 17, Loss: 80.3868\n",
            "Epoch 18, Loss: 80.3998\n",
            "Epoch 19, Loss: 80.3953\n",
            "Epoch 20, Loss: 80.3903\n",
            "Final Model Metrics -> Accuracy: 0.5051, Precision: 0.5052, Recall: 0.5051, F1 Score: 0.5042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class CausalGraphSAGE_IoT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0):\n",
        "        super(CausalGraphSAGE_IoT, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, time_window):\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Strict causality: only allow edges from earlier time steps\n",
        "        mask = time_window[row] < time_window[col]\n",
        "        edge_index = edge_index[:, mask]\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        # No activation here — apply CrossEntropyLoss outside\n",
        "        return x\n",
        "\n",
        "# Assuming noisy_data_list is already created and populated with your IoT graph snapshots\n",
        "for t, data in enumerate(noisy_data_list):\n",
        "    if not hasattr(data, 'time_window'):\n",
        "        data.time_window = torch.full((data.num_nodes,), t, dtype=torch.long)\n",
        "\n",
        "# Training loop example for your IoT data:\n",
        "# You'll need to determine the correct in_channels based on your node features\n",
        "if noisy_data_list:\n",
        "    in_channels_iot = noisy_data_list[0].x.shape[1]\n",
        "    hidden_channels_iot = 64 # You can adjust this\n",
        "    out_channels_iot = 2 # Assuming binary classification (e.g., normal/attack)\n",
        "    model_iot = CausalGraphSAGE_IoT(in_channels=in_channels_iot, hidden_channels=hidden_channels_iot, out_channels=out_channels_iot)\n",
        "    optimizer_iot = torch.optim.Adam(model_iot.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    loss_fn_iot = torch.nn.CrossEntropyLoss()\n",
        "    num_epochs_iot = 20 # You can adjust this\n",
        "\n",
        "    model_iot.train()\n",
        "    for epoch in range(num_epochs_iot):\n",
        "        total_loss = 0\n",
        "        for t, data in enumerate(noisy_data_list):\n",
        "            optimizer_iot.zero_grad()\n",
        "\n",
        "            # Forward pass with causal filtering inside the model\n",
        "            out = model_iot(data.x, data.edge_index, data.time_window)\n",
        "\n",
        "            loss = loss_fn_iot(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer_iot.step()\n",
        "            total_loss += loss.item()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "else:\n",
        "    print(\"Error: noisy_data_list is empty. Cannot perform training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKGCKbd2mRNs",
        "outputId": "f8cec7a8-64e9-499d-b875-68ff528d1b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7207\n",
            "Epoch 1, Loss: 1.4330\n",
            "Epoch 1, Loss: 2.1362\n",
            "Epoch 1, Loss: 2.8413\n",
            "Epoch 1, Loss: 3.5436\n",
            "Epoch 1, Loss: 4.2469\n",
            "Epoch 1, Loss: 4.9727\n",
            "Epoch 1, Loss: 5.6600\n",
            "Epoch 1, Loss: 6.3647\n",
            "Epoch 1, Loss: 7.0621\n",
            "Epoch 1, Loss: 7.7607\n",
            "Epoch 1, Loss: 8.4628\n",
            "Epoch 1, Loss: 9.1622\n",
            "Epoch 1, Loss: 9.8625\n",
            "Epoch 1, Loss: 10.5612\n",
            "Epoch 1, Loss: 11.2597\n",
            "Epoch 1, Loss: 11.9562\n",
            "Epoch 1, Loss: 12.6518\n",
            "Epoch 1, Loss: 13.3491\n",
            "Epoch 1, Loss: 14.0426\n",
            "Epoch 1, Loss: 14.7369\n",
            "Epoch 1, Loss: 15.4364\n",
            "Epoch 1, Loss: 16.1317\n",
            "Epoch 1, Loss: 16.8268\n",
            "Epoch 1, Loss: 17.5234\n",
            "Epoch 1, Loss: 18.2213\n",
            "Epoch 1, Loss: 18.9167\n",
            "Epoch 1, Loss: 19.6152\n",
            "Epoch 1, Loss: 20.3125\n",
            "Epoch 1, Loss: 21.0077\n",
            "Epoch 1, Loss: 21.7052\n",
            "Epoch 1, Loss: 22.4005\n",
            "Epoch 1, Loss: 23.0927\n",
            "Epoch 1, Loss: 23.7884\n",
            "Epoch 1, Loss: 24.4837\n",
            "Epoch 1, Loss: 25.1791\n",
            "Epoch 1, Loss: 25.8735\n",
            "Epoch 1, Loss: 26.5676\n",
            "Epoch 1, Loss: 27.2632\n",
            "Epoch 1, Loss: 27.9578\n",
            "Epoch 1, Loss: 28.6510\n",
            "Epoch 1, Loss: 29.3450\n",
            "Epoch 1, Loss: 30.0388\n",
            "Epoch 1, Loss: 30.7355\n",
            "Epoch 1, Loss: 31.4329\n",
            "Epoch 1, Loss: 32.1272\n",
            "Epoch 1, Loss: 32.8214\n",
            "Epoch 1, Loss: 33.5147\n",
            "Epoch 1, Loss: 34.2094\n",
            "Epoch 1, Loss: 34.9048\n",
            "Epoch 1, Loss: 35.5991\n",
            "Epoch 1, Loss: 36.2937\n",
            "Epoch 1, Loss: 36.9884\n",
            "Epoch 1, Loss: 37.6818\n",
            "Epoch 1, Loss: 38.3763\n",
            "Epoch 1, Loss: 39.0706\n",
            "Epoch 1, Loss: 39.7662\n",
            "Epoch 1, Loss: 40.4599\n",
            "Epoch 1, Loss: 41.1560\n",
            "Epoch 1, Loss: 41.8501\n",
            "Epoch 1, Loss: 42.5460\n",
            "Epoch 1, Loss: 43.2417\n",
            "Epoch 1, Loss: 43.9360\n",
            "Epoch 1, Loss: 44.6310\n",
            "Epoch 1, Loss: 45.3239\n",
            "Epoch 1, Loss: 46.0188\n",
            "Epoch 1, Loss: 46.7139\n",
            "Epoch 1, Loss: 47.4083\n",
            "Epoch 1, Loss: 48.1031\n",
            "Epoch 1, Loss: 48.7986\n",
            "Epoch 1, Loss: 49.4928\n",
            "Epoch 1, Loss: 50.1869\n",
            "Epoch 1, Loss: 50.8820\n",
            "Epoch 1, Loss: 51.5757\n",
            "Epoch 1, Loss: 52.2692\n",
            "Epoch 1, Loss: 52.9662\n",
            "Epoch 1, Loss: 53.6599\n",
            "Epoch 1, Loss: 54.3600\n",
            "Epoch 1, Loss: 55.0599\n",
            "Epoch 1, Loss: 55.7530\n",
            "Epoch 1, Loss: 56.4454\n",
            "Epoch 1, Loss: 57.1404\n",
            "Epoch 1, Loss: 57.8365\n",
            "Epoch 1, Loss: 58.5322\n",
            "Epoch 1, Loss: 59.2267\n",
            "Epoch 1, Loss: 59.9214\n",
            "Epoch 1, Loss: 60.6164\n",
            "Epoch 1, Loss: 61.3105\n",
            "Epoch 1, Loss: 62.0063\n",
            "Epoch 1, Loss: 62.7040\n",
            "Epoch 1, Loss: 63.3993\n",
            "Epoch 1, Loss: 64.0951\n",
            "Epoch 1, Loss: 64.7899\n",
            "Epoch 1, Loss: 65.4861\n",
            "Epoch 1, Loss: 66.1844\n",
            "Epoch 1, Loss: 66.8846\n",
            "Epoch 1, Loss: 67.5812\n",
            "Epoch 1, Loss: 68.2773\n",
            "Epoch 1, Loss: 68.9779\n",
            "Epoch 1, Loss: 69.6725\n",
            "Epoch 1, Loss: 70.3686\n",
            "Epoch 1, Loss: 71.0638\n",
            "Epoch 1, Loss: 71.7601\n",
            "Epoch 1, Loss: 72.4581\n",
            "Epoch 1, Loss: 73.1550\n",
            "Epoch 1, Loss: 73.8549\n",
            "Epoch 1, Loss: 74.5521\n",
            "Epoch 1, Loss: 75.2462\n",
            "Epoch 1, Loss: 75.9461\n",
            "Epoch 1, Loss: 76.6398\n",
            "Epoch 1, Loss: 77.3356\n",
            "Epoch 1, Loss: 78.0308\n",
            "Epoch 1, Loss: 78.7267\n",
            "Epoch 1, Loss: 79.4222\n",
            "Epoch 1, Loss: 80.1169\n",
            "Epoch 1, Loss: 80.8147\n",
            "Epoch 2, Loss: 0.6940\n",
            "Epoch 2, Loss: 1.3878\n",
            "Epoch 2, Loss: 2.0817\n",
            "Epoch 2, Loss: 2.7775\n",
            "Epoch 2, Loss: 3.4721\n",
            "Epoch 2, Loss: 4.1695\n",
            "Epoch 2, Loss: 4.8501\n",
            "Epoch 2, Loss: 5.5450\n",
            "Epoch 2, Loss: 6.2387\n",
            "Epoch 2, Loss: 6.9330\n",
            "Epoch 2, Loss: 7.6280\n",
            "Epoch 2, Loss: 8.3237\n",
            "Epoch 2, Loss: 9.0159\n",
            "Epoch 2, Loss: 9.7107\n",
            "Epoch 2, Loss: 10.4067\n",
            "Epoch 2, Loss: 11.1003\n",
            "Epoch 2, Loss: 11.7935\n",
            "Epoch 2, Loss: 12.4887\n",
            "Epoch 2, Loss: 13.1828\n",
            "Epoch 2, Loss: 13.8794\n",
            "Epoch 2, Loss: 14.5744\n",
            "Epoch 2, Loss: 15.2697\n",
            "Epoch 2, Loss: 15.9638\n",
            "Epoch 2, Loss: 16.6581\n",
            "Epoch 2, Loss: 17.3520\n",
            "Epoch 2, Loss: 18.0468\n",
            "Epoch 2, Loss: 18.7410\n",
            "Epoch 2, Loss: 19.4371\n",
            "Epoch 2, Loss: 20.1331\n",
            "Epoch 2, Loss: 20.8263\n",
            "Epoch 2, Loss: 21.5216\n",
            "Epoch 2, Loss: 22.2162\n",
            "Epoch 2, Loss: 22.9087\n",
            "Epoch 2, Loss: 23.6034\n",
            "Epoch 2, Loss: 24.2979\n",
            "Epoch 2, Loss: 24.9923\n",
            "Epoch 2, Loss: 25.6855\n",
            "Epoch 2, Loss: 26.3788\n",
            "Epoch 2, Loss: 27.0727\n",
            "Epoch 2, Loss: 27.7661\n",
            "Epoch 2, Loss: 28.4592\n",
            "Epoch 2, Loss: 29.1526\n",
            "Epoch 2, Loss: 29.8455\n",
            "Epoch 2, Loss: 30.5399\n",
            "Epoch 2, Loss: 31.2353\n",
            "Epoch 2, Loss: 31.9287\n",
            "Epoch 2, Loss: 32.6224\n",
            "Epoch 2, Loss: 33.3144\n",
            "Epoch 2, Loss: 34.0094\n",
            "Epoch 2, Loss: 34.7031\n",
            "Epoch 2, Loss: 35.3968\n",
            "Epoch 2, Loss: 36.0900\n",
            "Epoch 2, Loss: 36.7831\n",
            "Epoch 2, Loss: 37.4761\n",
            "Epoch 2, Loss: 38.1703\n",
            "Epoch 2, Loss: 38.8641\n",
            "Epoch 2, Loss: 39.5592\n",
            "Epoch 2, Loss: 40.2520\n",
            "Epoch 2, Loss: 40.9463\n",
            "Epoch 2, Loss: 41.6397\n",
            "Epoch 2, Loss: 42.3343\n",
            "Epoch 2, Loss: 43.0285\n",
            "Epoch 2, Loss: 43.7227\n",
            "Epoch 2, Loss: 44.4175\n",
            "Epoch 2, Loss: 45.1106\n",
            "Epoch 2, Loss: 45.8044\n",
            "Epoch 2, Loss: 46.4981\n",
            "Epoch 2, Loss: 47.1926\n",
            "Epoch 2, Loss: 47.8871\n",
            "Epoch 2, Loss: 48.5813\n",
            "Epoch 2, Loss: 49.2742\n",
            "Epoch 2, Loss: 49.9675\n",
            "Epoch 2, Loss: 50.6618\n",
            "Epoch 2, Loss: 51.3550\n",
            "Epoch 2, Loss: 52.0482\n",
            "Epoch 2, Loss: 52.7442\n",
            "Epoch 2, Loss: 53.4375\n",
            "Epoch 2, Loss: 54.1314\n",
            "Epoch 2, Loss: 54.8270\n",
            "Epoch 2, Loss: 55.5195\n",
            "Epoch 2, Loss: 56.2121\n",
            "Epoch 2, Loss: 56.9057\n",
            "Epoch 2, Loss: 57.6006\n",
            "Epoch 2, Loss: 58.2950\n",
            "Epoch 2, Loss: 58.9884\n",
            "Epoch 2, Loss: 59.6828\n",
            "Epoch 2, Loss: 60.3772\n",
            "Epoch 2, Loss: 61.0709\n",
            "Epoch 2, Loss: 61.7655\n",
            "Epoch 2, Loss: 62.4615\n",
            "Epoch 2, Loss: 63.1571\n",
            "Epoch 2, Loss: 63.8515\n",
            "Epoch 2, Loss: 64.5482\n",
            "Epoch 2, Loss: 65.2441\n",
            "Epoch 2, Loss: 65.9385\n",
            "Epoch 2, Loss: 66.6296\n",
            "Epoch 2, Loss: 67.3247\n",
            "Epoch 2, Loss: 68.0187\n",
            "Epoch 2, Loss: 68.7158\n",
            "Epoch 2, Loss: 69.4093\n",
            "Epoch 2, Loss: 70.1044\n",
            "Epoch 2, Loss: 70.7979\n",
            "Epoch 2, Loss: 71.4865\n",
            "Epoch 2, Loss: 72.1818\n",
            "Epoch 2, Loss: 72.8769\n",
            "Epoch 2, Loss: 73.5705\n",
            "Epoch 2, Loss: 74.2639\n",
            "Epoch 2, Loss: 74.9572\n",
            "Epoch 2, Loss: 75.6526\n",
            "Epoch 2, Loss: 76.3545\n",
            "Epoch 2, Loss: 77.0488\n",
            "Epoch 2, Loss: 77.7434\n",
            "Epoch 2, Loss: 78.4371\n",
            "Epoch 2, Loss: 79.1330\n",
            "Epoch 2, Loss: 79.8279\n",
            "Epoch 2, Loss: 80.5228\n",
            "Epoch 3, Loss: 0.6937\n",
            "Epoch 3, Loss: 1.3877\n",
            "Epoch 3, Loss: 2.0824\n",
            "Epoch 3, Loss: 2.7766\n",
            "Epoch 3, Loss: 3.4695\n",
            "Epoch 3, Loss: 4.1624\n",
            "Epoch 3, Loss: 4.8280\n",
            "Epoch 3, Loss: 5.5199\n",
            "Epoch 3, Loss: 6.2132\n",
            "Epoch 3, Loss: 6.9069\n",
            "Epoch 3, Loss: 7.6022\n",
            "Epoch 3, Loss: 8.2972\n",
            "Epoch 3, Loss: 8.9893\n",
            "Epoch 3, Loss: 9.6828\n",
            "Epoch 3, Loss: 10.3770\n",
            "Epoch 3, Loss: 11.0714\n",
            "Epoch 3, Loss: 11.7644\n",
            "Epoch 3, Loss: 12.4592\n",
            "Epoch 3, Loss: 13.1528\n",
            "Epoch 3, Loss: 13.8444\n",
            "Epoch 3, Loss: 14.5371\n",
            "Epoch 3, Loss: 15.2317\n",
            "Epoch 3, Loss: 15.9246\n",
            "Epoch 3, Loss: 16.6177\n",
            "Epoch 3, Loss: 17.3113\n",
            "Epoch 3, Loss: 18.0052\n",
            "Epoch 3, Loss: 18.6991\n",
            "Epoch 3, Loss: 19.3946\n",
            "Epoch 3, Loss: 20.0890\n",
            "Epoch 3, Loss: 20.7814\n",
            "Epoch 3, Loss: 21.4758\n",
            "Epoch 3, Loss: 22.1693\n",
            "Epoch 3, Loss: 22.8614\n",
            "Epoch 3, Loss: 23.5552\n",
            "Epoch 3, Loss: 24.2493\n",
            "Epoch 3, Loss: 24.9432\n",
            "Epoch 3, Loss: 25.6363\n",
            "Epoch 3, Loss: 26.3294\n",
            "Epoch 3, Loss: 27.0236\n",
            "Epoch 3, Loss: 27.7168\n",
            "Epoch 3, Loss: 28.4094\n",
            "Epoch 3, Loss: 29.1026\n",
            "Epoch 3, Loss: 29.7952\n",
            "Epoch 3, Loss: 30.4894\n",
            "Epoch 3, Loss: 31.1850\n",
            "Epoch 3, Loss: 31.8785\n",
            "Epoch 3, Loss: 32.5721\n",
            "Epoch 3, Loss: 33.2636\n",
            "Epoch 3, Loss: 33.9584\n",
            "Epoch 3, Loss: 34.6518\n",
            "Epoch 3, Loss: 35.3454\n",
            "Epoch 3, Loss: 36.0385\n",
            "Epoch 3, Loss: 36.7314\n",
            "Epoch 3, Loss: 37.4245\n",
            "Epoch 3, Loss: 38.1186\n",
            "Epoch 3, Loss: 38.8124\n",
            "Epoch 3, Loss: 39.5072\n",
            "Epoch 3, Loss: 40.2001\n",
            "Epoch 3, Loss: 40.8943\n",
            "Epoch 3, Loss: 41.5878\n",
            "Epoch 3, Loss: 42.2818\n",
            "Epoch 3, Loss: 42.9755\n",
            "Epoch 3, Loss: 43.6696\n",
            "Epoch 3, Loss: 44.3644\n",
            "Epoch 3, Loss: 45.0572\n",
            "Epoch 3, Loss: 45.7508\n",
            "Epoch 3, Loss: 46.4433\n",
            "Epoch 3, Loss: 47.1381\n",
            "Epoch 3, Loss: 47.8322\n",
            "Epoch 3, Loss: 48.5259\n",
            "Epoch 3, Loss: 49.2186\n",
            "Epoch 3, Loss: 49.9115\n",
            "Epoch 3, Loss: 50.6055\n",
            "Epoch 3, Loss: 51.2985\n",
            "Epoch 3, Loss: 51.9917\n",
            "Epoch 3, Loss: 52.6876\n",
            "Epoch 3, Loss: 53.3808\n",
            "Epoch 3, Loss: 54.0754\n",
            "Epoch 3, Loss: 54.7714\n",
            "Epoch 3, Loss: 55.4640\n",
            "Epoch 3, Loss: 56.1567\n",
            "Epoch 3, Loss: 56.8496\n",
            "Epoch 3, Loss: 57.5438\n",
            "Epoch 3, Loss: 58.2380\n",
            "Epoch 3, Loss: 58.9311\n",
            "Epoch 3, Loss: 59.6251\n",
            "Epoch 3, Loss: 60.3193\n",
            "Epoch 3, Loss: 61.0128\n",
            "Epoch 3, Loss: 61.7067\n",
            "Epoch 3, Loss: 62.3994\n",
            "Epoch 3, Loss: 63.0930\n",
            "Epoch 3, Loss: 63.7872\n",
            "Epoch 3, Loss: 64.4831\n",
            "Epoch 3, Loss: 65.1786\n",
            "Epoch 3, Loss: 65.8725\n",
            "Epoch 3, Loss: 66.5620\n",
            "Epoch 3, Loss: 67.2572\n",
            "Epoch 3, Loss: 67.9505\n",
            "Epoch 3, Loss: 68.6470\n",
            "Epoch 3, Loss: 69.3401\n",
            "Epoch 3, Loss: 70.0344\n",
            "Epoch 3, Loss: 70.7271\n",
            "Epoch 3, Loss: 71.4133\n",
            "Epoch 3, Loss: 72.1075\n",
            "Epoch 3, Loss: 72.8016\n",
            "Epoch 3, Loss: 73.4965\n",
            "Epoch 3, Loss: 74.1925\n",
            "Epoch 3, Loss: 74.8854\n",
            "Epoch 3, Loss: 75.5817\n",
            "Epoch 3, Loss: 76.2714\n",
            "Epoch 3, Loss: 76.9656\n",
            "Epoch 3, Loss: 77.6598\n",
            "Epoch 3, Loss: 78.3533\n",
            "Epoch 3, Loss: 79.0489\n",
            "Epoch 3, Loss: 79.7422\n",
            "Epoch 3, Loss: 80.4364\n",
            "Epoch 4, Loss: 0.6927\n",
            "Epoch 4, Loss: 1.3858\n",
            "Epoch 4, Loss: 2.0799\n",
            "Epoch 4, Loss: 2.7734\n",
            "Epoch 4, Loss: 3.4659\n",
            "Epoch 4, Loss: 4.1592\n",
            "Epoch 4, Loss: 4.8198\n",
            "Epoch 4, Loss: 5.5120\n",
            "Epoch 4, Loss: 6.2053\n",
            "Epoch 4, Loss: 6.8990\n",
            "Epoch 4, Loss: 7.5941\n",
            "Epoch 4, Loss: 8.2886\n",
            "Epoch 4, Loss: 8.9812\n",
            "Epoch 4, Loss: 9.6748\n",
            "Epoch 4, Loss: 10.3683\n",
            "Epoch 4, Loss: 11.0625\n",
            "Epoch 4, Loss: 11.7554\n",
            "Epoch 4, Loss: 12.4495\n",
            "Epoch 4, Loss: 13.1431\n",
            "Epoch 4, Loss: 13.8356\n",
            "Epoch 4, Loss: 14.5273\n",
            "Epoch 4, Loss: 15.2218\n",
            "Epoch 4, Loss: 15.9151\n",
            "Epoch 4, Loss: 16.6081\n",
            "Epoch 4, Loss: 17.3013\n",
            "Epoch 4, Loss: 17.9952\n",
            "Epoch 4, Loss: 18.6888\n",
            "Epoch 4, Loss: 19.3842\n",
            "Epoch 4, Loss: 20.0789\n",
            "Epoch 4, Loss: 20.7711\n",
            "Epoch 4, Loss: 21.4653\n",
            "Epoch 4, Loss: 22.1591\n",
            "Epoch 4, Loss: 22.8519\n",
            "Epoch 4, Loss: 23.5457\n",
            "Epoch 4, Loss: 24.2399\n",
            "Epoch 4, Loss: 24.9337\n",
            "Epoch 4, Loss: 25.6271\n",
            "Epoch 4, Loss: 26.3199\n",
            "Epoch 4, Loss: 27.0139\n",
            "Epoch 4, Loss: 27.7073\n",
            "Epoch 4, Loss: 28.4000\n",
            "Epoch 4, Loss: 29.0933\n",
            "Epoch 4, Loss: 29.7862\n",
            "Epoch 4, Loss: 30.4804\n",
            "Epoch 4, Loss: 31.1753\n",
            "Epoch 4, Loss: 31.8684\n",
            "Epoch 4, Loss: 32.5621\n",
            "Epoch 4, Loss: 33.2537\n",
            "Epoch 4, Loss: 33.9481\n",
            "Epoch 4, Loss: 34.6415\n",
            "Epoch 4, Loss: 35.3349\n",
            "Epoch 4, Loss: 36.0280\n",
            "Epoch 4, Loss: 36.7207\n",
            "Epoch 4, Loss: 37.4138\n",
            "Epoch 4, Loss: 38.1076\n",
            "Epoch 4, Loss: 38.8011\n",
            "Epoch 4, Loss: 39.4958\n",
            "Epoch 4, Loss: 40.1887\n",
            "Epoch 4, Loss: 40.8825\n",
            "Epoch 4, Loss: 41.5759\n",
            "Epoch 4, Loss: 42.2699\n",
            "Epoch 4, Loss: 42.9631\n",
            "Epoch 4, Loss: 43.6572\n",
            "Epoch 4, Loss: 44.3518\n",
            "Epoch 4, Loss: 45.0446\n",
            "Epoch 4, Loss: 45.7380\n",
            "Epoch 4, Loss: 46.4301\n",
            "Epoch 4, Loss: 47.1247\n",
            "Epoch 4, Loss: 47.8188\n",
            "Epoch 4, Loss: 48.5125\n",
            "Epoch 4, Loss: 49.2052\n",
            "Epoch 4, Loss: 49.8980\n",
            "Epoch 4, Loss: 50.5917\n",
            "Epoch 4, Loss: 51.2844\n",
            "Epoch 4, Loss: 51.9774\n",
            "Epoch 4, Loss: 52.6729\n",
            "Epoch 4, Loss: 53.3661\n",
            "Epoch 4, Loss: 54.0603\n",
            "Epoch 4, Loss: 54.7560\n",
            "Epoch 4, Loss: 55.4483\n",
            "Epoch 4, Loss: 56.1411\n",
            "Epoch 4, Loss: 56.8341\n",
            "Epoch 4, Loss: 57.5280\n",
            "Epoch 4, Loss: 58.2221\n",
            "Epoch 4, Loss: 58.9153\n",
            "Epoch 4, Loss: 59.6094\n",
            "Epoch 4, Loss: 60.3034\n",
            "Epoch 4, Loss: 60.9969\n",
            "Epoch 4, Loss: 61.6905\n",
            "Epoch 4, Loss: 62.3837\n",
            "Epoch 4, Loss: 63.0773\n",
            "Epoch 4, Loss: 63.7714\n",
            "Epoch 4, Loss: 64.4658\n",
            "Epoch 4, Loss: 65.1604\n",
            "Epoch 4, Loss: 65.8541\n",
            "Epoch 4, Loss: 66.5430\n",
            "Epoch 4, Loss: 67.2378\n",
            "Epoch 4, Loss: 67.9310\n",
            "Epoch 4, Loss: 68.6269\n",
            "Epoch 4, Loss: 69.3197\n",
            "Epoch 4, Loss: 70.0137\n",
            "Epoch 4, Loss: 70.7060\n",
            "Epoch 4, Loss: 71.3904\n",
            "Epoch 4, Loss: 72.0839\n",
            "Epoch 4, Loss: 72.7773\n",
            "Epoch 4, Loss: 73.4691\n",
            "Epoch 4, Loss: 74.1622\n",
            "Epoch 4, Loss: 74.8550\n",
            "Epoch 4, Loss: 75.5477\n",
            "Epoch 4, Loss: 76.2400\n",
            "Epoch 4, Loss: 76.9342\n",
            "Epoch 4, Loss: 77.6279\n",
            "Epoch 4, Loss: 78.3206\n",
            "Epoch 4, Loss: 79.0157\n",
            "Epoch 4, Loss: 79.7086\n",
            "Epoch 4, Loss: 80.4028\n",
            "Epoch 5, Loss: 0.6932\n",
            "Epoch 5, Loss: 1.3869\n",
            "Epoch 5, Loss: 2.0814\n",
            "Epoch 5, Loss: 2.7748\n",
            "Epoch 5, Loss: 3.4673\n",
            "Epoch 5, Loss: 4.1599\n",
            "Epoch 5, Loss: 4.8140\n",
            "Epoch 5, Loss: 5.5054\n",
            "Epoch 5, Loss: 6.1985\n",
            "Epoch 5, Loss: 6.8921\n",
            "Epoch 5, Loss: 7.5871\n",
            "Epoch 5, Loss: 8.2812\n",
            "Epoch 5, Loss: 8.9737\n",
            "Epoch 5, Loss: 9.6666\n",
            "Epoch 5, Loss: 10.3601\n",
            "Epoch 5, Loss: 11.0542\n",
            "Epoch 5, Loss: 11.7473\n",
            "Epoch 5, Loss: 12.4415\n",
            "Epoch 5, Loss: 13.1349\n",
            "Epoch 5, Loss: 13.8272\n",
            "Epoch 5, Loss: 14.5194\n",
            "Epoch 5, Loss: 15.2137\n",
            "Epoch 5, Loss: 15.9067\n",
            "Epoch 5, Loss: 16.5996\n",
            "Epoch 5, Loss: 17.2925\n",
            "Epoch 5, Loss: 17.9867\n",
            "Epoch 5, Loss: 18.6805\n",
            "Epoch 5, Loss: 19.3760\n",
            "Epoch 5, Loss: 20.0696\n",
            "Epoch 5, Loss: 20.7617\n",
            "Epoch 5, Loss: 21.4561\n",
            "Epoch 5, Loss: 22.1492\n",
            "Epoch 5, Loss: 22.8416\n",
            "Epoch 5, Loss: 23.5349\n",
            "Epoch 5, Loss: 24.2287\n",
            "Epoch 5, Loss: 24.9224\n",
            "Epoch 5, Loss: 25.6154\n",
            "Epoch 5, Loss: 26.3081\n",
            "Epoch 5, Loss: 27.0021\n",
            "Epoch 5, Loss: 27.6955\n",
            "Epoch 5, Loss: 28.3880\n",
            "Epoch 5, Loss: 29.0814\n",
            "Epoch 5, Loss: 29.7740\n",
            "Epoch 5, Loss: 30.4683\n",
            "Epoch 5, Loss: 31.1637\n",
            "Epoch 5, Loss: 31.8569\n",
            "Epoch 5, Loss: 32.5508\n",
            "Epoch 5, Loss: 33.2422\n",
            "Epoch 5, Loss: 33.9367\n",
            "Epoch 5, Loss: 34.6301\n",
            "Epoch 5, Loss: 35.3235\n",
            "Epoch 5, Loss: 36.0165\n",
            "Epoch 5, Loss: 36.7092\n",
            "Epoch 5, Loss: 37.4022\n",
            "Epoch 5, Loss: 38.0960\n",
            "Epoch 5, Loss: 38.7893\n",
            "Epoch 5, Loss: 39.4839\n",
            "Epoch 5, Loss: 40.1769\n",
            "Epoch 5, Loss: 40.8707\n",
            "Epoch 5, Loss: 41.5643\n",
            "Epoch 5, Loss: 42.2579\n",
            "Epoch 5, Loss: 42.9508\n",
            "Epoch 5, Loss: 43.6449\n",
            "Epoch 5, Loss: 44.3396\n",
            "Epoch 5, Loss: 45.0324\n",
            "Epoch 5, Loss: 45.7258\n",
            "Epoch 5, Loss: 46.4177\n",
            "Epoch 5, Loss: 47.1122\n",
            "Epoch 5, Loss: 47.8062\n",
            "Epoch 5, Loss: 48.4997\n",
            "Epoch 5, Loss: 49.1921\n",
            "Epoch 5, Loss: 49.8848\n",
            "Epoch 5, Loss: 50.5784\n",
            "Epoch 5, Loss: 51.2713\n",
            "Epoch 5, Loss: 51.9646\n",
            "Epoch 5, Loss: 52.6601\n",
            "Epoch 5, Loss: 53.3533\n",
            "Epoch 5, Loss: 54.0465\n",
            "Epoch 5, Loss: 54.7417\n",
            "Epoch 5, Loss: 55.4339\n",
            "Epoch 5, Loss: 56.1269\n",
            "Epoch 5, Loss: 56.8199\n",
            "Epoch 5, Loss: 57.5136\n",
            "Epoch 5, Loss: 58.2075\n",
            "Epoch 5, Loss: 58.9005\n",
            "Epoch 5, Loss: 59.5947\n",
            "Epoch 5, Loss: 60.2885\n",
            "Epoch 5, Loss: 60.9818\n",
            "Epoch 5, Loss: 61.6754\n",
            "Epoch 5, Loss: 62.3673\n",
            "Epoch 5, Loss: 63.0600\n",
            "Epoch 5, Loss: 63.7535\n",
            "Epoch 5, Loss: 64.4477\n",
            "Epoch 5, Loss: 65.1421\n",
            "Epoch 5, Loss: 65.8353\n",
            "Epoch 5, Loss: 66.5241\n",
            "Epoch 5, Loss: 67.2188\n",
            "Epoch 5, Loss: 67.9117\n",
            "Epoch 5, Loss: 68.6073\n",
            "Epoch 5, Loss: 69.3001\n",
            "Epoch 5, Loss: 69.9936\n",
            "Epoch 5, Loss: 70.6858\n",
            "Epoch 5, Loss: 71.3684\n",
            "Epoch 5, Loss: 72.0616\n",
            "Epoch 5, Loss: 72.7546\n",
            "Epoch 5, Loss: 73.4466\n",
            "Epoch 5, Loss: 74.1404\n",
            "Epoch 5, Loss: 74.8332\n",
            "Epoch 5, Loss: 75.5272\n",
            "Epoch 5, Loss: 76.2163\n",
            "Epoch 5, Loss: 76.9102\n",
            "Epoch 5, Loss: 77.6036\n",
            "Epoch 5, Loss: 78.2964\n",
            "Epoch 5, Loss: 78.9912\n",
            "Epoch 5, Loss: 79.6840\n",
            "Epoch 5, Loss: 80.3767\n",
            "Epoch 6, Loss: 0.6926\n",
            "Epoch 6, Loss: 1.3859\n",
            "Epoch 6, Loss: 2.0802\n",
            "Epoch 6, Loss: 2.7734\n",
            "Epoch 6, Loss: 3.4657\n",
            "Epoch 6, Loss: 4.1583\n",
            "Epoch 6, Loss: 4.8078\n",
            "Epoch 6, Loss: 5.4992\n",
            "Epoch 6, Loss: 6.1925\n",
            "Epoch 6, Loss: 6.8862\n",
            "Epoch 6, Loss: 7.5811\n",
            "Epoch 6, Loss: 8.2753\n",
            "Epoch 6, Loss: 8.9679\n",
            "Epoch 6, Loss: 9.6608\n",
            "Epoch 6, Loss: 10.3541\n",
            "Epoch 6, Loss: 11.0478\n",
            "Epoch 6, Loss: 11.7406\n",
            "Epoch 6, Loss: 12.4347\n",
            "Epoch 6, Loss: 13.1281\n",
            "Epoch 6, Loss: 13.8226\n",
            "Epoch 6, Loss: 14.5145\n",
            "Epoch 6, Loss: 15.2087\n",
            "Epoch 6, Loss: 15.9020\n",
            "Epoch 6, Loss: 16.5951\n",
            "Epoch 6, Loss: 17.2878\n",
            "Epoch 6, Loss: 17.9817\n",
            "Epoch 6, Loss: 18.6752\n",
            "Epoch 6, Loss: 19.3704\n",
            "Epoch 6, Loss: 20.0649\n",
            "Epoch 6, Loss: 20.7569\n",
            "Epoch 6, Loss: 21.4511\n",
            "Epoch 6, Loss: 22.1449\n",
            "Epoch 6, Loss: 22.8378\n",
            "Epoch 6, Loss: 23.5313\n",
            "Epoch 6, Loss: 24.2252\n",
            "Epoch 6, Loss: 24.9189\n",
            "Epoch 6, Loss: 25.6121\n",
            "Epoch 6, Loss: 26.3047\n",
            "Epoch 6, Loss: 26.9983\n",
            "Epoch 6, Loss: 27.6919\n",
            "Epoch 6, Loss: 28.3846\n",
            "Epoch 6, Loss: 29.0781\n",
            "Epoch 6, Loss: 29.7707\n",
            "Epoch 6, Loss: 30.4647\n",
            "Epoch 6, Loss: 31.1593\n",
            "Epoch 6, Loss: 31.8522\n",
            "Epoch 6, Loss: 32.5461\n",
            "Epoch 6, Loss: 33.2378\n",
            "Epoch 6, Loss: 33.9320\n",
            "Epoch 6, Loss: 34.6251\n",
            "Epoch 6, Loss: 35.3185\n",
            "Epoch 6, Loss: 36.0115\n",
            "Epoch 6, Loss: 36.7040\n",
            "Epoch 6, Loss: 37.3971\n",
            "Epoch 6, Loss: 38.0906\n",
            "Epoch 6, Loss: 38.7840\n",
            "Epoch 6, Loss: 39.4786\n",
            "Epoch 6, Loss: 40.1715\n",
            "Epoch 6, Loss: 40.8650\n",
            "Epoch 6, Loss: 41.5584\n",
            "Epoch 6, Loss: 42.2520\n",
            "Epoch 6, Loss: 42.9449\n",
            "Epoch 6, Loss: 43.6389\n",
            "Epoch 6, Loss: 44.3335\n",
            "Epoch 6, Loss: 45.0262\n",
            "Epoch 6, Loss: 45.7196\n",
            "Epoch 6, Loss: 46.4115\n",
            "Epoch 6, Loss: 47.1059\n",
            "Epoch 6, Loss: 47.7999\n",
            "Epoch 6, Loss: 48.4934\n",
            "Epoch 6, Loss: 49.1859\n",
            "Epoch 6, Loss: 49.8785\n",
            "Epoch 6, Loss: 50.5719\n",
            "Epoch 6, Loss: 51.2646\n",
            "Epoch 6, Loss: 51.9574\n",
            "Epoch 6, Loss: 52.6528\n",
            "Epoch 6, Loss: 53.3458\n",
            "Epoch 6, Loss: 54.0396\n",
            "Epoch 6, Loss: 54.7349\n",
            "Epoch 6, Loss: 55.4271\n",
            "Epoch 6, Loss: 56.1201\n",
            "Epoch 6, Loss: 56.8132\n",
            "Epoch 6, Loss: 57.5068\n",
            "Epoch 6, Loss: 58.2005\n",
            "Epoch 6, Loss: 58.8935\n",
            "Epoch 6, Loss: 59.5878\n",
            "Epoch 6, Loss: 60.2816\n",
            "Epoch 6, Loss: 60.9749\n",
            "Epoch 6, Loss: 61.6682\n",
            "Epoch 6, Loss: 62.3606\n",
            "Epoch 6, Loss: 63.0538\n",
            "Epoch 6, Loss: 63.7476\n",
            "Epoch 6, Loss: 64.4407\n",
            "Epoch 6, Loss: 65.1343\n",
            "Epoch 6, Loss: 65.8271\n",
            "Epoch 6, Loss: 66.5154\n",
            "Epoch 6, Loss: 67.2100\n",
            "Epoch 6, Loss: 67.9029\n",
            "Epoch 6, Loss: 68.5985\n",
            "Epoch 6, Loss: 69.2910\n",
            "Epoch 6, Loss: 69.9844\n",
            "Epoch 6, Loss: 70.6764\n",
            "Epoch 6, Loss: 71.3578\n",
            "Epoch 6, Loss: 72.0508\n",
            "Epoch 6, Loss: 72.7436\n",
            "Epoch 6, Loss: 73.4343\n",
            "Epoch 6, Loss: 74.1271\n",
            "Epoch 6, Loss: 74.8199\n",
            "Epoch 6, Loss: 75.5128\n",
            "Epoch 6, Loss: 76.2026\n",
            "Epoch 6, Loss: 76.8965\n",
            "Epoch 6, Loss: 77.5898\n",
            "Epoch 6, Loss: 78.2822\n",
            "Epoch 6, Loss: 78.9768\n",
            "Epoch 6, Loss: 79.6684\n",
            "Epoch 6, Loss: 80.3620\n",
            "Epoch 7, Loss: 0.6931\n",
            "Epoch 7, Loss: 1.3869\n",
            "Epoch 7, Loss: 2.0813\n",
            "Epoch 7, Loss: 2.7744\n",
            "Epoch 7, Loss: 3.4668\n",
            "Epoch 7, Loss: 4.1592\n",
            "Epoch 7, Loss: 4.8043\n",
            "Epoch 7, Loss: 5.4954\n",
            "Epoch 7, Loss: 6.1885\n",
            "Epoch 7, Loss: 6.8821\n",
            "Epoch 7, Loss: 7.5769\n",
            "Epoch 7, Loss: 8.2705\n",
            "Epoch 7, Loss: 8.9627\n",
            "Epoch 7, Loss: 9.6555\n",
            "Epoch 7, Loss: 10.3485\n",
            "Epoch 7, Loss: 11.0423\n",
            "Epoch 7, Loss: 11.7353\n",
            "Epoch 7, Loss: 12.4291\n",
            "Epoch 7, Loss: 13.1225\n",
            "Epoch 7, Loss: 13.8143\n",
            "Epoch 7, Loss: 14.5063\n",
            "Epoch 7, Loss: 15.2004\n",
            "Epoch 7, Loss: 15.8936\n",
            "Epoch 7, Loss: 16.5866\n",
            "Epoch 7, Loss: 17.2792\n",
            "Epoch 7, Loss: 17.9735\n",
            "Epoch 7, Loss: 18.6671\n",
            "Epoch 7, Loss: 19.3624\n",
            "Epoch 7, Loss: 20.0558\n",
            "Epoch 7, Loss: 20.7477\n",
            "Epoch 7, Loss: 21.4420\n",
            "Epoch 7, Loss: 22.1353\n",
            "Epoch 7, Loss: 22.8277\n",
            "Epoch 7, Loss: 23.5208\n",
            "Epoch 7, Loss: 24.2145\n",
            "Epoch 7, Loss: 24.9079\n",
            "Epoch 7, Loss: 25.6010\n",
            "Epoch 7, Loss: 26.2935\n",
            "Epoch 7, Loss: 26.9872\n",
            "Epoch 7, Loss: 27.6805\n",
            "Epoch 7, Loss: 28.3728\n",
            "Epoch 7, Loss: 29.0663\n",
            "Epoch 7, Loss: 29.7589\n",
            "Epoch 7, Loss: 30.4533\n",
            "Epoch 7, Loss: 31.1485\n",
            "Epoch 7, Loss: 31.8415\n",
            "Epoch 7, Loss: 32.5355\n",
            "Epoch 7, Loss: 33.2269\n",
            "Epoch 7, Loss: 33.9212\n",
            "Epoch 7, Loss: 34.6143\n",
            "Epoch 7, Loss: 35.3077\n",
            "Epoch 7, Loss: 36.0007\n",
            "Epoch 7, Loss: 36.6934\n",
            "Epoch 7, Loss: 37.3862\n",
            "Epoch 7, Loss: 38.0799\n",
            "Epoch 7, Loss: 38.7732\n",
            "Epoch 7, Loss: 39.4677\n",
            "Epoch 7, Loss: 40.1606\n",
            "Epoch 7, Loss: 40.8542\n",
            "Epoch 7, Loss: 41.5477\n",
            "Epoch 7, Loss: 42.2412\n",
            "Epoch 7, Loss: 42.9339\n",
            "Epoch 7, Loss: 43.6280\n",
            "Epoch 7, Loss: 44.3226\n",
            "Epoch 7, Loss: 45.0153\n",
            "Epoch 7, Loss: 45.7087\n",
            "Epoch 7, Loss: 46.4005\n",
            "Epoch 7, Loss: 47.0950\n",
            "Epoch 7, Loss: 47.7889\n",
            "Epoch 7, Loss: 48.4824\n",
            "Epoch 7, Loss: 49.1748\n",
            "Epoch 7, Loss: 49.8673\n",
            "Epoch 7, Loss: 50.5607\n",
            "Epoch 7, Loss: 51.2535\n",
            "Epoch 7, Loss: 51.9465\n",
            "Epoch 7, Loss: 52.6420\n",
            "Epoch 7, Loss: 53.3348\n",
            "Epoch 7, Loss: 54.0276\n",
            "Epoch 7, Loss: 54.7225\n",
            "Epoch 7, Loss: 55.4146\n",
            "Epoch 7, Loss: 56.1077\n",
            "Epoch 7, Loss: 56.8007\n",
            "Epoch 7, Loss: 57.4941\n",
            "Epoch 7, Loss: 58.1879\n",
            "Epoch 7, Loss: 58.8809\n",
            "Epoch 7, Loss: 59.5753\n",
            "Epoch 7, Loss: 60.2692\n",
            "Epoch 7, Loss: 60.9624\n",
            "Epoch 7, Loss: 61.6557\n",
            "Epoch 7, Loss: 62.3473\n",
            "Epoch 7, Loss: 63.0399\n",
            "Epoch 7, Loss: 63.7332\n",
            "Epoch 7, Loss: 64.4262\n",
            "Epoch 7, Loss: 65.1200\n",
            "Epoch 7, Loss: 65.8126\n",
            "Epoch 7, Loss: 66.5009\n",
            "Epoch 7, Loss: 67.1953\n",
            "Epoch 7, Loss: 67.8881\n",
            "Epoch 7, Loss: 68.5833\n",
            "Epoch 7, Loss: 69.2758\n",
            "Epoch 7, Loss: 69.9690\n",
            "Epoch 7, Loss: 70.6605\n",
            "Epoch 7, Loss: 71.3409\n",
            "Epoch 7, Loss: 72.0337\n",
            "Epoch 7, Loss: 72.7264\n",
            "Epoch 7, Loss: 73.4171\n",
            "Epoch 7, Loss: 74.1097\n",
            "Epoch 7, Loss: 74.8025\n",
            "Epoch 7, Loss: 75.4954\n",
            "Epoch 7, Loss: 76.1845\n",
            "Epoch 7, Loss: 76.8784\n",
            "Epoch 7, Loss: 77.5717\n",
            "Epoch 7, Loss: 78.2637\n",
            "Epoch 7, Loss: 78.9584\n",
            "Epoch 7, Loss: 79.6506\n",
            "Epoch 7, Loss: 80.3428\n",
            "Epoch 8, Loss: 0.6928\n",
            "Epoch 8, Loss: 1.3863\n",
            "Epoch 8, Loss: 2.0805\n",
            "Epoch 8, Loss: 2.7734\n",
            "Epoch 8, Loss: 3.4655\n",
            "Epoch 8, Loss: 4.1580\n",
            "Epoch 8, Loss: 4.7972\n",
            "Epoch 8, Loss: 5.4882\n",
            "Epoch 8, Loss: 6.1816\n",
            "Epoch 8, Loss: 6.8753\n",
            "Epoch 8, Loss: 7.5701\n",
            "Epoch 8, Loss: 8.2638\n",
            "Epoch 8, Loss: 8.9563\n",
            "Epoch 8, Loss: 9.6489\n",
            "Epoch 8, Loss: 10.3416\n",
            "Epoch 8, Loss: 11.0355\n",
            "Epoch 8, Loss: 11.7284\n",
            "Epoch 8, Loss: 12.4222\n",
            "Epoch 8, Loss: 13.1157\n",
            "Epoch 8, Loss: 13.8083\n",
            "Epoch 8, Loss: 14.4996\n",
            "Epoch 8, Loss: 15.1936\n",
            "Epoch 8, Loss: 15.8869\n",
            "Epoch 8, Loss: 16.5800\n",
            "Epoch 8, Loss: 17.2727\n",
            "Epoch 8, Loss: 17.9668\n",
            "Epoch 8, Loss: 18.6602\n",
            "Epoch 8, Loss: 19.3554\n",
            "Epoch 8, Loss: 20.0493\n",
            "Epoch 8, Loss: 20.7412\n",
            "Epoch 8, Loss: 21.4354\n",
            "Epoch 8, Loss: 22.1289\n",
            "Epoch 8, Loss: 22.8216\n",
            "Epoch 8, Loss: 23.5147\n",
            "Epoch 8, Loss: 24.2083\n",
            "Epoch 8, Loss: 24.9018\n",
            "Epoch 8, Loss: 25.5949\n",
            "Epoch 8, Loss: 26.2872\n",
            "Epoch 8, Loss: 26.9808\n",
            "Epoch 8, Loss: 27.6743\n",
            "Epoch 8, Loss: 28.3668\n",
            "Epoch 8, Loss: 29.0602\n",
            "Epoch 8, Loss: 29.7527\n",
            "Epoch 8, Loss: 30.4467\n",
            "Epoch 8, Loss: 31.1415\n",
            "Epoch 8, Loss: 31.8345\n",
            "Epoch 8, Loss: 32.5284\n",
            "Epoch 8, Loss: 33.2200\n",
            "Epoch 8, Loss: 33.9142\n",
            "Epoch 8, Loss: 34.6072\n",
            "Epoch 8, Loss: 35.3005\n",
            "Epoch 8, Loss: 35.9935\n",
            "Epoch 8, Loss: 36.6861\n",
            "Epoch 8, Loss: 37.3789\n",
            "Epoch 8, Loss: 38.0725\n",
            "Epoch 8, Loss: 38.7659\n",
            "Epoch 8, Loss: 39.4603\n",
            "Epoch 8, Loss: 40.1533\n",
            "Epoch 8, Loss: 40.8467\n",
            "Epoch 8, Loss: 41.5401\n",
            "Epoch 8, Loss: 42.2337\n",
            "Epoch 8, Loss: 42.9264\n",
            "Epoch 8, Loss: 43.6206\n",
            "Epoch 8, Loss: 44.3151\n",
            "Epoch 8, Loss: 45.0078\n",
            "Epoch 8, Loss: 45.7011\n",
            "Epoch 8, Loss: 46.3928\n",
            "Epoch 8, Loss: 47.0872\n",
            "Epoch 8, Loss: 47.7812\n",
            "Epoch 8, Loss: 48.4747\n",
            "Epoch 8, Loss: 49.1671\n",
            "Epoch 8, Loss: 49.8596\n",
            "Epoch 8, Loss: 50.5529\n",
            "Epoch 8, Loss: 51.2456\n",
            "Epoch 8, Loss: 51.9385\n",
            "Epoch 8, Loss: 52.6339\n",
            "Epoch 8, Loss: 53.3267\n",
            "Epoch 8, Loss: 54.0199\n",
            "Epoch 8, Loss: 54.7151\n",
            "Epoch 8, Loss: 55.4073\n",
            "Epoch 8, Loss: 56.1005\n",
            "Epoch 8, Loss: 56.7937\n",
            "Epoch 8, Loss: 57.4868\n",
            "Epoch 8, Loss: 58.1804\n",
            "Epoch 8, Loss: 58.8733\n",
            "Epoch 8, Loss: 59.5679\n",
            "Epoch 8, Loss: 60.2617\n",
            "Epoch 8, Loss: 60.9550\n",
            "Epoch 8, Loss: 61.6481\n",
            "Epoch 8, Loss: 62.3397\n",
            "Epoch 8, Loss: 63.0324\n",
            "Epoch 8, Loss: 63.7260\n",
            "Epoch 8, Loss: 64.4184\n",
            "Epoch 8, Loss: 65.1118\n",
            "Epoch 8, Loss: 65.8043\n",
            "Epoch 8, Loss: 66.4923\n",
            "Epoch 8, Loss: 67.1866\n",
            "Epoch 8, Loss: 67.8793\n",
            "Epoch 8, Loss: 68.5747\n",
            "Epoch 8, Loss: 69.2672\n",
            "Epoch 8, Loss: 69.9603\n",
            "Epoch 8, Loss: 70.6517\n",
            "Epoch 8, Loss: 71.3311\n",
            "Epoch 8, Loss: 72.0236\n",
            "Epoch 8, Loss: 72.7162\n",
            "Epoch 8, Loss: 73.4068\n",
            "Epoch 8, Loss: 74.0995\n",
            "Epoch 8, Loss: 74.7924\n",
            "Epoch 8, Loss: 75.4856\n",
            "Epoch 8, Loss: 76.1745\n",
            "Epoch 8, Loss: 76.8682\n",
            "Epoch 8, Loss: 77.5615\n",
            "Epoch 8, Loss: 78.2534\n",
            "Epoch 8, Loss: 78.9481\n",
            "Epoch 8, Loss: 79.6399\n",
            "Epoch 8, Loss: 80.3316\n",
            "Epoch 9, Loss: 0.6930\n",
            "Epoch 9, Loss: 1.3866\n",
            "Epoch 9, Loss: 2.0811\n",
            "Epoch 9, Loss: 2.7741\n",
            "Epoch 9, Loss: 3.4663\n",
            "Epoch 9, Loss: 4.1586\n",
            "Epoch 9, Loss: 4.7941\n",
            "Epoch 9, Loss: 5.4849\n",
            "Epoch 9, Loss: 6.1785\n",
            "Epoch 9, Loss: 6.8721\n",
            "Epoch 9, Loss: 7.5670\n",
            "Epoch 9, Loss: 8.2607\n",
            "Epoch 9, Loss: 8.9529\n",
            "Epoch 9, Loss: 9.6455\n",
            "Epoch 9, Loss: 10.3383\n",
            "Epoch 9, Loss: 11.0319\n",
            "Epoch 9, Loss: 11.7247\n",
            "Epoch 9, Loss: 12.4185\n",
            "Epoch 9, Loss: 13.1121\n",
            "Epoch 9, Loss: 13.8040\n",
            "Epoch 9, Loss: 14.4954\n",
            "Epoch 9, Loss: 15.1894\n",
            "Epoch 9, Loss: 15.8827\n",
            "Epoch 9, Loss: 16.5758\n",
            "Epoch 9, Loss: 17.2682\n",
            "Epoch 9, Loss: 17.9625\n",
            "Epoch 9, Loss: 18.6559\n",
            "Epoch 9, Loss: 19.3514\n",
            "Epoch 9, Loss: 20.0446\n",
            "Epoch 9, Loss: 20.7364\n",
            "Epoch 9, Loss: 21.4306\n",
            "Epoch 9, Loss: 22.1235\n",
            "Epoch 9, Loss: 22.8159\n",
            "Epoch 9, Loss: 23.5089\n",
            "Epoch 9, Loss: 24.2027\n",
            "Epoch 9, Loss: 24.8958\n",
            "Epoch 9, Loss: 25.5888\n",
            "Epoch 9, Loss: 26.2811\n",
            "Epoch 9, Loss: 26.9747\n",
            "Epoch 9, Loss: 27.6678\n",
            "Epoch 9, Loss: 28.3602\n",
            "Epoch 9, Loss: 29.0536\n",
            "Epoch 9, Loss: 29.7463\n",
            "Epoch 9, Loss: 30.4408\n",
            "Epoch 9, Loss: 31.1364\n",
            "Epoch 9, Loss: 31.8295\n",
            "Epoch 9, Loss: 32.5234\n",
            "Epoch 9, Loss: 33.2148\n",
            "Epoch 9, Loss: 33.9090\n",
            "Epoch 9, Loss: 34.6022\n",
            "Epoch 9, Loss: 35.2957\n",
            "Epoch 9, Loss: 35.9887\n",
            "Epoch 9, Loss: 36.6814\n",
            "Epoch 9, Loss: 37.3741\n",
            "Epoch 9, Loss: 38.0678\n",
            "Epoch 9, Loss: 38.7613\n",
            "Epoch 9, Loss: 39.4557\n",
            "Epoch 9, Loss: 40.1489\n",
            "Epoch 9, Loss: 40.8424\n",
            "Epoch 9, Loss: 41.5357\n",
            "Epoch 9, Loss: 42.2291\n",
            "Epoch 9, Loss: 42.9218\n",
            "Epoch 9, Loss: 43.6160\n",
            "Epoch 9, Loss: 44.3106\n",
            "Epoch 9, Loss: 45.0032\n",
            "Epoch 9, Loss: 45.6965\n",
            "Epoch 9, Loss: 46.3882\n",
            "Epoch 9, Loss: 47.0827\n",
            "Epoch 9, Loss: 47.7765\n",
            "Epoch 9, Loss: 48.4699\n",
            "Epoch 9, Loss: 49.1623\n",
            "Epoch 9, Loss: 49.8548\n",
            "Epoch 9, Loss: 50.5480\n",
            "Epoch 9, Loss: 51.2408\n",
            "Epoch 9, Loss: 51.9338\n",
            "Epoch 9, Loss: 52.6292\n",
            "Epoch 9, Loss: 53.3217\n",
            "Epoch 9, Loss: 54.0141\n",
            "Epoch 9, Loss: 54.7090\n",
            "Epoch 9, Loss: 55.4013\n",
            "Epoch 9, Loss: 56.0945\n",
            "Epoch 9, Loss: 56.7877\n",
            "Epoch 9, Loss: 57.4809\n",
            "Epoch 9, Loss: 58.1744\n",
            "Epoch 9, Loss: 58.8674\n",
            "Epoch 9, Loss: 59.5618\n",
            "Epoch 9, Loss: 60.2557\n",
            "Epoch 9, Loss: 60.9489\n",
            "Epoch 9, Loss: 61.6419\n",
            "Epoch 9, Loss: 62.3333\n",
            "Epoch 9, Loss: 63.0257\n",
            "Epoch 9, Loss: 63.7196\n",
            "Epoch 9, Loss: 64.4117\n",
            "Epoch 9, Loss: 65.1048\n",
            "Epoch 9, Loss: 65.7972\n",
            "Epoch 9, Loss: 66.4851\n",
            "Epoch 9, Loss: 67.1793\n",
            "Epoch 9, Loss: 67.8722\n",
            "Epoch 9, Loss: 68.5676\n",
            "Epoch 9, Loss: 69.2601\n",
            "Epoch 9, Loss: 69.9531\n",
            "Epoch 9, Loss: 70.6444\n",
            "Epoch 9, Loss: 71.3225\n",
            "Epoch 9, Loss: 72.0153\n",
            "Epoch 9, Loss: 72.7080\n",
            "Epoch 9, Loss: 73.3980\n",
            "Epoch 9, Loss: 74.0903\n",
            "Epoch 9, Loss: 74.7832\n",
            "Epoch 9, Loss: 75.4763\n",
            "Epoch 9, Loss: 76.1648\n",
            "Epoch 9, Loss: 76.8586\n",
            "Epoch 9, Loss: 77.5518\n",
            "Epoch 9, Loss: 78.2436\n",
            "Epoch 9, Loss: 78.9383\n",
            "Epoch 9, Loss: 79.6296\n",
            "Epoch 9, Loss: 80.3217\n",
            "Epoch 10, Loss: 0.6929\n",
            "Epoch 10, Loss: 1.3864\n",
            "Epoch 10, Loss: 2.0806\n",
            "Epoch 10, Loss: 2.7734\n",
            "Epoch 10, Loss: 3.4656\n",
            "Epoch 10, Loss: 4.1581\n",
            "Epoch 10, Loss: 4.7894\n",
            "Epoch 10, Loss: 5.4802\n",
            "Epoch 10, Loss: 6.1736\n",
            "Epoch 10, Loss: 6.8673\n",
            "Epoch 10, Loss: 7.5622\n",
            "Epoch 10, Loss: 8.2558\n",
            "Epoch 10, Loss: 8.9482\n",
            "Epoch 10, Loss: 9.6406\n",
            "Epoch 10, Loss: 10.3333\n",
            "Epoch 10, Loss: 11.0269\n",
            "Epoch 10, Loss: 11.7199\n",
            "Epoch 10, Loss: 12.4139\n",
            "Epoch 10, Loss: 13.1076\n",
            "Epoch 10, Loss: 13.8004\n",
            "Epoch 10, Loss: 14.4913\n",
            "Epoch 10, Loss: 15.1852\n",
            "Epoch 10, Loss: 15.8785\n",
            "Epoch 10, Loss: 16.5717\n",
            "Epoch 10, Loss: 17.2641\n",
            "Epoch 10, Loss: 17.9584\n",
            "Epoch 10, Loss: 18.6516\n",
            "Epoch 10, Loss: 19.3469\n",
            "Epoch 10, Loss: 20.0408\n",
            "Epoch 10, Loss: 20.7325\n",
            "Epoch 10, Loss: 21.4269\n",
            "Epoch 10, Loss: 22.1203\n",
            "Epoch 10, Loss: 22.8130\n",
            "Epoch 10, Loss: 23.5059\n",
            "Epoch 10, Loss: 24.1995\n",
            "Epoch 10, Loss: 24.8928\n",
            "Epoch 10, Loss: 25.5859\n",
            "Epoch 10, Loss: 26.2781\n",
            "Epoch 10, Loss: 26.9716\n",
            "Epoch 10, Loss: 27.6650\n",
            "Epoch 10, Loss: 28.3574\n",
            "Epoch 10, Loss: 29.0510\n",
            "Epoch 10, Loss: 29.7434\n",
            "Epoch 10, Loss: 30.4374\n",
            "Epoch 10, Loss: 31.1323\n",
            "Epoch 10, Loss: 31.8252\n",
            "Epoch 10, Loss: 32.5192\n",
            "Epoch 10, Loss: 33.2106\n",
            "Epoch 10, Loss: 33.9046\n",
            "Epoch 10, Loss: 34.5975\n",
            "Epoch 10, Loss: 35.2909\n",
            "Epoch 10, Loss: 35.9837\n",
            "Epoch 10, Loss: 36.6763\n",
            "Epoch 10, Loss: 37.3691\n",
            "Epoch 10, Loss: 38.0626\n",
            "Epoch 10, Loss: 38.7562\n",
            "Epoch 10, Loss: 39.4504\n",
            "Epoch 10, Loss: 40.1434\n",
            "Epoch 10, Loss: 40.8368\n",
            "Epoch 10, Loss: 41.5301\n",
            "Epoch 10, Loss: 42.2236\n",
            "Epoch 10, Loss: 42.9162\n",
            "Epoch 10, Loss: 43.6104\n",
            "Epoch 10, Loss: 44.3051\n",
            "Epoch 10, Loss: 44.9977\n",
            "Epoch 10, Loss: 45.6909\n",
            "Epoch 10, Loss: 46.3826\n",
            "Epoch 10, Loss: 47.0768\n",
            "Epoch 10, Loss: 47.7706\n",
            "Epoch 10, Loss: 48.4639\n",
            "Epoch 10, Loss: 49.1562\n",
            "Epoch 10, Loss: 49.8486\n",
            "Epoch 10, Loss: 50.5419\n",
            "Epoch 10, Loss: 51.2344\n",
            "Epoch 10, Loss: 51.9272\n",
            "Epoch 10, Loss: 52.6225\n",
            "Epoch 10, Loss: 53.3150\n",
            "Epoch 10, Loss: 54.0077\n",
            "Epoch 10, Loss: 54.7026\n",
            "Epoch 10, Loss: 55.3947\n",
            "Epoch 10, Loss: 56.0880\n",
            "Epoch 10, Loss: 56.7812\n",
            "Epoch 10, Loss: 57.4743\n",
            "Epoch 10, Loss: 58.1676\n",
            "Epoch 10, Loss: 58.8605\n",
            "Epoch 10, Loss: 59.5552\n",
            "Epoch 10, Loss: 60.2489\n",
            "Epoch 10, Loss: 60.9422\n",
            "Epoch 10, Loss: 61.6352\n",
            "Epoch 10, Loss: 62.3267\n",
            "Epoch 10, Loss: 63.0192\n",
            "Epoch 10, Loss: 63.7131\n",
            "Epoch 10, Loss: 64.4050\n",
            "Epoch 10, Loss: 65.0980\n",
            "Epoch 10, Loss: 65.7903\n",
            "Epoch 10, Loss: 66.4783\n",
            "Epoch 10, Loss: 67.1725\n",
            "Epoch 10, Loss: 67.8652\n",
            "Epoch 10, Loss: 68.5607\n",
            "Epoch 10, Loss: 69.2532\n",
            "Epoch 10, Loss: 69.9460\n",
            "Epoch 10, Loss: 70.6373\n",
            "Epoch 10, Loss: 71.3145\n",
            "Epoch 10, Loss: 72.0072\n",
            "Epoch 10, Loss: 72.7000\n",
            "Epoch 10, Loss: 73.3899\n",
            "Epoch 10, Loss: 74.0823\n",
            "Epoch 10, Loss: 74.7753\n",
            "Epoch 10, Loss: 75.4679\n",
            "Epoch 10, Loss: 76.1563\n",
            "Epoch 10, Loss: 76.8502\n",
            "Epoch 10, Loss: 77.5434\n",
            "Epoch 10, Loss: 78.2349\n",
            "Epoch 10, Loss: 78.9296\n",
            "Epoch 10, Loss: 79.6211\n",
            "Epoch 10, Loss: 80.3126\n",
            "Epoch 11, Loss: 0.6932\n",
            "Epoch 11, Loss: 1.3870\n",
            "Epoch 11, Loss: 2.0814\n",
            "Epoch 11, Loss: 2.7742\n",
            "Epoch 11, Loss: 3.4664\n",
            "Epoch 11, Loss: 4.1585\n",
            "Epoch 11, Loss: 4.7840\n",
            "Epoch 11, Loss: 5.4747\n",
            "Epoch 11, Loss: 6.1682\n",
            "Epoch 11, Loss: 6.8618\n",
            "Epoch 11, Loss: 7.5568\n",
            "Epoch 11, Loss: 8.2503\n",
            "Epoch 11, Loss: 8.9423\n",
            "Epoch 11, Loss: 9.6349\n",
            "Epoch 11, Loss: 10.3276\n",
            "Epoch 11, Loss: 11.0211\n",
            "Epoch 11, Loss: 11.7140\n",
            "Epoch 11, Loss: 12.4079\n",
            "Epoch 11, Loss: 13.1018\n",
            "Epoch 11, Loss: 13.7936\n",
            "Epoch 11, Loss: 14.4847\n",
            "Epoch 11, Loss: 15.1787\n",
            "Epoch 11, Loss: 15.8719\n",
            "Epoch 11, Loss: 16.5650\n",
            "Epoch 11, Loss: 17.2573\n",
            "Epoch 11, Loss: 17.9516\n",
            "Epoch 11, Loss: 18.6447\n",
            "Epoch 11, Loss: 19.3402\n",
            "Epoch 11, Loss: 20.0335\n",
            "Epoch 11, Loss: 20.7252\n",
            "Epoch 11, Loss: 21.4195\n",
            "Epoch 11, Loss: 22.1124\n",
            "Epoch 11, Loss: 22.8048\n",
            "Epoch 11, Loss: 23.4977\n",
            "Epoch 11, Loss: 24.1912\n",
            "Epoch 11, Loss: 24.8843\n",
            "Epoch 11, Loss: 25.5771\n",
            "Epoch 11, Loss: 26.2691\n",
            "Epoch 11, Loss: 26.9626\n",
            "Epoch 11, Loss: 27.6556\n",
            "Epoch 11, Loss: 28.3480\n",
            "Epoch 11, Loss: 29.0416\n",
            "Epoch 11, Loss: 29.7340\n",
            "Epoch 11, Loss: 30.4283\n",
            "Epoch 11, Loss: 31.1240\n",
            "Epoch 11, Loss: 31.8169\n",
            "Epoch 11, Loss: 32.5110\n",
            "Epoch 11, Loss: 33.2022\n",
            "Epoch 11, Loss: 33.8961\n",
            "Epoch 11, Loss: 34.5891\n",
            "Epoch 11, Loss: 35.2826\n",
            "Epoch 11, Loss: 35.9755\n",
            "Epoch 11, Loss: 36.6682\n",
            "Epoch 11, Loss: 37.3606\n",
            "Epoch 11, Loss: 38.0542\n",
            "Epoch 11, Loss: 38.7475\n",
            "Epoch 11, Loss: 39.4418\n",
            "Epoch 11, Loss: 40.1349\n",
            "Epoch 11, Loss: 40.8282\n",
            "Epoch 11, Loss: 41.5218\n",
            "Epoch 11, Loss: 42.2152\n",
            "Epoch 11, Loss: 42.9077\n",
            "Epoch 11, Loss: 43.6018\n",
            "Epoch 11, Loss: 44.2965\n",
            "Epoch 11, Loss: 44.9892\n",
            "Epoch 11, Loss: 45.6826\n",
            "Epoch 11, Loss: 46.3741\n",
            "Epoch 11, Loss: 47.0686\n",
            "Epoch 11, Loss: 47.7625\n",
            "Epoch 11, Loss: 48.4561\n",
            "Epoch 11, Loss: 49.1486\n",
            "Epoch 11, Loss: 49.8410\n",
            "Epoch 11, Loss: 50.5341\n",
            "Epoch 11, Loss: 51.2269\n",
            "Epoch 11, Loss: 51.9199\n",
            "Epoch 11, Loss: 52.6154\n",
            "Epoch 11, Loss: 53.3079\n",
            "Epoch 11, Loss: 54.0002\n",
            "Epoch 11, Loss: 54.6953\n",
            "Epoch 11, Loss: 55.3878\n",
            "Epoch 11, Loss: 56.0810\n",
            "Epoch 11, Loss: 56.7741\n",
            "Epoch 11, Loss: 57.4671\n",
            "Epoch 11, Loss: 58.1605\n",
            "Epoch 11, Loss: 58.8534\n",
            "Epoch 11, Loss: 59.5479\n",
            "Epoch 11, Loss: 60.2418\n",
            "Epoch 11, Loss: 60.9350\n",
            "Epoch 11, Loss: 61.6278\n",
            "Epoch 11, Loss: 62.3188\n",
            "Epoch 11, Loss: 63.0111\n",
            "Epoch 11, Loss: 63.7050\n",
            "Epoch 11, Loss: 64.3963\n",
            "Epoch 11, Loss: 65.0892\n",
            "Epoch 11, Loss: 65.7813\n",
            "Epoch 11, Loss: 66.4692\n",
            "Epoch 11, Loss: 67.1631\n",
            "Epoch 11, Loss: 67.8559\n",
            "Epoch 11, Loss: 68.5509\n",
            "Epoch 11, Loss: 69.2435\n",
            "Epoch 11, Loss: 69.9363\n",
            "Epoch 11, Loss: 70.6272\n",
            "Epoch 11, Loss: 71.3036\n",
            "Epoch 11, Loss: 71.9962\n",
            "Epoch 11, Loss: 72.6890\n",
            "Epoch 11, Loss: 73.3786\n",
            "Epoch 11, Loss: 74.0708\n",
            "Epoch 11, Loss: 74.7642\n",
            "Epoch 11, Loss: 75.4571\n",
            "Epoch 11, Loss: 76.1449\n",
            "Epoch 11, Loss: 76.8388\n",
            "Epoch 11, Loss: 77.5323\n",
            "Epoch 11, Loss: 78.2235\n",
            "Epoch 11, Loss: 78.9184\n",
            "Epoch 11, Loss: 79.6098\n",
            "Epoch 11, Loss: 80.3010\n",
            "Epoch 12, Loss: 0.6928\n",
            "Epoch 12, Loss: 1.3864\n",
            "Epoch 12, Loss: 2.0807\n",
            "Epoch 12, Loss: 2.7733\n",
            "Epoch 12, Loss: 3.4653\n",
            "Epoch 12, Loss: 4.1579\n",
            "Epoch 12, Loss: 4.7803\n",
            "Epoch 12, Loss: 5.4712\n",
            "Epoch 12, Loss: 6.1647\n",
            "Epoch 12, Loss: 6.8583\n",
            "Epoch 12, Loss: 7.5533\n",
            "Epoch 12, Loss: 8.2467\n",
            "Epoch 12, Loss: 8.9388\n",
            "Epoch 12, Loss: 9.6312\n",
            "Epoch 12, Loss: 10.3235\n",
            "Epoch 12, Loss: 11.0173\n",
            "Epoch 12, Loss: 11.7103\n",
            "Epoch 12, Loss: 12.4042\n",
            "Epoch 12, Loss: 13.0981\n",
            "Epoch 12, Loss: 13.7906\n",
            "Epoch 12, Loss: 14.4814\n",
            "Epoch 12, Loss: 15.1751\n",
            "Epoch 12, Loss: 15.8682\n",
            "Epoch 12, Loss: 16.5615\n",
            "Epoch 12, Loss: 17.2543\n",
            "Epoch 12, Loss: 17.9488\n",
            "Epoch 12, Loss: 18.6420\n",
            "Epoch 12, Loss: 19.3376\n",
            "Epoch 12, Loss: 20.0311\n",
            "Epoch 12, Loss: 20.7228\n",
            "Epoch 12, Loss: 21.4169\n",
            "Epoch 12, Loss: 22.1102\n",
            "Epoch 12, Loss: 22.8029\n",
            "Epoch 12, Loss: 23.4957\n",
            "Epoch 12, Loss: 24.1892\n",
            "Epoch 12, Loss: 24.8826\n",
            "Epoch 12, Loss: 25.5755\n",
            "Epoch 12, Loss: 26.2676\n",
            "Epoch 12, Loss: 26.9611\n",
            "Epoch 12, Loss: 27.6542\n",
            "Epoch 12, Loss: 28.3465\n",
            "Epoch 12, Loss: 29.0401\n",
            "Epoch 12, Loss: 29.7325\n",
            "Epoch 12, Loss: 30.4265\n",
            "Epoch 12, Loss: 31.1214\n",
            "Epoch 12, Loss: 31.8143\n",
            "Epoch 12, Loss: 32.5084\n",
            "Epoch 12, Loss: 33.1995\n",
            "Epoch 12, Loss: 33.8933\n",
            "Epoch 12, Loss: 34.5861\n",
            "Epoch 12, Loss: 35.2794\n",
            "Epoch 12, Loss: 35.9723\n",
            "Epoch 12, Loss: 36.6651\n",
            "Epoch 12, Loss: 37.3578\n",
            "Epoch 12, Loss: 38.0511\n",
            "Epoch 12, Loss: 38.7446\n",
            "Epoch 12, Loss: 39.4388\n",
            "Epoch 12, Loss: 40.1318\n",
            "Epoch 12, Loss: 40.8251\n",
            "Epoch 12, Loss: 41.5185\n",
            "Epoch 12, Loss: 42.2118\n",
            "Epoch 12, Loss: 42.9043\n",
            "Epoch 12, Loss: 43.5984\n",
            "Epoch 12, Loss: 44.2931\n",
            "Epoch 12, Loss: 44.9857\n",
            "Epoch 12, Loss: 45.6789\n",
            "Epoch 12, Loss: 46.3707\n",
            "Epoch 12, Loss: 47.0648\n",
            "Epoch 12, Loss: 47.7585\n",
            "Epoch 12, Loss: 48.4519\n",
            "Epoch 12, Loss: 49.1441\n",
            "Epoch 12, Loss: 49.8365\n",
            "Epoch 12, Loss: 50.5296\n",
            "Epoch 12, Loss: 51.2222\n",
            "Epoch 12, Loss: 51.9152\n",
            "Epoch 12, Loss: 52.6104\n",
            "Epoch 12, Loss: 53.3028\n",
            "Epoch 12, Loss: 53.9953\n",
            "Epoch 12, Loss: 54.6902\n",
            "Epoch 12, Loss: 55.3825\n",
            "Epoch 12, Loss: 56.0757\n",
            "Epoch 12, Loss: 56.7689\n",
            "Epoch 12, Loss: 57.4620\n",
            "Epoch 12, Loss: 58.1552\n",
            "Epoch 12, Loss: 58.8481\n",
            "Epoch 12, Loss: 59.5428\n",
            "Epoch 12, Loss: 60.2367\n",
            "Epoch 12, Loss: 60.9298\n",
            "Epoch 12, Loss: 61.6228\n",
            "Epoch 12, Loss: 62.3140\n",
            "Epoch 12, Loss: 63.0062\n",
            "Epoch 12, Loss: 63.7005\n",
            "Epoch 12, Loss: 64.3920\n",
            "Epoch 12, Loss: 65.0848\n",
            "Epoch 12, Loss: 65.7768\n",
            "Epoch 12, Loss: 66.4644\n",
            "Epoch 12, Loss: 67.1583\n",
            "Epoch 12, Loss: 67.8510\n",
            "Epoch 12, Loss: 68.5462\n",
            "Epoch 12, Loss: 69.2387\n",
            "Epoch 12, Loss: 69.9313\n",
            "Epoch 12, Loss: 70.6222\n",
            "Epoch 12, Loss: 71.2978\n",
            "Epoch 12, Loss: 71.9905\n",
            "Epoch 12, Loss: 72.6834\n",
            "Epoch 12, Loss: 73.3731\n",
            "Epoch 12, Loss: 74.0653\n",
            "Epoch 12, Loss: 74.7586\n",
            "Epoch 12, Loss: 75.4513\n",
            "Epoch 12, Loss: 76.1395\n",
            "Epoch 12, Loss: 76.8335\n",
            "Epoch 12, Loss: 77.5270\n",
            "Epoch 12, Loss: 78.2176\n",
            "Epoch 12, Loss: 78.9122\n",
            "Epoch 12, Loss: 79.6032\n",
            "Epoch 12, Loss: 80.2943\n",
            "Epoch 13, Loss: 0.6930\n",
            "Epoch 13, Loss: 1.3871\n",
            "Epoch 13, Loss: 2.0815\n",
            "Epoch 13, Loss: 2.7742\n",
            "Epoch 13, Loss: 3.4663\n",
            "Epoch 13, Loss: 4.1584\n",
            "Epoch 13, Loss: 4.7754\n",
            "Epoch 13, Loss: 5.4661\n",
            "Epoch 13, Loss: 6.1595\n",
            "Epoch 13, Loss: 6.8532\n",
            "Epoch 13, Loss: 7.5482\n",
            "Epoch 13, Loss: 8.2416\n",
            "Epoch 13, Loss: 8.9334\n",
            "Epoch 13, Loss: 9.6257\n",
            "Epoch 13, Loss: 10.3182\n",
            "Epoch 13, Loss: 11.0116\n",
            "Epoch 13, Loss: 11.7043\n",
            "Epoch 13, Loss: 12.3984\n",
            "Epoch 13, Loss: 13.0923\n",
            "Epoch 13, Loss: 13.7842\n",
            "Epoch 13, Loss: 14.4753\n",
            "Epoch 13, Loss: 15.1690\n",
            "Epoch 13, Loss: 15.8621\n",
            "Epoch 13, Loss: 16.5552\n",
            "Epoch 13, Loss: 17.2478\n",
            "Epoch 13, Loss: 17.9420\n",
            "Epoch 13, Loss: 18.6353\n",
            "Epoch 13, Loss: 19.3309\n",
            "Epoch 13, Loss: 20.0241\n",
            "Epoch 13, Loss: 20.7156\n",
            "Epoch 13, Loss: 21.4099\n",
            "Epoch 13, Loss: 22.1030\n",
            "Epoch 13, Loss: 22.7953\n",
            "Epoch 13, Loss: 23.4882\n",
            "Epoch 13, Loss: 24.1817\n",
            "Epoch 13, Loss: 24.8748\n",
            "Epoch 13, Loss: 25.5673\n",
            "Epoch 13, Loss: 26.2591\n",
            "Epoch 13, Loss: 26.9527\n",
            "Epoch 13, Loss: 27.6454\n",
            "Epoch 13, Loss: 28.3376\n",
            "Epoch 13, Loss: 29.0315\n",
            "Epoch 13, Loss: 29.7238\n",
            "Epoch 13, Loss: 30.4182\n",
            "Epoch 13, Loss: 31.1139\n",
            "Epoch 13, Loss: 31.8067\n",
            "Epoch 13, Loss: 32.5008\n",
            "Epoch 13, Loss: 33.1920\n",
            "Epoch 13, Loss: 33.8856\n",
            "Epoch 13, Loss: 34.5785\n",
            "Epoch 13, Loss: 35.2718\n",
            "Epoch 13, Loss: 35.9647\n",
            "Epoch 13, Loss: 36.6573\n",
            "Epoch 13, Loss: 37.3497\n",
            "Epoch 13, Loss: 38.0433\n",
            "Epoch 13, Loss: 38.7363\n",
            "Epoch 13, Loss: 39.4305\n",
            "Epoch 13, Loss: 40.1234\n",
            "Epoch 13, Loss: 40.8169\n",
            "Epoch 13, Loss: 41.5103\n",
            "Epoch 13, Loss: 42.2035\n",
            "Epoch 13, Loss: 42.8961\n",
            "Epoch 13, Loss: 43.5901\n",
            "Epoch 13, Loss: 44.2848\n",
            "Epoch 13, Loss: 44.9774\n",
            "Epoch 13, Loss: 45.6708\n",
            "Epoch 13, Loss: 46.3624\n",
            "Epoch 13, Loss: 47.0567\n",
            "Epoch 13, Loss: 47.7508\n",
            "Epoch 13, Loss: 48.4444\n",
            "Epoch 13, Loss: 49.1366\n",
            "Epoch 13, Loss: 49.8289\n",
            "Epoch 13, Loss: 50.5221\n",
            "Epoch 13, Loss: 51.2150\n",
            "Epoch 13, Loss: 51.9082\n",
            "Epoch 13, Loss: 52.6036\n",
            "Epoch 13, Loss: 53.2959\n",
            "Epoch 13, Loss: 53.9882\n",
            "Epoch 13, Loss: 54.6833\n",
            "Epoch 13, Loss: 55.3756\n",
            "Epoch 13, Loss: 56.0688\n",
            "Epoch 13, Loss: 56.7619\n",
            "Epoch 13, Loss: 57.4547\n",
            "Epoch 13, Loss: 58.1478\n",
            "Epoch 13, Loss: 58.8408\n",
            "Epoch 13, Loss: 59.5354\n",
            "Epoch 13, Loss: 60.2295\n",
            "Epoch 13, Loss: 60.9227\n",
            "Epoch 13, Loss: 61.6156\n",
            "Epoch 13, Loss: 62.3063\n",
            "Epoch 13, Loss: 62.9986\n",
            "Epoch 13, Loss: 63.6929\n",
            "Epoch 13, Loss: 64.3840\n",
            "Epoch 13, Loss: 65.0766\n",
            "Epoch 13, Loss: 65.7687\n",
            "Epoch 13, Loss: 66.4562\n",
            "Epoch 13, Loss: 67.1499\n",
            "Epoch 13, Loss: 67.8426\n",
            "Epoch 13, Loss: 68.5375\n",
            "Epoch 13, Loss: 69.2299\n",
            "Epoch 13, Loss: 69.9225\n",
            "Epoch 13, Loss: 70.6135\n",
            "Epoch 13, Loss: 71.2880\n",
            "Epoch 13, Loss: 71.9807\n",
            "Epoch 13, Loss: 72.6735\n",
            "Epoch 13, Loss: 73.3628\n",
            "Epoch 13, Loss: 74.0546\n",
            "Epoch 13, Loss: 74.7482\n",
            "Epoch 13, Loss: 75.4411\n",
            "Epoch 13, Loss: 76.1288\n",
            "Epoch 13, Loss: 76.8228\n",
            "Epoch 13, Loss: 77.5165\n",
            "Epoch 13, Loss: 78.2077\n",
            "Epoch 13, Loss: 78.9028\n",
            "Epoch 13, Loss: 79.5940\n",
            "Epoch 13, Loss: 80.2846\n",
            "Epoch 14, Loss: 0.6927\n",
            "Epoch 14, Loss: 1.3866\n",
            "Epoch 14, Loss: 2.0808\n",
            "Epoch 14, Loss: 2.7734\n",
            "Epoch 14, Loss: 3.4653\n",
            "Epoch 14, Loss: 4.1576\n",
            "Epoch 14, Loss: 4.7702\n",
            "Epoch 14, Loss: 5.4610\n",
            "Epoch 14, Loss: 6.1546\n",
            "Epoch 14, Loss: 6.8482\n",
            "Epoch 14, Loss: 7.5432\n",
            "Epoch 14, Loss: 8.2367\n",
            "Epoch 14, Loss: 8.9288\n",
            "Epoch 14, Loss: 9.6210\n",
            "Epoch 14, Loss: 10.3133\n",
            "Epoch 14, Loss: 11.0071\n",
            "Epoch 14, Loss: 11.6999\n",
            "Epoch 14, Loss: 12.3940\n",
            "Epoch 14, Loss: 13.0880\n",
            "Epoch 14, Loss: 13.7811\n",
            "Epoch 14, Loss: 14.4716\n",
            "Epoch 14, Loss: 15.1652\n",
            "Epoch 14, Loss: 15.8583\n",
            "Epoch 14, Loss: 16.5515\n",
            "Epoch 14, Loss: 17.2440\n",
            "Epoch 14, Loss: 17.9384\n",
            "Epoch 14, Loss: 18.6314\n",
            "Epoch 14, Loss: 19.3272\n",
            "Epoch 14, Loss: 20.0210\n",
            "Epoch 14, Loss: 20.7127\n",
            "Epoch 14, Loss: 21.4071\n",
            "Epoch 14, Loss: 22.1004\n",
            "Epoch 14, Loss: 22.7931\n",
            "Epoch 14, Loss: 23.4859\n",
            "Epoch 14, Loss: 24.1794\n",
            "Epoch 14, Loss: 24.8727\n",
            "Epoch 14, Loss: 25.5653\n",
            "Epoch 14, Loss: 26.2571\n",
            "Epoch 14, Loss: 26.9506\n",
            "Epoch 14, Loss: 27.6436\n",
            "Epoch 14, Loss: 28.3359\n",
            "Epoch 14, Loss: 29.0297\n",
            "Epoch 14, Loss: 29.7220\n",
            "Epoch 14, Loss: 30.4159\n",
            "Epoch 14, Loss: 31.1111\n",
            "Epoch 14, Loss: 31.8038\n",
            "Epoch 14, Loss: 32.4980\n",
            "Epoch 14, Loss: 33.1892\n",
            "Epoch 14, Loss: 33.8828\n",
            "Epoch 14, Loss: 34.5753\n",
            "Epoch 14, Loss: 35.2685\n",
            "Epoch 14, Loss: 35.9615\n",
            "Epoch 14, Loss: 36.6542\n",
            "Epoch 14, Loss: 37.3466\n",
            "Epoch 14, Loss: 38.0401\n",
            "Epoch 14, Loss: 38.7333\n",
            "Epoch 14, Loss: 39.4274\n",
            "Epoch 14, Loss: 40.1203\n",
            "Epoch 14, Loss: 40.8138\n",
            "Epoch 14, Loss: 41.5073\n",
            "Epoch 14, Loss: 42.2004\n",
            "Epoch 14, Loss: 42.8929\n",
            "Epoch 14, Loss: 43.5869\n",
            "Epoch 14, Loss: 44.2815\n",
            "Epoch 14, Loss: 44.9741\n",
            "Epoch 14, Loss: 45.6674\n",
            "Epoch 14, Loss: 46.3591\n",
            "Epoch 14, Loss: 47.0532\n",
            "Epoch 14, Loss: 47.7471\n",
            "Epoch 14, Loss: 48.4406\n",
            "Epoch 14, Loss: 49.1328\n",
            "Epoch 14, Loss: 49.8251\n",
            "Epoch 14, Loss: 50.5183\n",
            "Epoch 14, Loss: 51.2109\n",
            "Epoch 14, Loss: 51.9040\n",
            "Epoch 14, Loss: 52.5993\n",
            "Epoch 14, Loss: 53.2914\n",
            "Epoch 14, Loss: 53.9836\n",
            "Epoch 14, Loss: 54.6785\n",
            "Epoch 14, Loss: 55.3708\n",
            "Epoch 14, Loss: 56.0640\n",
            "Epoch 14, Loss: 56.7571\n",
            "Epoch 14, Loss: 57.4500\n",
            "Epoch 14, Loss: 58.1429\n",
            "Epoch 14, Loss: 58.8358\n",
            "Epoch 14, Loss: 59.5305\n",
            "Epoch 14, Loss: 60.2244\n",
            "Epoch 14, Loss: 60.9175\n",
            "Epoch 14, Loss: 61.6105\n",
            "Epoch 14, Loss: 62.3016\n",
            "Epoch 14, Loss: 62.9941\n",
            "Epoch 14, Loss: 63.6886\n",
            "Epoch 14, Loss: 64.3799\n",
            "Epoch 14, Loss: 65.0724\n",
            "Epoch 14, Loss: 65.7643\n",
            "Epoch 14, Loss: 66.4516\n",
            "Epoch 14, Loss: 67.1452\n",
            "Epoch 14, Loss: 67.8379\n",
            "Epoch 14, Loss: 68.5331\n",
            "Epoch 14, Loss: 69.2258\n",
            "Epoch 14, Loss: 69.9183\n",
            "Epoch 14, Loss: 70.6089\n",
            "Epoch 14, Loss: 71.2833\n",
            "Epoch 14, Loss: 71.9758\n",
            "Epoch 14, Loss: 72.6687\n",
            "Epoch 14, Loss: 73.3582\n",
            "Epoch 14, Loss: 74.0502\n",
            "Epoch 14, Loss: 74.7438\n",
            "Epoch 14, Loss: 75.4368\n",
            "Epoch 14, Loss: 76.1245\n",
            "Epoch 14, Loss: 76.8186\n",
            "Epoch 14, Loss: 77.5122\n",
            "Epoch 14, Loss: 78.2029\n",
            "Epoch 14, Loss: 78.8975\n",
            "Epoch 14, Loss: 79.5891\n",
            "Epoch 14, Loss: 80.2789\n",
            "Epoch 15, Loss: 0.6930\n",
            "Epoch 15, Loss: 1.3869\n",
            "Epoch 15, Loss: 2.0814\n",
            "Epoch 15, Loss: 2.7741\n",
            "Epoch 15, Loss: 3.4661\n",
            "Epoch 15, Loss: 4.1583\n",
            "Epoch 15, Loss: 4.7647\n",
            "Epoch 15, Loss: 5.4554\n",
            "Epoch 15, Loss: 6.1489\n",
            "Epoch 15, Loss: 6.8425\n",
            "Epoch 15, Loss: 7.5373\n",
            "Epoch 15, Loss: 8.2305\n",
            "Epoch 15, Loss: 8.9222\n",
            "Epoch 15, Loss: 9.6145\n",
            "Epoch 15, Loss: 10.3070\n",
            "Epoch 15, Loss: 11.0009\n",
            "Epoch 15, Loss: 11.6936\n",
            "Epoch 15, Loss: 12.3875\n",
            "Epoch 15, Loss: 13.0816\n",
            "Epoch 15, Loss: 13.7735\n",
            "Epoch 15, Loss: 14.4645\n",
            "Epoch 15, Loss: 15.1582\n",
            "Epoch 15, Loss: 15.8514\n",
            "Epoch 15, Loss: 16.5442\n",
            "Epoch 15, Loss: 17.2366\n",
            "Epoch 15, Loss: 17.9309\n",
            "Epoch 15, Loss: 18.6241\n",
            "Epoch 15, Loss: 19.3197\n",
            "Epoch 15, Loss: 20.0128\n",
            "Epoch 15, Loss: 20.7045\n",
            "Epoch 15, Loss: 21.3987\n",
            "Epoch 15, Loss: 22.0914\n",
            "Epoch 15, Loss: 22.7839\n",
            "Epoch 15, Loss: 23.4767\n",
            "Epoch 15, Loss: 24.1702\n",
            "Epoch 15, Loss: 24.8633\n",
            "Epoch 15, Loss: 25.5556\n",
            "Epoch 15, Loss: 26.2474\n",
            "Epoch 15, Loss: 26.9410\n",
            "Epoch 15, Loss: 27.6334\n",
            "Epoch 15, Loss: 28.3258\n",
            "Epoch 15, Loss: 29.0196\n",
            "Epoch 15, Loss: 29.7121\n",
            "Epoch 15, Loss: 30.4063\n",
            "Epoch 15, Loss: 31.1020\n",
            "Epoch 15, Loss: 31.7948\n",
            "Epoch 15, Loss: 32.4890\n",
            "Epoch 15, Loss: 33.1800\n",
            "Epoch 15, Loss: 33.8734\n",
            "Epoch 15, Loss: 34.5662\n",
            "Epoch 15, Loss: 35.2596\n",
            "Epoch 15, Loss: 35.9526\n",
            "Epoch 15, Loss: 36.6454\n",
            "Epoch 15, Loss: 37.3376\n",
            "Epoch 15, Loss: 38.0312\n",
            "Epoch 15, Loss: 38.7243\n",
            "Epoch 15, Loss: 39.4184\n",
            "Epoch 15, Loss: 40.1113\n",
            "Epoch 15, Loss: 40.8047\n",
            "Epoch 15, Loss: 41.4982\n",
            "Epoch 15, Loss: 42.1913\n",
            "Epoch 15, Loss: 42.8840\n",
            "Epoch 15, Loss: 43.5779\n",
            "Epoch 15, Loss: 44.2725\n",
            "Epoch 15, Loss: 44.9651\n",
            "Epoch 15, Loss: 45.6584\n",
            "Epoch 15, Loss: 46.3502\n",
            "Epoch 15, Loss: 47.0444\n",
            "Epoch 15, Loss: 47.7385\n",
            "Epoch 15, Loss: 48.4322\n",
            "Epoch 15, Loss: 49.1245\n",
            "Epoch 15, Loss: 49.8167\n",
            "Epoch 15, Loss: 50.5098\n",
            "Epoch 15, Loss: 51.2026\n",
            "Epoch 15, Loss: 51.8960\n",
            "Epoch 15, Loss: 52.5912\n",
            "Epoch 15, Loss: 53.2833\n",
            "Epoch 15, Loss: 53.9753\n",
            "Epoch 15, Loss: 54.6706\n",
            "Epoch 15, Loss: 55.3630\n",
            "Epoch 15, Loss: 56.0563\n",
            "Epoch 15, Loss: 56.7494\n",
            "Epoch 15, Loss: 57.4421\n",
            "Epoch 15, Loss: 58.1351\n",
            "Epoch 15, Loss: 58.8280\n",
            "Epoch 15, Loss: 59.5227\n",
            "Epoch 15, Loss: 60.2166\n",
            "Epoch 15, Loss: 60.9098\n",
            "Epoch 15, Loss: 61.6027\n",
            "Epoch 15, Loss: 62.2932\n",
            "Epoch 15, Loss: 62.9852\n",
            "Epoch 15, Loss: 63.6800\n",
            "Epoch 15, Loss: 64.3711\n",
            "Epoch 15, Loss: 65.0636\n",
            "Epoch 15, Loss: 65.7553\n",
            "Epoch 15, Loss: 66.4428\n",
            "Epoch 15, Loss: 67.1364\n",
            "Epoch 15, Loss: 67.8293\n",
            "Epoch 15, Loss: 68.5241\n",
            "Epoch 15, Loss: 69.2167\n",
            "Epoch 15, Loss: 69.9091\n",
            "Epoch 15, Loss: 70.6000\n",
            "Epoch 15, Loss: 71.2742\n",
            "Epoch 15, Loss: 71.9666\n",
            "Epoch 15, Loss: 72.6593\n",
            "Epoch 15, Loss: 73.3484\n",
            "Epoch 15, Loss: 74.0401\n",
            "Epoch 15, Loss: 74.7337\n",
            "Epoch 15, Loss: 75.4268\n",
            "Epoch 15, Loss: 76.1148\n",
            "Epoch 15, Loss: 76.8089\n",
            "Epoch 15, Loss: 77.5028\n",
            "Epoch 15, Loss: 78.1935\n",
            "Epoch 15, Loss: 78.8882\n",
            "Epoch 15, Loss: 79.5785\n",
            "Epoch 15, Loss: 80.2696\n",
            "Epoch 16, Loss: 0.6927\n",
            "Epoch 16, Loss: 1.3863\n",
            "Epoch 16, Loss: 2.0804\n",
            "Epoch 16, Loss: 2.7728\n",
            "Epoch 16, Loss: 3.4644\n",
            "Epoch 16, Loss: 4.1571\n",
            "Epoch 16, Loss: 4.7629\n",
            "Epoch 16, Loss: 5.4537\n",
            "Epoch 16, Loss: 6.1469\n",
            "Epoch 16, Loss: 6.8407\n",
            "Epoch 16, Loss: 7.5354\n",
            "Epoch 16, Loss: 8.2285\n",
            "Epoch 16, Loss: 8.9204\n",
            "Epoch 16, Loss: 9.6127\n",
            "Epoch 16, Loss: 10.3048\n",
            "Epoch 16, Loss: 10.9986\n",
            "Epoch 16, Loss: 11.6915\n",
            "Epoch 16, Loss: 12.3855\n",
            "Epoch 16, Loss: 13.0797\n",
            "Epoch 16, Loss: 13.7724\n",
            "Epoch 16, Loss: 14.4629\n",
            "Epoch 16, Loss: 15.1568\n",
            "Epoch 16, Loss: 15.8500\n",
            "Epoch 16, Loss: 16.5431\n",
            "Epoch 16, Loss: 17.2357\n",
            "Epoch 16, Loss: 17.9302\n",
            "Epoch 16, Loss: 18.6233\n",
            "Epoch 16, Loss: 19.3193\n",
            "Epoch 16, Loss: 20.0130\n",
            "Epoch 16, Loss: 20.7046\n",
            "Epoch 16, Loss: 21.3991\n",
            "Epoch 16, Loss: 22.0924\n",
            "Epoch 16, Loss: 22.7851\n",
            "Epoch 16, Loss: 23.4779\n",
            "Epoch 16, Loss: 24.1714\n",
            "Epoch 16, Loss: 24.8645\n",
            "Epoch 16, Loss: 25.5568\n",
            "Epoch 16, Loss: 26.2485\n",
            "Epoch 16, Loss: 26.9419\n",
            "Epoch 16, Loss: 27.6349\n",
            "Epoch 16, Loss: 28.3271\n",
            "Epoch 16, Loss: 29.0208\n",
            "Epoch 16, Loss: 29.7133\n",
            "Epoch 16, Loss: 30.4070\n",
            "Epoch 16, Loss: 31.1022\n",
            "Epoch 16, Loss: 31.7949\n",
            "Epoch 16, Loss: 32.4890\n",
            "Epoch 16, Loss: 33.1801\n",
            "Epoch 16, Loss: 33.8737\n",
            "Epoch 16, Loss: 34.5664\n",
            "Epoch 16, Loss: 35.2596\n",
            "Epoch 16, Loss: 35.9526\n",
            "Epoch 16, Loss: 36.6453\n",
            "Epoch 16, Loss: 37.3378\n",
            "Epoch 16, Loss: 38.0312\n",
            "Epoch 16, Loss: 38.7245\n",
            "Epoch 16, Loss: 39.4186\n",
            "Epoch 16, Loss: 40.1115\n",
            "Epoch 16, Loss: 40.8049\n",
            "Epoch 16, Loss: 41.4983\n",
            "Epoch 16, Loss: 42.1916\n",
            "Epoch 16, Loss: 42.8841\n",
            "Epoch 16, Loss: 43.5780\n",
            "Epoch 16, Loss: 44.2726\n",
            "Epoch 16, Loss: 44.9652\n",
            "Epoch 16, Loss: 45.6584\n",
            "Epoch 16, Loss: 46.3506\n",
            "Epoch 16, Loss: 47.0448\n",
            "Epoch 16, Loss: 47.7384\n",
            "Epoch 16, Loss: 48.4318\n",
            "Epoch 16, Loss: 49.1239\n",
            "Epoch 16, Loss: 49.8162\n",
            "Epoch 16, Loss: 50.5094\n",
            "Epoch 16, Loss: 51.2019\n",
            "Epoch 16, Loss: 51.8951\n",
            "Epoch 16, Loss: 52.5904\n",
            "Epoch 16, Loss: 53.2824\n",
            "Epoch 16, Loss: 53.9748\n",
            "Epoch 16, Loss: 54.6703\n",
            "Epoch 16, Loss: 55.3625\n",
            "Epoch 16, Loss: 56.0559\n",
            "Epoch 16, Loss: 56.7489\n",
            "Epoch 16, Loss: 57.4418\n",
            "Epoch 16, Loss: 58.1347\n",
            "Epoch 16, Loss: 58.8275\n",
            "Epoch 16, Loss: 59.5222\n",
            "Epoch 16, Loss: 60.2162\n",
            "Epoch 16, Loss: 60.9093\n",
            "Epoch 16, Loss: 61.6022\n",
            "Epoch 16, Loss: 62.2929\n",
            "Epoch 16, Loss: 62.9851\n",
            "Epoch 16, Loss: 63.6801\n",
            "Epoch 16, Loss: 64.3711\n",
            "Epoch 16, Loss: 65.0633\n",
            "Epoch 16, Loss: 65.7554\n",
            "Epoch 16, Loss: 66.4424\n",
            "Epoch 16, Loss: 67.1358\n",
            "Epoch 16, Loss: 67.8286\n",
            "Epoch 16, Loss: 68.5238\n",
            "Epoch 16, Loss: 69.2166\n",
            "Epoch 16, Loss: 69.9090\n",
            "Epoch 16, Loss: 70.5991\n",
            "Epoch 16, Loss: 71.2727\n",
            "Epoch 16, Loss: 71.9657\n",
            "Epoch 16, Loss: 72.6588\n",
            "Epoch 16, Loss: 73.3483\n",
            "Epoch 16, Loss: 74.0400\n",
            "Epoch 16, Loss: 74.7337\n",
            "Epoch 16, Loss: 75.4258\n",
            "Epoch 16, Loss: 76.1142\n",
            "Epoch 16, Loss: 76.8084\n",
            "Epoch 16, Loss: 77.5021\n",
            "Epoch 16, Loss: 78.1926\n",
            "Epoch 16, Loss: 78.8875\n",
            "Epoch 16, Loss: 79.5782\n",
            "Epoch 16, Loss: 80.2695\n",
            "Epoch 17, Loss: 0.6931\n",
            "Epoch 17, Loss: 1.3870\n",
            "Epoch 17, Loss: 2.0815\n",
            "Epoch 17, Loss: 2.7741\n",
            "Epoch 17, Loss: 3.4662\n",
            "Epoch 17, Loss: 4.1581\n",
            "Epoch 17, Loss: 4.7578\n",
            "Epoch 17, Loss: 5.4488\n",
            "Epoch 17, Loss: 6.1421\n",
            "Epoch 17, Loss: 6.8357\n",
            "Epoch 17, Loss: 7.5303\n",
            "Epoch 17, Loss: 8.2234\n",
            "Epoch 17, Loss: 8.9152\n",
            "Epoch 17, Loss: 9.6071\n",
            "Epoch 17, Loss: 10.2996\n",
            "Epoch 17, Loss: 10.9933\n",
            "Epoch 17, Loss: 11.6859\n",
            "Epoch 17, Loss: 12.3799\n",
            "Epoch 17, Loss: 13.0742\n",
            "Epoch 17, Loss: 13.7661\n",
            "Epoch 17, Loss: 14.4570\n",
            "Epoch 17, Loss: 15.1507\n",
            "Epoch 17, Loss: 15.8439\n",
            "Epoch 17, Loss: 16.5368\n",
            "Epoch 17, Loss: 17.2293\n",
            "Epoch 17, Loss: 17.9236\n",
            "Epoch 17, Loss: 18.6170\n",
            "Epoch 17, Loss: 19.3128\n",
            "Epoch 17, Loss: 20.0056\n",
            "Epoch 17, Loss: 20.6972\n",
            "Epoch 17, Loss: 21.3914\n",
            "Epoch 17, Loss: 22.0841\n",
            "Epoch 17, Loss: 22.7765\n",
            "Epoch 17, Loss: 23.4692\n",
            "Epoch 17, Loss: 24.1627\n",
            "Epoch 17, Loss: 24.8557\n",
            "Epoch 17, Loss: 25.5482\n",
            "Epoch 17, Loss: 26.2399\n",
            "Epoch 17, Loss: 26.9335\n",
            "Epoch 17, Loss: 27.6259\n",
            "Epoch 17, Loss: 28.3180\n",
            "Epoch 17, Loss: 29.0120\n",
            "Epoch 17, Loss: 29.7043\n",
            "Epoch 17, Loss: 30.3989\n",
            "Epoch 17, Loss: 31.0949\n",
            "Epoch 17, Loss: 31.7874\n",
            "Epoch 17, Loss: 32.4816\n",
            "Epoch 17, Loss: 33.1725\n",
            "Epoch 17, Loss: 33.8660\n",
            "Epoch 17, Loss: 34.5589\n",
            "Epoch 17, Loss: 35.2523\n",
            "Epoch 17, Loss: 35.9453\n",
            "Epoch 17, Loss: 36.6380\n",
            "Epoch 17, Loss: 37.3302\n",
            "Epoch 17, Loss: 38.0238\n",
            "Epoch 17, Loss: 38.7169\n",
            "Epoch 17, Loss: 39.4109\n",
            "Epoch 17, Loss: 40.1042\n",
            "Epoch 17, Loss: 40.7975\n",
            "Epoch 17, Loss: 41.4911\n",
            "Epoch 17, Loss: 42.1843\n",
            "Epoch 17, Loss: 42.8770\n",
            "Epoch 17, Loss: 43.5709\n",
            "Epoch 17, Loss: 44.2655\n",
            "Epoch 17, Loss: 44.9581\n",
            "Epoch 17, Loss: 45.6514\n",
            "Epoch 17, Loss: 46.3432\n",
            "Epoch 17, Loss: 47.0375\n",
            "Epoch 17, Loss: 47.7315\n",
            "Epoch 17, Loss: 48.4250\n",
            "Epoch 17, Loss: 49.1176\n",
            "Epoch 17, Loss: 49.8097\n",
            "Epoch 17, Loss: 50.5029\n",
            "Epoch 17, Loss: 51.1956\n",
            "Epoch 17, Loss: 51.8890\n",
            "Epoch 17, Loss: 52.5840\n",
            "Epoch 17, Loss: 53.2762\n",
            "Epoch 17, Loss: 53.9682\n",
            "Epoch 17, Loss: 54.6635\n",
            "Epoch 17, Loss: 55.3558\n",
            "Epoch 17, Loss: 56.0494\n",
            "Epoch 17, Loss: 56.7423\n",
            "Epoch 17, Loss: 57.4351\n",
            "Epoch 17, Loss: 58.1279\n",
            "Epoch 17, Loss: 58.8208\n",
            "Epoch 17, Loss: 59.5155\n",
            "Epoch 17, Loss: 60.2092\n",
            "Epoch 17, Loss: 60.9022\n",
            "Epoch 17, Loss: 61.5950\n",
            "Epoch 17, Loss: 62.2854\n",
            "Epoch 17, Loss: 62.9773\n",
            "Epoch 17, Loss: 63.6721\n",
            "Epoch 17, Loss: 64.3630\n",
            "Epoch 17, Loss: 65.0555\n",
            "Epoch 17, Loss: 65.7472\n",
            "Epoch 17, Loss: 66.4340\n",
            "Epoch 17, Loss: 67.1274\n",
            "Epoch 17, Loss: 67.8203\n",
            "Epoch 17, Loss: 68.5152\n",
            "Epoch 17, Loss: 69.2079\n",
            "Epoch 17, Loss: 69.9002\n",
            "Epoch 17, Loss: 70.5910\n",
            "Epoch 17, Loss: 71.2637\n",
            "Epoch 17, Loss: 71.9559\n",
            "Epoch 17, Loss: 72.6486\n",
            "Epoch 17, Loss: 73.3375\n",
            "Epoch 17, Loss: 74.0292\n",
            "Epoch 17, Loss: 74.7234\n",
            "Epoch 17, Loss: 75.4165\n",
            "Epoch 17, Loss: 76.1038\n",
            "Epoch 17, Loss: 76.7980\n",
            "Epoch 17, Loss: 77.4921\n",
            "Epoch 17, Loss: 78.1822\n",
            "Epoch 17, Loss: 78.8773\n",
            "Epoch 17, Loss: 79.5697\n",
            "Epoch 17, Loss: 80.2591\n",
            "Epoch 18, Loss: 0.6925\n",
            "Epoch 18, Loss: 1.3861\n",
            "Epoch 18, Loss: 2.0803\n",
            "Epoch 18, Loss: 2.7729\n",
            "Epoch 18, Loss: 3.4645\n",
            "Epoch 18, Loss: 4.1567\n",
            "Epoch 18, Loss: 4.7527\n",
            "Epoch 18, Loss: 5.4433\n",
            "Epoch 18, Loss: 6.1367\n",
            "Epoch 18, Loss: 6.8306\n",
            "Epoch 18, Loss: 7.5255\n",
            "Epoch 18, Loss: 8.2191\n",
            "Epoch 18, Loss: 8.9111\n",
            "Epoch 18, Loss: 9.6030\n",
            "Epoch 18, Loss: 10.2951\n",
            "Epoch 18, Loss: 10.9888\n",
            "Epoch 18, Loss: 11.6816\n",
            "Epoch 18, Loss: 12.3756\n",
            "Epoch 18, Loss: 13.0696\n",
            "Epoch 18, Loss: 13.7627\n",
            "Epoch 18, Loss: 14.4534\n",
            "Epoch 18, Loss: 15.1471\n",
            "Epoch 18, Loss: 15.8403\n",
            "Epoch 18, Loss: 16.5333\n",
            "Epoch 18, Loss: 17.2261\n",
            "Epoch 18, Loss: 17.9206\n",
            "Epoch 18, Loss: 18.6137\n",
            "Epoch 18, Loss: 19.3093\n",
            "Epoch 18, Loss: 20.0030\n",
            "Epoch 18, Loss: 20.6947\n",
            "Epoch 18, Loss: 21.3893\n",
            "Epoch 18, Loss: 22.0828\n",
            "Epoch 18, Loss: 22.7752\n",
            "Epoch 18, Loss: 23.4680\n",
            "Epoch 18, Loss: 24.1612\n",
            "Epoch 18, Loss: 24.8540\n",
            "Epoch 18, Loss: 25.5462\n",
            "Epoch 18, Loss: 26.2379\n",
            "Epoch 18, Loss: 26.9314\n",
            "Epoch 18, Loss: 27.6242\n",
            "Epoch 18, Loss: 28.3165\n",
            "Epoch 18, Loss: 29.0102\n",
            "Epoch 18, Loss: 29.7023\n",
            "Epoch 18, Loss: 30.3963\n",
            "Epoch 18, Loss: 31.0913\n",
            "Epoch 18, Loss: 31.7841\n",
            "Epoch 18, Loss: 32.4782\n",
            "Epoch 18, Loss: 33.1692\n",
            "Epoch 18, Loss: 33.8624\n",
            "Epoch 18, Loss: 34.5548\n",
            "Epoch 18, Loss: 35.2482\n",
            "Epoch 18, Loss: 35.9412\n",
            "Epoch 18, Loss: 36.6340\n",
            "Epoch 18, Loss: 37.3264\n",
            "Epoch 18, Loss: 38.0196\n",
            "Epoch 18, Loss: 38.7129\n",
            "Epoch 18, Loss: 39.4070\n",
            "Epoch 18, Loss: 40.0997\n",
            "Epoch 18, Loss: 40.7935\n",
            "Epoch 18, Loss: 41.4869\n",
            "Epoch 18, Loss: 42.1802\n",
            "Epoch 18, Loss: 42.8726\n",
            "Epoch 18, Loss: 43.5666\n",
            "Epoch 18, Loss: 44.2612\n",
            "Epoch 18, Loss: 44.9537\n",
            "Epoch 18, Loss: 45.6472\n",
            "Epoch 18, Loss: 46.3391\n",
            "Epoch 18, Loss: 47.0332\n",
            "Epoch 18, Loss: 47.7267\n",
            "Epoch 18, Loss: 48.4200\n",
            "Epoch 18, Loss: 49.1123\n",
            "Epoch 18, Loss: 49.8045\n",
            "Epoch 18, Loss: 50.4977\n",
            "Epoch 18, Loss: 51.1902\n",
            "Epoch 18, Loss: 51.8834\n",
            "Epoch 18, Loss: 52.5785\n",
            "Epoch 18, Loss: 53.2704\n",
            "Epoch 18, Loss: 53.9626\n",
            "Epoch 18, Loss: 54.6578\n",
            "Epoch 18, Loss: 55.3502\n",
            "Epoch 18, Loss: 56.0434\n",
            "Epoch 18, Loss: 56.7365\n",
            "Epoch 18, Loss: 57.4291\n",
            "Epoch 18, Loss: 58.1217\n",
            "Epoch 18, Loss: 58.8147\n",
            "Epoch 18, Loss: 59.5094\n",
            "Epoch 18, Loss: 60.2034\n",
            "Epoch 18, Loss: 60.8964\n",
            "Epoch 18, Loss: 61.5893\n",
            "Epoch 18, Loss: 62.2802\n",
            "Epoch 18, Loss: 62.9725\n",
            "Epoch 18, Loss: 63.6679\n",
            "Epoch 18, Loss: 64.3589\n",
            "Epoch 18, Loss: 65.0509\n",
            "Epoch 18, Loss: 65.7429\n",
            "Epoch 18, Loss: 66.4298\n",
            "Epoch 18, Loss: 67.1231\n",
            "Epoch 18, Loss: 67.8160\n",
            "Epoch 18, Loss: 68.5110\n",
            "Epoch 18, Loss: 69.2035\n",
            "Epoch 18, Loss: 69.8959\n",
            "Epoch 18, Loss: 70.5868\n",
            "Epoch 18, Loss: 71.2595\n",
            "Epoch 18, Loss: 71.9520\n",
            "Epoch 18, Loss: 72.6446\n",
            "Epoch 18, Loss: 73.3335\n",
            "Epoch 18, Loss: 74.0252\n",
            "Epoch 18, Loss: 74.7192\n",
            "Epoch 18, Loss: 75.4125\n",
            "Epoch 18, Loss: 76.1001\n",
            "Epoch 18, Loss: 76.7947\n",
            "Epoch 18, Loss: 77.4886\n",
            "Epoch 18, Loss: 78.1791\n",
            "Epoch 18, Loss: 78.8740\n",
            "Epoch 18, Loss: 79.5642\n",
            "Epoch 18, Loss: 80.2540\n",
            "Epoch 19, Loss: 0.6929\n",
            "Epoch 19, Loss: 1.3867\n",
            "Epoch 19, Loss: 2.0811\n",
            "Epoch 19, Loss: 2.7736\n",
            "Epoch 19, Loss: 3.4653\n",
            "Epoch 19, Loss: 4.1576\n",
            "Epoch 19, Loss: 4.7518\n",
            "Epoch 19, Loss: 5.4428\n",
            "Epoch 19, Loss: 6.1361\n",
            "Epoch 19, Loss: 6.8298\n",
            "Epoch 19, Loss: 7.5245\n",
            "Epoch 19, Loss: 8.2173\n",
            "Epoch 19, Loss: 8.9091\n",
            "Epoch 19, Loss: 9.6009\n",
            "Epoch 19, Loss: 10.2930\n",
            "Epoch 19, Loss: 10.9869\n",
            "Epoch 19, Loss: 11.6795\n",
            "Epoch 19, Loss: 12.3735\n",
            "Epoch 19, Loss: 13.0677\n",
            "Epoch 19, Loss: 13.7596\n",
            "Epoch 19, Loss: 14.4506\n",
            "Epoch 19, Loss: 15.1445\n",
            "Epoch 19, Loss: 15.8379\n",
            "Epoch 19, Loss: 16.5308\n",
            "Epoch 19, Loss: 17.2235\n",
            "Epoch 19, Loss: 17.9179\n",
            "Epoch 19, Loss: 18.6112\n",
            "Epoch 19, Loss: 19.3071\n",
            "Epoch 19, Loss: 20.0000\n",
            "Epoch 19, Loss: 20.6915\n",
            "Epoch 19, Loss: 21.3860\n",
            "Epoch 19, Loss: 22.0789\n",
            "Epoch 19, Loss: 22.7713\n",
            "Epoch 19, Loss: 23.4638\n",
            "Epoch 19, Loss: 24.1573\n",
            "Epoch 19, Loss: 24.8503\n",
            "Epoch 19, Loss: 25.5426\n",
            "Epoch 19, Loss: 26.2344\n",
            "Epoch 19, Loss: 26.9276\n",
            "Epoch 19, Loss: 27.6199\n",
            "Epoch 19, Loss: 28.3120\n",
            "Epoch 19, Loss: 29.0058\n",
            "Epoch 19, Loss: 29.6985\n",
            "Epoch 19, Loss: 30.3928\n",
            "Epoch 19, Loss: 31.0887\n",
            "Epoch 19, Loss: 31.7812\n",
            "Epoch 19, Loss: 32.4755\n",
            "Epoch 19, Loss: 33.1666\n",
            "Epoch 19, Loss: 33.8599\n",
            "Epoch 19, Loss: 34.5528\n",
            "Epoch 19, Loss: 35.2462\n",
            "Epoch 19, Loss: 35.9392\n",
            "Epoch 19, Loss: 36.6317\n",
            "Epoch 19, Loss: 37.3239\n",
            "Epoch 19, Loss: 38.0174\n",
            "Epoch 19, Loss: 38.7105\n",
            "Epoch 19, Loss: 39.4044\n",
            "Epoch 19, Loss: 40.0973\n",
            "Epoch 19, Loss: 40.7908\n",
            "Epoch 19, Loss: 41.4843\n",
            "Epoch 19, Loss: 42.1773\n",
            "Epoch 19, Loss: 42.8699\n",
            "Epoch 19, Loss: 43.5638\n",
            "Epoch 19, Loss: 44.2584\n",
            "Epoch 19, Loss: 44.9509\n",
            "Epoch 19, Loss: 45.6444\n",
            "Epoch 19, Loss: 46.3361\n",
            "Epoch 19, Loss: 47.0304\n",
            "Epoch 19, Loss: 47.7241\n",
            "Epoch 19, Loss: 48.4175\n",
            "Epoch 19, Loss: 49.1100\n",
            "Epoch 19, Loss: 49.8020\n",
            "Epoch 19, Loss: 50.4952\n",
            "Epoch 19, Loss: 51.1879\n",
            "Epoch 19, Loss: 51.8813\n",
            "Epoch 19, Loss: 52.5762\n",
            "Epoch 19, Loss: 53.2681\n",
            "Epoch 19, Loss: 53.9601\n",
            "Epoch 19, Loss: 54.6557\n",
            "Epoch 19, Loss: 55.3483\n",
            "Epoch 19, Loss: 56.0417\n",
            "Epoch 19, Loss: 56.7348\n",
            "Epoch 19, Loss: 57.4274\n",
            "Epoch 19, Loss: 58.1201\n",
            "Epoch 19, Loss: 58.8130\n",
            "Epoch 19, Loss: 59.5078\n",
            "Epoch 19, Loss: 60.2017\n",
            "Epoch 19, Loss: 60.8951\n",
            "Epoch 19, Loss: 61.5878\n",
            "Epoch 19, Loss: 62.2781\n",
            "Epoch 19, Loss: 62.9700\n",
            "Epoch 19, Loss: 63.6653\n",
            "Epoch 19, Loss: 64.3558\n",
            "Epoch 19, Loss: 65.0480\n",
            "Epoch 19, Loss: 65.7397\n",
            "Epoch 19, Loss: 66.4260\n",
            "Epoch 19, Loss: 67.1192\n",
            "Epoch 19, Loss: 67.8120\n",
            "Epoch 19, Loss: 68.5068\n",
            "Epoch 19, Loss: 69.1992\n",
            "Epoch 19, Loss: 69.8913\n",
            "Epoch 19, Loss: 70.5820\n",
            "Epoch 19, Loss: 71.2532\n",
            "Epoch 19, Loss: 71.9455\n",
            "Epoch 19, Loss: 72.6388\n",
            "Epoch 19, Loss: 73.3279\n",
            "Epoch 19, Loss: 74.0194\n",
            "Epoch 19, Loss: 74.7140\n",
            "Epoch 19, Loss: 75.4060\n",
            "Epoch 19, Loss: 76.0949\n",
            "Epoch 19, Loss: 76.7894\n",
            "Epoch 19, Loss: 77.4837\n",
            "Epoch 19, Loss: 78.1736\n",
            "Epoch 19, Loss: 78.8689\n",
            "Epoch 19, Loss: 79.5590\n",
            "Epoch 19, Loss: 80.2499\n",
            "Epoch 20, Loss: 0.6927\n",
            "Epoch 20, Loss: 1.3864\n",
            "Epoch 20, Loss: 2.0805\n",
            "Epoch 20, Loss: 2.7728\n",
            "Epoch 20, Loss: 3.4645\n",
            "Epoch 20, Loss: 4.1567\n",
            "Epoch 20, Loss: 4.7444\n",
            "Epoch 20, Loss: 5.4353\n",
            "Epoch 20, Loss: 6.1287\n",
            "Epoch 20, Loss: 6.8227\n",
            "Epoch 20, Loss: 7.5176\n",
            "Epoch 20, Loss: 8.2110\n",
            "Epoch 20, Loss: 8.9027\n",
            "Epoch 20, Loss: 9.5946\n",
            "Epoch 20, Loss: 10.2865\n",
            "Epoch 20, Loss: 10.9801\n",
            "Epoch 20, Loss: 11.6727\n",
            "Epoch 20, Loss: 12.3665\n",
            "Epoch 20, Loss: 13.0604\n",
            "Epoch 20, Loss: 13.7532\n",
            "Epoch 20, Loss: 14.4439\n",
            "Epoch 20, Loss: 15.1375\n",
            "Epoch 20, Loss: 15.8309\n",
            "Epoch 20, Loss: 16.5239\n",
            "Epoch 20, Loss: 17.2168\n",
            "Epoch 20, Loss: 17.9112\n",
            "Epoch 20, Loss: 18.6044\n",
            "Epoch 20, Loss: 19.3003\n",
            "Epoch 20, Loss: 19.9939\n",
            "Epoch 20, Loss: 20.6855\n",
            "Epoch 20, Loss: 21.3801\n",
            "Epoch 20, Loss: 22.0735\n",
            "Epoch 20, Loss: 22.7658\n",
            "Epoch 20, Loss: 23.4588\n",
            "Epoch 20, Loss: 24.1524\n",
            "Epoch 20, Loss: 24.8451\n",
            "Epoch 20, Loss: 25.5375\n",
            "Epoch 20, Loss: 26.2293\n",
            "Epoch 20, Loss: 26.9228\n",
            "Epoch 20, Loss: 27.6155\n",
            "Epoch 20, Loss: 28.3080\n",
            "Epoch 20, Loss: 29.0018\n",
            "Epoch 20, Loss: 29.6939\n",
            "Epoch 20, Loss: 30.3877\n",
            "Epoch 20, Loss: 31.0827\n",
            "Epoch 20, Loss: 31.7751\n",
            "Epoch 20, Loss: 32.4694\n",
            "Epoch 20, Loss: 33.1606\n",
            "Epoch 20, Loss: 33.8541\n",
            "Epoch 20, Loss: 34.5468\n",
            "Epoch 20, Loss: 35.2400\n",
            "Epoch 20, Loss: 35.9330\n",
            "Epoch 20, Loss: 36.6255\n",
            "Epoch 20, Loss: 37.3179\n",
            "Epoch 20, Loss: 38.0114\n",
            "Epoch 20, Loss: 38.7045\n",
            "Epoch 20, Loss: 39.3985\n",
            "Epoch 20, Loss: 40.0912\n",
            "Epoch 20, Loss: 40.7847\n",
            "Epoch 20, Loss: 41.4781\n",
            "Epoch 20, Loss: 42.1710\n",
            "Epoch 20, Loss: 42.8635\n",
            "Epoch 20, Loss: 43.5571\n",
            "Epoch 20, Loss: 44.2517\n",
            "Epoch 20, Loss: 44.9442\n",
            "Epoch 20, Loss: 45.6376\n",
            "Epoch 20, Loss: 46.3296\n",
            "Epoch 20, Loss: 47.0238\n",
            "Epoch 20, Loss: 47.7173\n",
            "Epoch 20, Loss: 48.4107\n",
            "Epoch 20, Loss: 49.1031\n",
            "Epoch 20, Loss: 49.7950\n",
            "Epoch 20, Loss: 50.4883\n",
            "Epoch 20, Loss: 51.1807\n",
            "Epoch 20, Loss: 51.8738\n",
            "Epoch 20, Loss: 52.5689\n",
            "Epoch 20, Loss: 53.2610\n",
            "Epoch 20, Loss: 53.9528\n",
            "Epoch 20, Loss: 54.6482\n",
            "Epoch 20, Loss: 55.3407\n",
            "Epoch 20, Loss: 56.0337\n",
            "Epoch 20, Loss: 56.7266\n",
            "Epoch 20, Loss: 57.4191\n",
            "Epoch 20, Loss: 58.1116\n",
            "Epoch 20, Loss: 58.8045\n",
            "Epoch 20, Loss: 59.4993\n",
            "Epoch 20, Loss: 60.1933\n",
            "Epoch 20, Loss: 60.8864\n",
            "Epoch 20, Loss: 61.5792\n",
            "Epoch 20, Loss: 62.2697\n",
            "Epoch 20, Loss: 62.9617\n",
            "Epoch 20, Loss: 63.6567\n",
            "Epoch 20, Loss: 64.3471\n",
            "Epoch 20, Loss: 65.0390\n",
            "Epoch 20, Loss: 65.7309\n",
            "Epoch 20, Loss: 66.4174\n",
            "Epoch 20, Loss: 67.1104\n",
            "Epoch 20, Loss: 67.8032\n",
            "Epoch 20, Loss: 68.4978\n",
            "Epoch 20, Loss: 69.1904\n",
            "Epoch 20, Loss: 69.8828\n",
            "Epoch 20, Loss: 70.5724\n",
            "Epoch 20, Loss: 71.2437\n",
            "Epoch 20, Loss: 71.9361\n",
            "Epoch 20, Loss: 72.6286\n",
            "Epoch 20, Loss: 73.3175\n",
            "Epoch 20, Loss: 74.0093\n",
            "Epoch 20, Loss: 74.7034\n",
            "Epoch 20, Loss: 75.3964\n",
            "Epoch 20, Loss: 76.0838\n",
            "Epoch 20, Loss: 76.7784\n",
            "Epoch 20, Loss: 77.4723\n",
            "Epoch 20, Loss: 78.1623\n",
            "Epoch 20, Loss: 78.8575\n",
            "Epoch 20, Loss: 79.5494\n",
            "Epoch 20, Loss: 80.2377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final Model Metrics -> Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-imdhhhoLaH",
        "outputId": "6fd53a02-4d6b-4c9d-848b-ae047f49d9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Metrics -> Accuracy: 0.5051, Precision: 0.5052, Recall: 0.5051, F1 Score: 0.5042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues', xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix for IoT Dataset\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xticks(ticks=[0, 1], labels=['Benign', 'Malware']) # Assuming binary classification\n",
        "plt.yticks(ticks=[0, 1], labels=['Benign', 'Malware'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "CHnjn9_FopgT",
        "outputId": "697224cb-959d-4915-f98b-e432f41c1bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'model' is your trained CausalGraphSAGE model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "all_probs = []\n",
        "all_labels_roc = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in noisy_data_list:\n",
        "        out = model(data.x, data.edge_index, data.time_window)\n",
        "        probs = F.softmax(out, dim=1).cpu().numpy()  # Get probabilities for each class\n",
        "        labels = data.y.cpu().numpy()\n",
        "\n",
        "        all_probs.extend(probs)\n",
        "        all_labels_roc.extend(labels)\n",
        "\n",
        "all_probs = np.array(all_probs)\n",
        "all_labels_roc = np.array(all_labels_roc)\n",
        "\n",
        "# Get false positive rate (fpr), true positive rate (tpr), thresholds for the positive class (assuming index 1 is 'Malware')\n",
        "fpr, tpr, thresholds = roc_curve(all_labels_roc, all_probs[:, 1])\n",
        "\n",
        "# Calculate AUC (Area Under the ROC Curve)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')  # Diagonal line representing random guessing\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for IoT Dataset')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "GUlldnwqqV9S",
        "outputId": "ee73a162-c997-4e4c-9c71-edb4c5d82f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}